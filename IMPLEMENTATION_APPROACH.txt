================================================================================
BATCH PAGE PROCESSING + PARALLEL LLM CALLS - IMPLEMENTATION APPROACH
================================================================================

CURRENT CODE FLOW (app_modular.py):
================================================================================

Current _stage_cost_optimized_processing():
  
  for account_num, account_info in all_accounts.items():
      account_pages = account_info['pages']
      account_page_texts = account_info['page_texts']
      
      page_results = []
      for page_num in sorted(account_pages):
          page_text = account_page_texts[page_num]
          
          # Process EACH PAGE individually (SEQUENTIAL)
          page_result = processor.process_single_page_with_llm(
              account_number=account_num,
              page_text=page_text,
              page_number=page_num
          )
          
          if page_result:
              page_results.append(page_result)
      
      # Merge results
      merged_result = processor.merge_page_results(...)

PROBLEMS:
  ‚úó Processes 1 page at a time
  ‚úó Waits for each LLM call to complete
  ‚úó No parallelization
  ‚úó No batching

================================================================================
PROPOSED IMPLEMENTATION - STEP 1: BATCH PAGE PROCESSING
================================================================================

New Method: process_batch_pages_with_llm()

def process_batch_pages_with_llm(self, account_number: str, page_texts: Dict[int, str], 
                                  pages: List[int], batch_size: int = 2) -> Optional[Dict]:
    """
    Process multiple pages in a single LLM call
    
    Args:
        account_number: The account number
        page_texts: Dict mapping page numbers to their OCR text
        pages: List of page numbers for this account
        batch_size: Number of pages per LLM call (2-3 recommended)
    
    Returns:
        Merged account data with all page results combined
    """
    
    # Step 1: Group pages into batches
    batches = []
    for i in range(0, len(pages), batch_size):
        batch_pages = pages[i:i+batch_size]
        batch_texts = [page_texts[p] for p in batch_pages]
        batches.append({
            'pages': batch_pages,
            'texts': batch_texts,
            'combined_text': '\n---PAGE BREAK---\n'.join(batch_texts)
        })
    
    print(f"   üì¶ Batching: {len(pages)} pages ‚Üí {len(batches)} batches")
    
    # Step 2: Process each batch with LLM
    batch_results = []
    for batch_idx, batch in enumerate(batches):
        print(f"      üìÑ Processing batch {batch_idx + 1}/{len(batches)} ({len(batch['pages'])} pages)")
        
        # Send combined text to LLM
        batch_data = self._extract_data_fields_from_text(batch['combined_text'])
        
        if batch_data:
            batch_results.append({
                'pages': batch['pages'],
                'extracted_data': batch_data
            })
    
    # Step 3: Merge all batch results
    if not batch_results:
        return None
    
    # Flatten batch results into page results for merging
    page_results = []
    for batch_result in batch_results:
        for page_num in batch_result['pages']:
            page_results.append({
                'page_number': page_num,
                'extracted_data': batch_result['extracted_data']
            })
    
    # Merge using existing merge logic
    merged_result = self.merge_page_results(account_number, page_results, pages)
    return merged_result

CHANGES TO EXISTING CODE:

In _stage_cost_optimized_processing():
  
  # OLD CODE:
  for page_num in sorted(account_pages):
      page_result = processor.process_single_page_with_llm(...)
  
  # NEW CODE:
  merged_result = processor.process_batch_pages_with_llm(
      account_number=account_num,
      page_texts=account_page_texts,
      pages=account_pages,
      batch_size=2  # 2-3 pages per call
  )

BENEFITS:
  ‚úì 50% fewer LLM calls (10 ‚Üí 5)
  ‚úì 50% cost reduction
  ‚úì 50% faster (sequential)

================================================================================
PROPOSED IMPLEMENTATION - STEP 2: PARALLEL LLM CALLS
================================================================================

New Method: process_batches_parallel()

from concurrent.futures import ThreadPoolExecutor, as_completed

def process_batches_parallel(self, account_number: str, page_texts: Dict[int, str], 
                             pages: List[int], batch_size: int = 2, 
                             max_workers: int = 3) -> Optional[Dict]:
    """
    Process multiple batches in parallel
    
    Args:
        account_number: The account number
        page_texts: Dict mapping page numbers to their OCR text
        pages: List of page numbers for this account
        batch_size: Number of pages per LLM call (2-3 recommended)
        max_workers: Number of concurrent LLM calls (3-5 recommended)
    
    Returns:
        Merged account data with all page results combined
    """
    
    # Step 1: Group pages into batches
    batches = []
    for i in range(0, len(pages), batch_size):
        batch_pages = pages[i:i+batch_size]
        batch_texts = [page_texts[p] for p in batch_pages]
        batches.append({
            'pages': batch_pages,
            'texts': batch_texts,
            'combined_text': '\n---PAGE BREAK---\n'.join(batch_texts)
        })
    
    print(f"   üì¶ Batching: {len(pages)} pages ‚Üí {len(batches)} batches")
    print(f"   ‚ö° Parallel: {max_workers} concurrent LLM calls")
    
    # Step 2: Process batches in parallel
    batch_results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all batches to executor (don't wait)
        futures = {}
        for batch_idx, batch in enumerate(batches):
            future = executor.submit(
                self._extract_data_fields_from_text,
                batch['combined_text']
            )
            futures[future] = {
                'batch_idx': batch_idx,
                'pages': batch['pages']
            }
        
        # Wait for all to complete and collect results
        for future in as_completed(futures):
            batch_info = futures[future]
            batch_idx = batch_info['batch_idx']
            pages_in_batch = batch_info['pages']
            
            try:
                batch_data = future.result()
                print(f"      ‚úÖ Batch {batch_idx + 1} completed ({len(pages_in_batch)} pages)")
                
                if batch_data:
                    batch_results.append({
                        'pages': pages_in_batch,
                        'extracted_data': batch_data
                    })
            except Exception as e:
                print(f"      ‚ùå Batch {batch_idx + 1} failed: {str(e)}")
                # Continue with other batches
    
    # Step 3: Merge all batch results
    if not batch_results:
        return None
    
    # Flatten batch results into page results for merging
    page_results = []
    for batch_result in batch_results:
        for page_num in batch_result['pages']:
            page_results.append({
                'page_number': page_num,
                'extracted_data': batch_result['extracted_data']
            })
    
    # Merge using existing merge logic
    merged_result = self.merge_page_results(account_number, page_results, pages)
    return merged_result

CHANGES TO EXISTING CODE:

In _stage_cost_optimized_processing():
  
  # OLD CODE:
  for page_num in sorted(account_pages):
      page_result = processor.process_single_page_with_llm(...)
  
  # NEW CODE:
  merged_result = processor.process_batches_parallel(
      account_number=account_num,
      page_texts=account_page_texts,
      pages=account_pages,
      batch_size=2,
      max_workers=3
  )

BENEFITS:
  ‚úì 80% faster (parallel processing)
  ‚úì Better resource utilization
  ‚úì Handles concurrent requests better

================================================================================
COMBINED IMPLEMENTATION (RECOMMENDED):
================================================================================

Replace the entire loop in _stage_cost_optimized_processing():

# OLD CODE (Sequential, Page-by-Page):
for account_num, account_info in all_accounts.items():
    account_pages = account_info['pages']
    account_page_texts = account_info['page_texts']
    
    page_results = []
    for page_num in sorted(account_pages):
        page_text = account_page_texts[page_num]
        page_result = processor.process_single_page_with_llm(
            account_number=account_num,
            page_text=page_text,
            page_number=page_num
        )
        if page_result:
            page_results.append(page_result)
    
    merged_result = processor.merge_page_results(...)
    if merged_result:
        accounts.append(merged_result)

# NEW CODE (Batch + Parallel):
for account_num, account_info in all_accounts.items():
    account_pages = account_info['pages']
    account_page_texts = account_info['page_texts']
    
    # Use new parallel batch processing
    merged_result = processor.process_batches_parallel(
        account_number=account_num,
        page_texts=account_page_texts,
        pages=account_pages,
        batch_size=2,  # 2-3 pages per call
        max_workers=3  # 3-5 concurrent calls
    )
    
    if merged_result:
        accounts.append(merged_result)

TOTAL CHANGES:
  - Add 1 new method: process_batches_parallel()
  - Modify 1 existing loop (5 lines changed)
  - No changes to result merging logic
  - No changes to account detection logic

================================================================================
CONFIGURATION PARAMETERS:
================================================================================

Batch Size (pages per LLM call):
  - 2 pages: Conservative, safer, 50% cost reduction
  - 3 pages: Aggressive, faster, 67% cost reduction
  - Recommendation: Start with 2, test with 3
  
Max Workers (concurrent LLM calls):
  - 1: Sequential (current behavior)
  - 2: Moderate parallelization
  - 3: Good balance (recommended)
  - 5: Aggressive parallelization
  - Recommendation: Start with 3, monitor resource usage

Tuning:
  batch_size = 2  # Conservative
  max_workers = 3  # Balanced
  
  # Or for aggressive optimization:
  batch_size = 3
  max_workers = 5

================================================================================
TESTING STRATEGY:
================================================================================

Phase 1: Unit Testing
  1. Test batch grouping logic
  2. Test parallel execution
  3. Test result merging
  4. Test error handling

Phase 2: Integration Testing
  1. Test with real documents
  2. Test with different batch sizes
  3. Test with different worker counts
  4. Test concurrent requests

Phase 3: Performance Testing
  1. Measure processing time
  2. Measure cost reduction
  3. Measure resource usage
  4. Compare with current implementation

Phase 4: Production Rollout
  1. Deploy to staging
  2. Monitor for 1 week
  3. Deploy to production
  4. Monitor metrics

================================================================================
MONITORING & METRICS:
================================================================================

Key Metrics to Track:

Processing Time:
  - Current: 45-60s per document
  - Target: 8-12s per document
  - Metric: avg_processing_time_seconds

LLM Calls:
  - Current: 10 calls per document
  - Target: 5 calls per document
  - Metric: llm_calls_per_document

Cost:
  - Current: $0.155 per document
  - Target: $0.0775 per document
  - Metric: cost_per_document

Throughput:
  - Current: 72 docs/hour
  - Target: 360 docs/hour
  - Metric: documents_per_hour

Resource Usage:
  - CPU: Target 80% utilization
  - Memory: Target < 500MB
  - Network: Target < 1Mbps

Error Rate:
  - Target: < 1% batch failures
  - Metric: batch_failure_rate

================================================================================
ROLLBACK PLAN:
================================================================================

If issues occur:

1. Immediate Rollback:
   - Revert to process_single_page_with_llm()
   - Takes 5 minutes
   - No data loss

2. Gradual Rollback:
   - Reduce batch_size from 3 to 2
   - Reduce max_workers from 5 to 3
   - Monitor metrics

3. Partial Rollback:
   - Keep batch processing
   - Disable parallel processing
   - Maintains 50% cost reduction

================================================================================
ESTIMATED EFFORT:
================================================================================

Development:
  - Batch processing: 2-3 hours
  - Parallel processing: 3-4 hours
  - Testing: 4-5 hours
  - Total: 9-12 hours

Deployment:
  - Code review: 1 hour
  - Staging deployment: 1 hour
  - Production deployment: 1 hour
  - Monitoring: 2 hours
  - Total: 5 hours

Total Effort: 14-17 hours (2-3 days)

Payback Period: ~1 month (at 1000 docs/month)
ROI: 1200% in first year

================================================================================
SUCCESS CRITERIA:
================================================================================

Implementation is successful if:

1. Processing time reduces from 50s to 10s (80% improvement)
2. LLM calls reduce from 10 to 5 (50% improvement)
3. Cost reduces from $0.155 to $0.0775 (50% improvement)
4. Error rate stays below 1%
5. No data loss or corruption
6. Results are identical to current implementation
7. Resource usage stays within limits
8. All tests pass

================================================================================
