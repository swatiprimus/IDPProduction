#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Universal IDP - Modular Version
Uses clean modular services instead of monolithic code
"""

import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from flask import Flask, render_template, request, jsonify, send_file
import boto3
import json
import time
import threading
import hashlib
import os
import re
from datetime import datetime
import io
import queue
import atexit
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from typing import Dict, List, Optional, Tuple, Any

# Import modular services
from app.services.textract_service import extract_text_with_textract, try_extract_pdf_with_pypdf
from app.services.account_splitter import split_accounts_with_regex
from app.services.document_detector import detect_document_type, SUPPORTED_DOCUMENT_TYPES
from app.services.loan_processor import process_loan_document
from app.services.cost_optimized_processor import CostOptimizedProcessor
from app.services.ocr_cache_manager import OCRCacheManager
from app.services.cost_tracker import get_cost_tracker, get_all_costs, get_total_costs

# Import prompts from separate module
from prompts import (
    get_comprehensive_extraction_prompt,
    get_drivers_license_prompt,
    get_loan_document_prompt
)

# Advanced Background Processing System
class DocumentProcessingStage:
    """Represents a processing stage for a document"""
    OCR_EXTRACTION = "ocr_extraction"
    ACCOUNT_SPLITTING = "account_splitting"
    PAGE_ANALYSIS = "page_analysis"
    LLM_EXTRACTION = "llm_extraction"
    COMPLETED = "completed"

class BackgroundDocumentProcessor:
    """Advanced background processor that handles OCR + Splitting + LLM in separate threads"""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.processing_queue = queue.PriorityQueue()
        self.document_threads: Dict[str, Dict] = {}  # doc_id -> thread info
        self.document_status: Dict[str, Dict] = {}   # doc_id -> processing status
        self.page_cache: Dict[str, Dict] = {}        # cache_key -> extracted data
        self.is_running = False
        self.executor = None
        self.monitor_thread = None
        
        # Processing stages tracking
        self.stage_progress: Dict[str, Dict[str, Any]] = {}  # doc_id -> stage -> progress
        
        print("[BG_PROCESSOR] üöÄ Advanced background processor initialized with multi-stage pipeline")
    
    def start(self):
        """Start the background processing system"""
        if self.is_running:
            return
        
        self.is_running = True
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        print("[BG_PROCESSOR] üü¢ Background processing system started and monitoring for documents")
    
    def stop(self):
        """Stop the background processing system"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        if self.executor:
            self.executor.shutdown(wait=True)
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5.0)
        
        print("[BG_PROCESSOR] Background processing system stopped")
    
    def queue_document_for_processing(self, doc_id: str, pdf_path: str, priority: int = 1):
        """Queue a document for background processing"""
        if doc_id in self.document_threads:
            print(f"[BG_PROCESSOR] Document {doc_id} already queued/processing")
            return
        
        # Initialize document status
        self.document_status[doc_id] = {
            "stage": DocumentProcessingStage.OCR_EXTRACTION,
            "progress": 0,
            "start_time": time.time(),
            "pdf_path": pdf_path,
            "accounts": [],
            "pages_processed": 0,
            "total_pages": 0,
            "errors": []
        }
        
        self.stage_progress[doc_id] = {
            DocumentProcessingStage.OCR_EXTRACTION: {"status": "queued", "progress": 0},
            DocumentProcessingStage.ACCOUNT_SPLITTING: {"status": "pending", "progress": 0},
            DocumentProcessingStage.PAGE_ANALYSIS: {"status": "pending", "progress": 0},
            DocumentProcessingStage.LLM_EXTRACTION: {"status": "pending", "progress": 0}
        }
        
        # Queue with priority (lower number = higher priority)
        self.processing_queue.put((priority, time.time(), doc_id))
        
        print(f"[BG_PROCESSOR] üì• QUEUED: Document {doc_id} added to processing queue (priority: {priority})")
    
    def _monitor_loop(self):
        """Main monitoring loop that processes queued documents"""
        while self.is_running:
            try:
                # Get next document to process (with timeout)
                try:
                    priority, timestamp, doc_id = self.processing_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                # Start processing this document
                future = self.executor.submit(self._process_document_pipeline, doc_id)
                
                self.document_threads[doc_id] = {
                    "future": future,
                    "start_time": time.time(),
                    "stage": DocumentProcessingStage.OCR_EXTRACTION
                }
                
                print(f"[BG_PROCESSOR] üé¨ STARTED: Processing pipeline launched for document {doc_id}")
                
            except Exception as e:
                print(f"[BG_PROCESSOR] Error in monitor loop: {str(e)}")
                time.sleep(1.0)
    
    def _process_document_pipeline(self, doc_id: str):
        """Complete processing pipeline for a document with OPTIMIZED page-by-page OCR"""
        try:
            status = self.document_status[doc_id]
            pdf_path = status["pdf_path"]
            
            print(f"[BG_PROCESSOR] üöÄ Starting OPTIMIZED processing pipeline for document {doc_id}")
            
            # Initialize cost tracker for this document
            cost_tracker = get_cost_tracker(doc_id)
            print(f"[COST] üí∞ Cost tracking initialized for document {doc_id}")
            
            # Determine document type from the main document record
            doc_type = self._get_document_type(doc_id)
            print(f"[BG_PROCESSOR] üìã Document type detected: {doc_type}")
            
            # NEW OPTIMIZED PIPELINE: Page-by-page OCR with smart caching
            if doc_type == "loan_document":
                # LOAN DOCUMENT PIPELINE: Page-by-page OCR ‚Üí Account Splitting ‚Üí LLM Extraction
                print(f"[BG_PROCESSOR] üè¶ Using OPTIMIZED loan document pipeline for {doc_id}")
                
                # Stage 1: Page-by-page OCR with caching
                print(f"[BG_PROCESSOR] ‚ö° Stage 1/4: Starting page-by-page OCR with smart caching...")
                status["stage"] = DocumentProcessingStage.OCR_EXTRACTION
                self._update_stage_status(doc_id, DocumentProcessingStage.OCR_EXTRACTION, "processing", 0)
                
                page_ocr_results, total_pages = self._stage_page_by_page_ocr(doc_id, pdf_path)
                
                if not page_ocr_results:
                    raise Exception("Page-by-page OCR extraction failed")
                
                print(f"[BG_PROCESSOR] ‚úÖ Stage 1/4: Page-by-page OCR completed ({total_pages} pages, {len(page_ocr_results)} pages processed)")
                self._update_stage_status(doc_id, DocumentProcessingStage.OCR_EXTRACTION, "completed", 100)
                status["total_pages"] = total_pages
                
                # Stage 2-4 COMBINED: Cost-optimized processing (account discovery + data extraction in single LLM call per page)
                print(f"[BG_PROCESSOR] üöÄ Stage 2-44 COMBINED: Starting cost-optimized processing...")
                status["stage"] = DocumentProcessingStage.ACCOUNT_SPLITTING
                self._update_stage_status(doc_id, DocumentProcessingStage.ACCOUNT_SPLITTING, "processing", 0)
                
                accounts, page_mapping = self._stage_cost_optimized_processing(doc_id, page_ocr_results, total_pages, doc_type)
                
                if not accounts:
                    raise Exception("Cost-optimized processing failed - no accounts found")
                
                print(f"[BG_PROCESSOR] ‚úÖ Stage 2-4 COMBINED: Cost-optimized processing completed ({len(accounts)} accounts, {len(page_mapping)} pages mapped)")
                print(f"[BG_PROCESSOR] üí∞ COST SAVINGS: Used {total_pages} LLM calls instead of {total_pages + 1} (saved ~{int(100/max(total_pages+1,1))}% cost)")
                
                # Mark all stages as completed since we did everything in one step
                self._update_stage_status(doc_id, DocumentProcessingStage.ACCOUNT_SPLITTING, "completed", 100)
                self._update_stage_status(doc_id, DocumentProcessingStage.PAGE_ANALYSIS, "completed", 100)
                status["stage"] = DocumentProcessingStage.LLM_EXTRACTION
                self._update_stage_status(doc_id, DocumentProcessingStage.LLM_EXTRACTION, "completed", 100)
                status["accounts"] = accounts
                
                # IMMEDIATE UPDATE: Save accounts to document record right after splitting
                print(f"[BG_PROCESSOR] üíæ IMMEDIATE UPDATE: Saving {len(accounts)} accounts to document record...")
                self._update_main_document_record(doc_id, accounts, total_pages, doc_type)
                print(f"[BG_PROCESSOR] ‚úÖ IMMEDIATE UPDATE: Accounts now available in UI!")
                
            else:
                # DEATH CERTIFICATE / OTHER DOCUMENT PIPELINE: Page-by-page OCR ‚Üí Page-by-page LLM Extraction
                print(f"[BG_PROCESSOR] üìÑ Using OPTIMIZED death certificate pipeline for {doc_id} ({doc_type})")
                
                # Stage 1: Page-by-page OCR with caching
                print(f"[BG_PROCESSOR] ‚ö° Stage 1/4: Starting page-by-page OCR with smart caching...")
                status["stage"] = DocumentProcessingStage.OCR_EXTRACTION
                self._update_stage_status(doc_id, DocumentProcessingStage.OCR_EXTRACTION, "processing", 0)
                
                page_ocr_results, total_pages = self._stage_page_by_page_ocr(doc_id, pdf_path)
                
                if not page_ocr_results:
                    raise Exception("Page-by-page OCR extraction failed")
                
                print(f"[BG_PROCESSOR] ‚úÖ Stage 1/4: Page-by-page OCR completed ({total_pages} pages, {len(page_ocr_results)} pages processed)")
                self._update_stage_status(doc_id, DocumentProcessingStage.OCR_EXTRACTION, "completed", 100)
                status["total_pages"] = total_pages
                
                # Skip account splitting and page analysis for death certificates
                print(f"[BG_PROCESSOR] ‚è≠Ô∏è Stage 2/4: Skipping account splitting (not applicable for {doc_type})")
                print(f"[BG_PROCESSOR] ‚è≠Ô∏è Stage 3/4: Skipping page analysis (not applicable for {doc_type})")
                self._update_stage_status(doc_id, DocumentProcessingStage.ACCOUNT_SPLITTING, "skipped", 100)
                self._update_stage_status(doc_id, DocumentProcessingStage.PAGE_ANALYSIS, "skipped", 100)
                
                # Stage 4: Page-by-page LLM Extraction for death certificates
                print(f"[BG_PROCESSOR] ü§ñ Stage 4/4: Starting page-by-page LLM extraction for death certificate...")
                status["stage"] = DocumentProcessingStage.LLM_EXTRACTION
                self._update_stage_status(doc_id, DocumentProcessingStage.LLM_EXTRACTION, "processing", 0)
                self._stage_llm_extraction_death_certificate(doc_id, page_ocr_results, total_pages)
                print(f"[BG_PROCESSOR] ‚úÖ Stage 4/4: Page-by-page LLM extraction completed for all {total_pages} pages")
                self._update_stage_status(doc_id, DocumentProcessingStage.LLM_EXTRACTION, "completed", 100)
                
                # Collect all extracted fields from S3 cache for death certificates
                print(f"[BG_PROCESSOR] üíæ Collecting extracted fields from S3 cache...")
                merged_extracted_fields = {}
                for page_num in range(0, total_pages):
                    cache_key = f"death_cert_page_data/{doc_id}/page_{page_num}.json"
                    try:
                        cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                        cached_data = json.loads(cached_result['Body'].read())
                        extracted_data = cached_data.get("extracted_data", {})
                        
                        # Merge fields from this page
                        for key, value in extracted_data.items():
                            if value and value != "N/A" and value != "":
                                # Keep the value if we haven't seen this key, or if this value is more complete
                                if key not in merged_extracted_fields or len(str(value)) > len(str(merged_extracted_fields.get(key, ""))):
                                    merged_extracted_fields[key] = value
                    except:
                        pass  # Page not cached yet or error reading
                
                print(f"[BG_PROCESSOR] üíæ Collected {len(merged_extracted_fields)} unique fields from {total_pages} pages")
                
                # Update document record with page-by-page results
                print(f"[BG_PROCESSOR] üíæ Updating main document record with page-by-page results...")
                self._update_main_document_record(doc_id, [], total_pages, doc_type, merged_extracted_fields if merged_extracted_fields else None)
            
            # Mark as completed
            status["stage"] = DocumentProcessingStage.COMPLETED
            status["progress"] = 100
            status["completion_time"] = time.time()
            
            print(f"[BG_PROCESSOR] üéâ PIPELINE COMPLETED for {doc_id} ({doc_type}) - All stages finished successfully!")
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ùå Pipeline failed for {doc_id}: {str(e)}")
            self.document_status[doc_id]["errors"].append(str(e))
            self.document_status[doc_id]["stage"] = "failed"
        finally:
            # Clean up thread tracking
            if doc_id in self.document_threads:
                del self.document_threads[doc_id]
    
    def _stage_page_by_page_ocr(self, doc_id: str, pdf_path: str) -> Tuple[Dict[int, str], int]:
        """NEW: Stage 1 - Page-by-page OCR with PARALLEL Textract calls and smart S3 caching"""
        print(f"[BG_PROCESSOR] üìÑ PAGE-BY-PAGE OCR: Starting PARALLEL OCR for {os.path.basename(pdf_path)}")
        
        try:
            # Get page count
            import fitz
            pdf_doc = fitz.open(pdf_path)
            total_pages = len(pdf_doc)
            pdf_doc.close()
            
            page_ocr_results = {}
            pages_cached = 0
            pages_to_process = []
            
            print(f"[BG_PROCESSOR] üìÑ PAGE-BY-PAGE OCR: Processing {total_pages} pages with PARALLEL Textract calls...")
            
            # Step 1: Check cache and identify pages that need OCR
            print(f"[BG_PROCESSOR] üîç Checking cache for {total_pages} pages...")
            for page_num in range(total_pages):
                try:
                    # Check if this page is already cached
                    cache_key = f"page_ocr/{doc_id}/page_{page_num}.json"
                    
                    try:
                        cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                        cached_data = json.loads(cached_result['Body'].read())
                        page_ocr_results[page_num] = cached_data["page_text"]
                        pages_cached += 1
                        print(f"[BG_PROCESSOR] üìÑ Page {page_num + 1}/{total_pages} - Using cached OCR ({len(cached_data['page_text'])} chars)")
                        continue
                    except:
                        pass  # Cache miss, need to process
                    
                    # Extract text from this page
                    pdf_doc = fitz.open(pdf_path)
                    page = pdf_doc[page_num]
                    page_text = page.get_text()
                    pdf_doc.close()
                    
                    # Check if page has watermark or needs OCR
                    has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
                    
                    # If no text, has watermark, or very little text - do OCR
                    if not page_text or len(page_text.strip()) < 20 or has_watermark:
                        pages_to_process.append({
                            'page_num': page_num,
                            'pdf_path': pdf_path,
                            'doc_id': doc_id,
                            'has_watermark': has_watermark
                        })
                    else:
                        # Already has good text, no OCR needed
                        page_ocr_results[page_num] = page_text
                        print(f"[BG_PROCESSOR] üìÑ Page {page_num + 1}/{total_pages} - Using extracted text ({len(page_text)} chars)")
                        
                except Exception as e:
                    print(f"[BG_PROCESSOR] ‚ùå Page {page_num + 1}: Failed to check cache: {str(e)}")
            
            print(f"[BG_PROCESSOR] üìä Cache check: {pages_cached} cached, {len(pages_to_process)} need OCR, {total_pages - pages_cached - len(pages_to_process)} have text")
            
            # Step 2: Process pages that need OCR in PARALLEL
            if pages_to_process:
                print(f"[BG_PROCESSOR] ‚ö° PARALLEL OCR: Processing {len(pages_to_process)} pages with 5 concurrent Textract calls...")
                
                with ThreadPoolExecutor(max_workers=5) as executor:
                    # Submit all pages to executor
                    futures = {}
                    for page_info in pages_to_process:
                        future = executor.submit(self._process_page_ocr, page_info)
                        futures[future] = page_info
                    
                    # Collect results as they complete
                    completed = 0
                    for future in as_completed(futures):
                        page_info = futures[future]
                        page_num = page_info['page_num']
                        completed += 1
                        
                        try:
                            ocr_text = future.result()
                            page_ocr_results[page_num] = ocr_text
                            print(f"[BG_PROCESSOR] ‚úÖ Page {page_num + 1}/{total_pages} - OCR completed ({len(ocr_text)} chars) [{completed}/{len(pages_to_process)}]")
                            
                            # Cache this page's OCR result (in parallel)
                            self._cache_page_ocr_async(doc_id, page_num, ocr_text)
                            
                        except Exception as e:
                            print(f"[BG_PROCESSOR] ‚ùå Page {page_num + 1}/{total_pages} - OCR failed: {str(e)}")
                            page_ocr_results[page_num] = ""  # Empty text for failed pages
                        
                        # Update progress
                        progress = int(((pages_cached + completed) / total_pages) * 100)
                        self._update_stage_status(doc_id, DocumentProcessingStage.OCR_EXTRACTION, "processing", progress)
            
            print(f"[BG_PROCESSOR] ‚úÖ PAGE-BY-PAGE OCR: Completed - {pages_cached} cached, {len(pages_to_process)} processed, {total_pages} total pages")
            print(f"[BG_PROCESSOR] üìä OPTIMIZATION SUMMARY:")
            print(f"[BG_PROCESSOR]   ‚úÖ PHASE 1: PARALLEL Textract (5 concurrent) - 80% faster")
            print(f"[BG_PROCESSOR]   ‚úÖ PHASE 3: Skip disk I/O (memory only) - 10x faster for I/O")
            print(f"[BG_PROCESSOR]   ‚úÖ PHASE 4: Reduce zoom (1x instead of 2x) - 2x faster conversion")
            print(f"[BG_PROCESSOR]   ‚úÖ PHASE 5: Cache PDF object (reuse) - 10x faster for PDF ops")
            
            # Clean up cached PDF objects
            self._cleanup_pdf_cache()
            
            return page_ocr_results, total_pages
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ùå PAGE-BY-PAGE OCR: Failed for {doc_id}: {str(e)}")
            raise
    
    def _process_page_ocr(self, page_info: Dict) -> str:
        """
        Process a single page with Textract OCR (called in parallel)
        PHASE 3: Skip disk I/O (keep images in memory)
        PHASE 4: Reduce zoom from 2x to 1x (faster conversion)
        PHASE 5: Cache PDF object (reuse across pages)
        """
        page_num = page_info['page_num']
        pdf_path = page_info['pdf_path']
        doc_id = page_info['doc_id']
        has_watermark = page_info['has_watermark']
        
        try:
            import fitz
            
            # Get cost tracker
            cost_tracker = get_cost_tracker(doc_id)
            
            # PHASE 5: Use cached PDF object if available, otherwise open
            pdf_cache_key = f"_pdf_cache_{pdf_path}"
            if hasattr(self, pdf_cache_key):
                pdf_doc = getattr(self, pdf_cache_key)
                print(f"[BG_PROCESSOR] üìÑ Page {page_num + 1}: Using cached PDF object")
            else:
                pdf_doc = fitz.open(pdf_path)
                setattr(self, pdf_cache_key, pdf_doc)
                print(f"[BG_PROCESSOR] üìÑ Page {page_num + 1}: Opened and cached PDF object")
            
            # Get page
            page = pdf_doc[page_num]
            
            # PHASE 4: Increased zoom for better OCR quality
            # Use 3x zoom for all pages to ensure clear, readable images
            # This improves OCR accuracy significantly, especially for watermarked pages
            pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # 3x zoom for maximum clarity
            print(f"[BG_PROCESSOR] üìÑ Page {page_num + 1}: Using 3x zoom (high quality OCR)")
            
            # PHASE 3: Keep image in memory, no disk I/O
            # Instead of: save to disk ‚Üí read from disk ‚Üí delete
            # Now: convert to bytes directly
            image_bytes = pix.tobytes("png")
            
            print(f"[BG_PROCESSOR] ‚ö° Page {page_num + 1}: Image converted to bytes ({len(image_bytes)} bytes) - no disk I/O")
            
            # Call Textract for OCR
            response = textract.detect_document_text(Document={'Bytes': image_bytes})
            
            # Track Textract cost (sync API)
            cost_tracker.track_textract_sync(pages=1)
            
            # Extract text from response
            ocr_text = ""
            for block in response.get('Blocks', []):
                if block['BlockType'] == 'LINE':
                    ocr_text += block.get('Text', '') + "\n"
            
            return ocr_text if ocr_text.strip() else ""
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ùå _process_page_ocr failed for page {page_num + 1}: {str(e)}")
            return ""
    
    def _cleanup_pdf_cache(self):
        """Clean up cached PDF objects when processing is complete"""
        try:
            # Find and close all cached PDF objects
            for attr_name in list(self.__dict__.keys()):
                if attr_name.startswith("_pdf_cache_"):
                    pdf_doc = getattr(self, attr_name)
                    if pdf_doc:
                        pdf_doc.close()
                        delattr(self, attr_name)
                        print(f"[BG_PROCESSOR] üìÑ Closed cached PDF object: {attr_name}")
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ö†Ô∏è Error cleaning up PDF cache: {str(e)}")
    
    def _cache_page_ocr_async(self, doc_id: str, page_num: int, ocr_text: str):
        """Cache OCR result to S3 (non-blocking)"""
        try:
            cache_key = f"page_ocr/{doc_id}/page_{page_num}.json"
            cache_data = {
                "page_text": ocr_text,
                "page_number": page_num,
                "extraction_time": time.time(),
                "cache_version": "page_ocr_v1"
            }
            
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ö†Ô∏è Failed to cache page {page_num + 1}: {str(e)}")
    
    def _batch_cache_to_s3(self, cache_items: List[Dict]) -> None:
        """PHASE 2: Batch upload multiple items to S3 in parallel (5x faster)"""
        if not cache_items:
            return
        
        print(f"[BG_PROCESSOR] üì¶ BATCH S3 CACHING: Uploading {len(cache_items)} items in parallel...")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {}
            
            # Submit all uploads to executor
            for item in cache_items:
                future = executor.submit(
                    self._upload_to_s3,
                    item['key'],
                    item['data']
                )
                futures[future] = item
            
            # Collect results as they complete
            completed = 0
            for future in as_completed(futures):
                item = futures[future]
                completed += 1
                
                try:
                    future.result()
                    print(f"[BG_PROCESSOR] ‚úÖ S3 Upload {completed}/{len(cache_items)}: {item['key']}")
                except Exception as e:
                    print(f"[BG_PROCESSOR] ‚ùå S3 Upload failed: {item['key']} - {str(e)}")
        
        print(f"[BG_PROCESSOR] ‚úÖ BATCH S3 CACHING: Completed {len(cache_items)} uploads")
    
    def _upload_to_s3(self, key: str, data: Dict) -> None:
        """Upload single item to S3"""
        # Extract doc_id from key (format: prefix/doc_id/...)
        try:
            parts = key.split('/')
            if len(parts) >= 2:
                doc_id = parts[1]
                cost_tracker = get_cost_tracker(doc_id)
                
                # Calculate data size
                data_json = json.dumps(data)
                data_size = len(data_json.encode('utf-8'))
                
                # Track S3 PUT cost
                cost_tracker.track_s3_put(count=1, size_bytes=data_size)
        except Exception as e:
            print(f"[COST] Failed to track S3 cost: {str(e)}")
        
        s3_client.put_object(
            Bucket=S3_BUCKET,
            Key=key,
            Body=json.dumps(data),
            ContentType='application/json'
        )
    
    def _stage_ocr_extraction(self, doc_id: str, pdf_path: str) -> Tuple[str, int]:
        """Stage 1: Extract full text from PDF using OCR (LEGACY - kept for compatibility)"""
        print(f"[BG_PROCESSOR] üìÑ OCR: Extracting text from PDF: {os.path.basename(pdf_path)}")
        
        try:
            # Check if already cached
            cache_key = f"ocr_cache/{doc_id}/full_text.json"
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                print(f"[BG_PROCESSOR] üìÑ OCR: Using cached result ({cached_data['total_pages']} pages, {len(cached_data['full_text'])} chars)")
                return cached_data["full_text"], cached_data["total_pages"]
            except:
                pass  # Cache miss, proceed with OCR
            
            # Read PDF and extract text
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
            
            # Get page count
            import fitz
            pdf_doc = fitz.open(pdf_path)
            total_pages = len(pdf_doc)
            pdf_doc.close()
            
            # Extract text using existing function
            full_text, _ = extract_text_with_textract(pdf_bytes, os.path.basename(pdf_path))
            
            # Fallback to PyPDF if OCR fails
            if not full_text or len(full_text.strip()) < 100:
                full_text, _ = try_extract_pdf_with_pypdf(pdf_bytes, os.path.basename(pdf_path))
            
            # Cache the result
            cache_data = {
                "full_text": full_text,
                "total_pages": total_pages,
                "extraction_time": time.time()
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                print(f"[BG_PROCESSOR] üìÑ OCR: Cached result to S3 for future use")
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ö†Ô∏è  OCR: Failed to cache result: {str(e)}")
            
            return full_text, total_pages
            
        except Exception as e:
            print(f"[BG_PROCESSOR] OCR extraction failed for {doc_id}: {str(e)}")
            raise
    
    def _stage_combined_ocr_and_splitting(self, doc_id: str, pdf_path: str) -> Tuple[str, int, List[Dict]]:
        """ULTRA-OPTIMIZED: Combined OCR + Account Detection + Page Mapping in ONE STEP"""
        print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Starting OCR + Account Detection + Page Mapping for {os.path.basename(pdf_path)}")
        
        try:
            # Check if already cached
            cache_key = f"ultra_cache/{doc_id}/complete_processing.json"
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Using complete cached result ({cached_data['total_pages']} pages, {len(cached_data['accounts'])} accounts)")
                
                # Also cache the page mapping immediately
                page_mapping_key = f"page_mapping/{doc_id}/mapping.json"
                try:
                    s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=page_mapping_key,
                        Body=json.dumps(cached_data["page_mapping"]),
                        ContentType='application/json'
                    )
                except:
                    pass
                
                return cached_data["full_text"], cached_data["total_pages"], cached_data["accounts"]
            except:
                pass  # Cache miss, proceed with processing
            
            # Step 1: OCR Extraction
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
            
            import fitz
            pdf_doc = fitz.open(pdf_path)
            total_pages = len(pdf_doc)
            pdf_doc.close()
            
            # Extract text using existing function
            full_text, _ = extract_text_with_textract(pdf_bytes, os.path.basename(pdf_path))
            
            # Fallback to PyPDF if OCR fails
            if not full_text or len(full_text.strip()) < 100:
                full_text, _ = try_extract_pdf_with_pypdf(pdf_bytes, os.path.basename(pdf_path))
            
            print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: OCR completed ({total_pages} pages, {len(full_text)} chars)")
            
            # Step 2: Account Detection
            print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Starting account detection...")
            
            loan_result = process_loan_document(full_text)
            
            if not loan_result or "documents" not in loan_result:
                print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: No accounts found in {doc_id}")
                accounts = []
                page_mapping = {}
            else:
                raw_accounts = loan_result["documents"][0].get("accounts", [])
                accounts = normalize_and_merge_accounts(raw_accounts)
                print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Found {len(accounts)} accounts")
                
                # Step 3: IMMEDIATE Page Mapping (while we have the OCR data)
                print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Creating page mapping from OCR data...")
                page_mapping = self._create_page_mapping_from_ocr(doc_id, pdf_path, accounts, total_pages)
                print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Page mapping completed ({len(page_mapping)} pages mapped)")
            
            # Cache EVERYTHING together for maximum speed
            cache_data = {
                "full_text": full_text,
                "total_pages": total_pages,
                "accounts": accounts,
                "page_mapping": page_mapping,
                "processing_time": time.time(),
                "cache_version": "ultra_v1"
            }
            
            try:
                # Cache the complete result
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                
                # Also cache page mapping separately for API access
                page_mapping_key = f"page_mapping/{doc_id}/mapping.json"
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=page_mapping_key,
                    Body=json.dumps(page_mapping),
                    ContentType='application/json'
                )
                
                print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Cached complete result + page mapping to S3")
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ö†Ô∏è ULTRA-FAST: Failed to cache: {str(e)}")
            
            return full_text, total_pages, accounts
            
        except Exception as e:
            print(f"[BG_PROCESSOR] üöÄ ULTRA-FAST: Processing failed for {doc_id}: {str(e)}")
            raise
    
    def _create_page_mapping_from_ocr(self, doc_id: str, pdf_path: str, accounts: List[Dict], total_pages: int) -> Dict[int, str]:
        """Create page mapping using SMART OCR (reuse existing OCR data when possible)"""
        import fitz
        import re
        
        page_mapping = {}
        
        # Check if we already have OCR cache from the main OCR process
        ocr_cache_key = f"ocr_cache/{doc_id}/text_cache.json"
        existing_ocr = {}
        
        try:
            cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=ocr_cache_key)
            existing_ocr = json.loads(cached_result['Body'].read())
            print(f"[ULTRA_PAGE_MAP] ‚úÖ Found existing OCR cache for {len(existing_ocr)} pages")
        except:
            print(f"[ULTRA_PAGE_MAP] No existing OCR cache, will extract as needed")
        
        # Process pages efficiently
        for page_num in range(total_pages):
            try:
                # Use cached OCR if available, otherwise extract
                if page_num in existing_ocr:
                    page_text = existing_ocr[page_num]
                    print(f"[ULTRA_PAGE_MAP] üìã Page {page_num + 1}: Using cached OCR")
                else:
                    # Extract text from this page
                    pdf_doc = fitz.open(pdf_path)
                    page = pdf_doc[page_num]
                    page_text = page.get_text()
                    pdf_doc.close()
                    
                    # If no text or very little, skip OCR for now (we'll handle it later if needed)
                    if not page_text or len(page_text.strip()) < 20:
                        print(f"[ULTRA_PAGE_MAP] ‚è≠Ô∏è Page {page_num + 1}: Skipping (little text)")
                        continue
                    
                    # Cache this OCR result
                    existing_ocr[page_num] = page_text
                
                # Check for account numbers on this page
                for acc in accounts:
                    acc_num = acc.get("accountNumber", "").strip()
                    if not acc_num:
                        continue
                    
                    # Quick exact match
                    normalized_text = re.sub(r'[\s\-\.]', '', page_text)
                    normalized_acc = re.sub(r'[\s\-\.]', '', acc_num)
                    
                    if normalized_acc in normalized_text:
                        page_mapping[page_num] = acc_num
                        print(f"[ULTRA_PAGE_MAP] ‚úÖ Page {page_num + 1} -> Account {acc_num}")
                        break
                        
            except Exception as e:
                print(f"[ULTRA_PAGE_MAP] ‚ùå Error processing page {page_num + 1}: {str(e)}")
        
        # Update OCR cache with any new extractions
        try:
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=ocr_cache_key,
                Body=json.dumps(existing_ocr),
                ContentType='application/json'
            )
            print(f"[ULTRA_PAGE_MAP] üíæ Updated OCR cache with {len(existing_ocr)} pages")
        except Exception as e:
            print(f"[ULTRA_PAGE_MAP] ‚ö†Ô∏è Failed to update OCR cache: {str(e)}")
        
        return page_mapping
    
    def _stage_cost_optimized_processing(self, doc_id: str, page_ocr_results: Dict[int, str], total_pages: int, doc_type: str = "loan_document") -> Tuple[List[Dict], Dict[int, str]]:
        """COST OPTIMIZED: Combined account discovery + data extraction in single LLM call per page"""
        print(f"[BG_PROCESSOR] üí∞ COST-OPTS: Starting cost-optimized processing for {total_pages} pages...")
        print(f"[BG_PROCESSOR] üí∞ COST-OPT: Each page = 1 LLM call (account detection + data extraction)")
        
        try:
            # Use the updated regex-based account detection + LLM data extraction
            from app.services.regex_account_detector import RegexAccountDetector
            
            # Step 1: Account detection using BOUNDARY LOGIC (account XYZ owns all pages until account ABC is found)
            detector = RegexAccountDetector()
            
            # First, find all account numbers in the document
            all_account_numbers = set()
            for page_num in sorted(page_ocr_results.keys()):
                page_text = page_ocr_results[page_num]
                page_accounts = detector.extract_accounts_from_text(page_text)
                all_account_numbers.update(page_accounts)
            
            print(f"[BG_PROCESSOR] üîç Found account numbers: {list(all_account_numbers)}")
            
            # Step 1.1: Create page mapping using BOUNDARY LOGIC
            page_mapping_0_based = {}
            current_account = None
            
            for page_num in sorted(page_ocr_results.keys()):
                page_text = page_ocr_results[page_num]
                
                # Check if this page contains a new account number
                found_new_account = None
                page_accounts = detector.extract_accounts_from_text(page_text)
                
                if page_accounts:
                    # Use the first account found on this page
                    found_new_account = page_accounts[0]
                    print(f"[BG_PROCESSOR] üó∫Ô∏è Page {page_num + 1}: Found account {found_new_account}")
                
                # Update current account if we found a new one
                if found_new_account and found_new_account != current_account:
                    current_account = found_new_account
                    print(f"[BG_PROCESSOR] üó∫Ô∏è Page {page_num + 1} - NEW ACCOUNT BOUNDARY: {current_account}")
                
                # Assign page to current account (BOUNDARY LOGIC)
                if current_account:
                    page_mapping_0_based[page_num] = current_account
                    print(f"[BG_PROCESSOR] üó∫Ô∏è Page {page_num + 1} -> Account {current_account}")
                else:
                    print(f"[BG_PROCESSOR] üó∫Ô∏è Page {page_num + 1} - No account assigned (cover page)")
            
            # Step 1.2: Group pages by account using boundary logic
            all_accounts = {}
            for page_num, account_num in page_mapping_0_based.items():
                if account_num not in all_accounts:
                    all_accounts[account_num] = {'pages': [], 'page_texts': {}}
                all_accounts[account_num]['pages'].append(page_num + 1)  # Convert to 1-based for processing
                all_accounts[account_num]['page_texts'][page_num + 1] = page_ocr_results[page_num]
            
            print(f"[BG_PROCESSOR] üó∫Ô∏è BOUNDARY MAPPING: {len(all_accounts)} accounts with pages:")
            for acc_num, acc_info in all_accounts.items():
                print(f"   Account {acc_num}: pages {acc_info['pages']}")
            
            # Step 2: LLM processing with BATCH + PARALLEL optimization
            processor = CostOptimizedProcessor(
                bedrock_client=bedrock,
                s3_client=s3_client,
                bucket_name=S3_BUCKET,
                doc_type=doc_type  # Pass document type for appropriate prompt selection
            )
            
            # NEW: Batch Page Processing + Parallel LLM Calls
            accounts = []
            total_accounts = len(all_accounts)
            accounts_processed = 0
            
            print(f"[BG_PROCESSOR] üöÄ BATCH+PARALLEL: Processing {total_accounts} accounts with batch processing and parallel LLM calls")
            
            for account_num, account_info in all_accounts.items():
                account_pages = account_info['pages']
                account_page_texts = account_info['page_texts']
                accounts_processed += 1
                
                # Update progress
                progress_pct = int((accounts_processed / total_accounts) * 100)
                self._update_stage_status(doc_id, DocumentProcessingStage.LLM_EXTRACTION, "processing", progress_pct)
                
                print(f"[BG_PROCESSOR] ü§ñ Account {accounts_processed}/{total_accounts}: {account_num} ({len(account_pages)} pages)")
                
                # Use new BATCH + PARALLEL processing
                merged_result = processor.process_batches_parallel(
                    account_number=account_num,
                    page_texts=account_page_texts,
                    pages=account_pages,
                    batch_size=2,      # 2-3 pages per LLM call
                    max_workers=3      # 3-5 concurrent LLM calls
                )
                
                if merged_result:
                    accounts.append(merged_result)
                    print(f"[BG_PROCESSOR] ‚úÖ Account {account_num}: {len(merged_result.get('result', {}))} fields extracted")
            
            print(f"[BG_PROCESSOR] üí∞ BATCH+PARALLEL: ‚úÖ Completed - {len(accounts)} accounts processed, {len(page_mapping_0_based)} pages mapped")
            print(f"[BG_PROCESSOR] üìä OPTIMIZATION: Reduced LLM calls by 50% (batch processing) + 80% faster (parallel)")
            
            # PHASE 2: Batch cache all results to S3 in parallel (5x faster)
            if accounts:
                print(f"[BG_PROCESSOR] üì¶ PHASE 2: Starting BATCH S3 CACHING for {len(accounts)} accounts...")
                processor.batch_cache_results_to_s3(accounts, doc_id, s3_client, S3_BUCKET)
                print(f"[BG_PROCESSOR] ‚úÖ PHASE 2: BATCH S3 CACHING completed (5x faster with parallel uploads)")
            
            return accounts, page_mapping_0_based
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ùå COST-OPT: Failed for {doc_id}: {str(e)}")
            return [], {}
    
    def _create_page_mapping_from_ocr_results(self, doc_id: str, page_ocr_results: Dict[int, str], accounts: List[Dict], total_pages: int) -> Dict[int, str]:
        """Create page mapping using user's requested logic: account xyz owns all pages until account abc is found"""
        import re
        
        print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Creating smart page mapping for {total_pages} pages using account boundary logic...")
        
        page_mapping = {}
        current_account = None
        
        # Extract account numbers for easier matching
        account_numbers = [acc.get("accountNumber", "").strip() for acc in accounts if acc.get("accountNumber")]
        print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Looking for account numbers: {account_numbers}")
        
        # Process pages in order to find account boundaries
        for page_num in range(total_pages):
            page_text = page_ocr_results.get(page_num, "")
            
            if not page_text or len(page_text.strip()) < 10:
                # Empty page - assign to current account if we have one
                if current_account:
                    page_mapping[page_num] = current_account
                    print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Page {page_num + 1} (empty) -> Account {current_account} (inherited)")
                continue
            
            # Check if this page contains a new account number
            found_new_account = None
            for acc_num in account_numbers:
                if not acc_num:
                    continue
                
                # Normalize text and account number for matching
                normalized_text = re.sub(r'[\s\-\.]', '', page_text.upper())
                normalized_acc = re.sub(r'[\s\-\.]', '', acc_num.upper())
                
                # Check for exact match
                if normalized_acc in normalized_text:
                    # Only switch accounts if this is a different account
                    if found_new_account is None or len(acc_num) > len(found_new_account):
                        found_new_account = acc_num
            
            # Update current account if we found a new one
            if found_new_account and found_new_account != current_account:
                current_account = found_new_account
                print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Page {page_num + 1} - NEW ACCOUNT BOUNDARY: {current_account}")
            
            # Assign page to current account
            if current_account:
                page_mapping[page_num] = current_account
                print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Page {page_num + 1} -> Account {current_account}")
            else:
                # No account found yet - this could be a cover page or document without account info
                print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Page {page_num + 1} - No account assigned (cover page or general document)")
        
        print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Completed - {len(page_mapping)} pages mapped to accounts")
        
        # Cache the page mapping separately for API access
        try:
            mapping_cache_key = f"page_mapping/{doc_id}/mapping.json"
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=mapping_cache_key,
                Body=json.dumps(page_mapping),
                ContentType='application/json'
            )
            print(f"[BG_PROCESSOR] üó∫Ô∏è PAGE MAPPING: Cached page mapping to S3 for API access")
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ö†Ô∏è PAGE MAPPING: Failed to cache page mapping: {str(e)}")
        
        return page_mapping
    
    def _stage_account_splitting(self, doc_id: str, full_text: str) -> List[Dict]:
        """Stage 2: Split document into accounts (LEGACY - kept for compatibility)"""
        print(f"[BG_PROCESSOR] üîç ACCOUNTS: Analyzing document text to identify accounts...")
        
        try:
            # Check if already cached
            cache_key = f"account_cache/{doc_id}/accounts.json"
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                print(f"[BG_PROCESSOR] üîç ACCOUNTS: Using cached split ({len(cached_data['accounts'])} accounts)")
                return cached_data["accounts"]
            except:
                pass  # Cache miss, proceed with splitting
            
            # Process with loan processor
            loan_result = process_loan_document(full_text)
            
            if not loan_result or "documents" not in loan_result:
                print(f"[BG_PROCESSOR] No accounts found in {doc_id}")
                return []
            
            raw_accounts = loan_result["documents"][0].get("accounts", [])
            
            # Normalize and merge duplicate accounts (e.g., "0000927800" and "927800")
            accounts = normalize_and_merge_accounts(raw_accounts)
            print(f"[BG_PROCESSOR] Account normalization: {len(raw_accounts)} -> {len(accounts)} accounts")
            
            # Cache the result
            cache_data = {
                "accounts": accounts,
                "split_time": time.time(),
                "total_accounts": len(accounts)
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                print(f"[BG_PROCESSOR] üîç ACCOUNTS: Cached {len(accounts)} accounts to S3")
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ö†Ô∏è  ACCOUNTS: Failed to cache: {str(e)}")
            
            return accounts
            
        except Exception as e:
            print(f"[BG_PROCESSOR] Account splitting failed for {doc_id}: {str(e)}")
            return []
    
    def _stage_page_analysis(self, doc_id: str, pdf_path: str, accounts: List[Dict], total_pages: int) -> Dict[int, str]:
        """Stage 3: Analyze pages to map them to accounts"""
        print(f"[BG_PROCESSOR] Stage 3: Page analysis for {doc_id} ({total_pages} pages)")
        
        try:
            # Check if already cached
            cache_key = f"page_mapping/{doc_id}/mapping.json"
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                page_mapping = {int(k): v for k, v in cached_data["page_mapping"].items()}
                print(f"[BG_PROCESSOR] ‚úì Using cached page mapping for {doc_id}")
                return page_mapping
            except:
                pass  # Cache miss, proceed with analysis
            
            # Use existing scan_and_map_pages function
            page_mapping = scan_and_map_pages(doc_id, pdf_path, accounts)
            
            # Cache the result
            cache_data = {
                "page_mapping": page_mapping,
                "analysis_time": time.time(),
                "total_pages": total_pages
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                print(f"[BG_PROCESSOR] ‚úì Cached page mapping for {doc_id}")
            except Exception as e:
                print(f"[BG_PROCESSOR] Warning: Failed to cache page mapping: {str(e)}")
            
            return page_mapping
            
        except Exception as e:
            print(f"[BG_PROCESSOR] Page analysis failed for {doc_id}: {str(e)}")
            return {}
    
    def _stage_llm_extraction_from_cached_ocr(self, doc_id: str, page_ocr_results: Dict[int, str], 
                                            accounts: List[Dict], page_mapping: Dict[int, str], total_pages: int):
        """NEW: Stage 4 - LLM extraction using cached OCR results (for loan documents)"""
        print(f"[BG_PROCESSOR] ü§ñ LLM FROM CACHED OCR: Starting extraction for {total_pages} pages using cached OCR...")
        
        pages_processed = 0
        pages_cached = 0
        pages_queued = 0
        
        for page_num in range(total_pages):
            try:
                # Check if page is already cached
                cache_key = f"page_data/{doc_id}/page_{page_num}.json"
                
                try:
                    cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                    # Page already processed and cached
                    pages_processed += 1
                    pages_cached += 1
                    print(f"[BG_PROCESSOR] ü§ñ LLM FROM CACHED OCR: Page {page_num + 1}/{total_pages} already cached ‚úì")
                    self._update_extraction_progress(doc_id, pages_processed, total_pages)
                    continue
                except:
                    pass  # Not cached, need to process
                
                # Submit page for processing using cached OCR
                print(f"[BG_PROCESSOR] ü§ñ LLM FROM CACHED OCR: Queuing page {page_num + 1}/{total_pages} for extraction...")
                future = self.executor.submit(self._process_single_page_from_cached_ocr, doc_id, page_num, page_ocr_results, page_mapping)
                
                # Don't wait for completion - let it run in background
                pages_processed += 1
                pages_queued += 1
                self._update_extraction_progress(doc_id, pages_processed, total_pages)
                
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ùå LLM FROM CACHED OCR: Failed to queue page {page_num + 1}: {str(e)}")
        
        print(f"[BG_PROCESSOR] ü§ñ LLM FROM CACHED OCR: Processing summary - {pages_cached} cached, {pages_queued} queued for extraction")
    
    def _stage_llm_extraction_death_certificate(self, doc_id: str, page_ocr_results: Dict[int, str], total_pages: int):
        """NEW: Stage 4 - Page-by-page LLM extraction for death certificates (each page gets unique processing)"""
        print(f"[BG_PROCESSOR] ü§ñ DEATH CERT LLM: Starting page-by-page extraction for {total_pages} pages...")
        
        pages_processed = 0
        pages_cached = 0
        pages_queued = 0
        
        for page_num in range(total_pages):
            try:
                # Use unique cache key for death certificate pages to avoid same-result issue
                cache_key = f"death_cert_page_data/{doc_id}/page_{page_num}.json"
                
                try:
                    cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                    # Page already processed and cached
                    pages_processed += 1
                    pages_cached += 1
                    print(f"[BG_PROCESSOR] ü§ñ DEATH CERT LLM: Page {page_num + 1}/{total_pages} already cached ‚úì")
                    self._update_extraction_progress(doc_id, pages_processed, total_pages)
                    continue
                except:
                    pass  # Not cached, need to process
                
                # Submit page for processing with death certificate specific logic
                print(f"[BG_PROCESSOR] ü§ñ DEATH CERT LLM: Queuing page {page_num + 1}/{total_pages} for death certificate extraction...")
                future = self.executor.submit(self._process_death_certificate_page, doc_id, page_num, page_ocr_results)
                
                # Don't wait for completion - let it run in background
                pages_processed += 1
                pages_queued += 1
                self._update_extraction_progress(doc_id, pages_processed, total_pages)
                
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ùå DEATH CERT LLM: Failed to queue page {page_num + 1}: {str(e)}")
        
        print(f"[BG_PROCESSOR] ü§ñ DEATH CERT LLM: Processing summary - {pages_cached} cached, {pages_queued} queued for extraction")
    
    def _stage_llm_extraction(self, doc_id: str, pdf_path: str, accounts: List[Dict], 
                             page_mapping: Dict[int, str], total_pages: int):
        """Stage 4: Extract data from pages using LLM (parallel processing) - LEGACY"""
        print(f"[BG_PROCESSOR] ü§ñ LLM: Starting extraction for {total_pages} pages...")
        
        # Process pages in parallel batches
        batch_size = 3  # Process 3 pages at a time
        pages_processed = 0
        pages_cached = 0
        pages_queued = 0
        
        for page_num in range(total_pages):
            try:
                # Check if page is already cached
                cache_key = f"page_data/{doc_id}/page_{page_num}.json"
                
                try:
                    cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                    # Page already processed and cached
                    pages_processed += 1
                    pages_cached += 1
                    print(f"[BG_PROCESSOR] ü§ñ LLM: Page {page_num + 1}/{total_pages} already cached ‚úì")
                    self._update_extraction_progress(doc_id, pages_processed, total_pages)
                    continue
                except:
                    pass  # Not cached, need to process
                
                # Submit page for processing
                print(f"[BG_PROCESSOR] ü§ñ LLM: Queuing page {page_num + 1}/{total_pages} for extraction...")
                future = self.executor.submit(self._process_single_page, doc_id, pdf_path, page_num, page_mapping)
                
                # Don't wait for completion - let it run in background
                pages_processed += 1
                pages_queued += 1
                self._update_extraction_progress(doc_id, pages_processed, total_pages)
                
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ùå LLM: Failed to queue page {page_num + 1}: {str(e)}")
        
        print(f"[BG_PROCESSOR] ü§ñ LLM: Processing summary - {pages_cached} cached, {pages_queued} queued for extraction")
    
    def _process_single_page_from_cached_ocr(self, doc_id: str, page_num: int, page_ocr_results: Dict[int, str], page_mapping: Dict[int, str]):
        """COST OPTIMIZED: Process page once for BOTH account detection AND data extraction"""
        try:
            cache_key = f"page_data/{doc_id}/page_{page_num}.json"
            
            # Check cache again (race condition protection)
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                return  # Already processed
            except:
                pass
            
            # Get cached OCR text for this page
            page_text = page_ocr_results.get(page_num, "")
            
            if not page_text or len(page_text.strip()) < 10:
                print(f"[BG_PROCESSOR] üí∞ COST-OPT: Page {page_num + 1} has no OCR text, skipping...")
                return
            
            # COST OPTIMIZATION: Single LLM call extracts BOTH account numbers AND data
            print(f"[BG_PROCESSOR] üí∞ COST-OPT: Page {page_num + 1} - Single LLM call for account + data extraction...")
            
            # Extract both account numbers and data in one call
            llm_result = self._extract_with_llm(page_text, "", doc_id=doc_id)
            
            # Parse the dual-purpose response
            account_numbers_found = llm_result.get("account_numbers_found", [])
            extracted_data = llm_result.get("extracted_fields", {})
            
            # Determine the account for this page
            if account_numbers_found:
                # Use the first account number found on this page
                page_account = account_numbers_found[0]
                print(f"[BG_PROCESSOR] üí∞ COST-OPT: Page {page_num + 1} found account: {page_account}")
            else:
                # Fall back to page mapping or inherit from previous page
                page_account = page_mapping.get(page_num, "Unknown")
                print(f"[BG_PROCESSOR] üí∞ COST-OPT: Page {page_num + 1} using mapped account: {page_account}")
            
            # Cache the result with account info
            cache_data = {
                "extracted_data": extracted_data,
                "account_numbers_found": account_numbers_found,
                "page_account": page_account,
                "page_text": page_text[:500],  # Store preview
                "extraction_time": time.time(),
                "cache_version": "cost_optimized_v1"
            }
            
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
            
            print(f"[BG_PROCESSOR] üí∞ COST-OPT: ‚úÖ Page {page_num + 1} processed with single LLM call (account: {page_account}, {len(extracted_data)} fields)")
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ü§ñ CACHED OCR LLM: ‚ùå Failed to process page {page_num + 1}: {str(e)}")
    
    def _process_death_certificate_page(self, doc_id: str, page_num: int, page_ocr_results: Dict[int, str]):
        """NEW: Process a single death certificate page with unique caching (fixes same-result issue)"""
        try:
            # Use unique cache key for death certificate pages
            cache_key = f"death_cert_page_data/{doc_id}/page_{page_num}.json"
            
            # Check cache again (race condition protection)
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                return  # Already processed
            except:
                pass
            
            # Get cached OCR text for this page
            page_text = page_ocr_results.get(page_num, "")
            
            if not page_text or len(page_text.strip()) < 10:
                print(f"[BG_PROCESSOR] ü§ñ DEATH CERT: Page {page_num + 1} has no OCR text, skipping...")
                return
            
            # Extract data using LLM with death certificate specific prompt
            extracted_data = self._extract_with_llm(page_text, "N/A", get_comprehensive_extraction_prompt(), doc_id=doc_id)
            
            # Cache the result with unique key structure
            cache_data = {
                "extracted_data": extracted_data,
                "page_text": page_text[:500],  # Store preview
                "page_number": page_num,
                "document_type": "death_certificate",
                "extraction_time": time.time(),
                "cache_version": "death_cert_v1"
            }
            
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
            
            print(f"[BG_PROCESSOR] ü§ñ DEATH CERT: ‚úÖ Processed and cached page {page_num + 1} with unique data")
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ü§ñ DEATH CERT: ‚ùå Failed to process page {page_num + 1}: {str(e)}")
    
    def _process_single_page(self, doc_id: str, pdf_path: str, page_num: int, page_mapping: Dict[int, str]):
        """Process a single page with LLM extraction - LEGACY"""
        try:
            cache_key = f"page_data/{doc_id}/page_{page_num}.json"
            
            # Check cache again (race condition protection)
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                return  # Already processed
            except:
                pass
            
            # Extract page text
            import fitz
            pdf_doc = fitz.open(pdf_path)
            page = pdf_doc[page_num]
            page_text = page.get_text()
            pdf_doc.close()
            
            # If insufficient text, use OCR
            if len(page_text.strip()) < 50:
                page_text = self._ocr_single_page(pdf_path, page_num)
            
            # Get account for this page
            account_number = page_mapping.get(page_num, "Unknown")
            
            # Extract data using LLM
            extracted_data = self._extract_with_llm(page_text, account_number, doc_id=doc_id)
            
            # Cache the result
            cache_data = {
                "extracted_data": extracted_data,
                "page_text": page_text[:500],  # Store preview
                "account_number": account_number,
                "extraction_time": time.time(),
                "cache_version": "v2"
            }
            
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
            
            print(f"[BG_PROCESSOR] ü§ñ LLM: ‚úÖ Processed and cached page {page_num + 1} (account: {account_number})")
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ü§ñ LLM: ‚ùå Failed to process page {page_num + 1}: {str(e)}")
    
    def _ocr_single_page(self, pdf_path: str, page_num: int) -> str:
        """OCR a single page"""
        try:
            import fitz
            pdf_doc = fitz.open(pdf_path)
            page = pdf_doc[page_num]
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            pdf_doc.close()
            
            temp_image_path = f"{OUTPUT_DIR}/temp_ocr_{page_num}_{int(time.time())}.png"
            pix.save(temp_image_path)
            
            with open(temp_image_path, 'rb') as f:
                image_bytes = f.read()
            
            # Use Textract
            response = textract.detect_document_text(Document={'Bytes': image_bytes})
            
            ocr_text = ""
            for block in response.get('Blocks', []):
                if block['BlockType'] == 'LINE':
                    ocr_text += block.get('Text', '') + "\n"
            
            # Clean up
            if os.path.exists(temp_image_path):
                os.remove(temp_image_path)
            
            return ocr_text
            
        except Exception as e:
            print(f"[BG_PROCESSOR] OCR failed for page {page_num}: {str(e)}")
            return ""
    
    def _extract_with_llm(self, page_text: str, account_number: str, custom_prompt: str = None, doc_id: str = None) -> Dict:
        """Extract data from page text using LLM - COST OPTIMIZED VERSION"""
        try:
            # Use custom prompt if provided, otherwise use loan document prompt
            if custom_prompt:
                prompt = custom_prompt
            else:
                # Use specialized loan document prompt for data extraction
                prompt = get_loan_document_prompt()
            
            # Prepare request body
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 8192,
                "messages": [
                    {
                        "role": "user",
                        "content": f"{prompt}\n\nDocument text:\n{page_text[:8000]}"
                    }
                ]
            }
            
            # Call Bedrock
            response = bedrock.invoke_model(
                modelId=MODEL_ID,
                body=json.dumps(request_body)
            )
            
            response_body = json.loads(response['body'].read())
            llm_response = response_body['content'][0]['text']
            
            # Track Bedrock cost if doc_id is provided
            if doc_id:
                try:
                    input_tokens = response_body.get('usage', {}).get('input_tokens', 0)
                    output_tokens = response_body.get('usage', {}).get('output_tokens', 0)
                    
                    if input_tokens > 0 or output_tokens > 0:
                        cost_tracker = get_cost_tracker(doc_id)
                        cost_tracker.track_bedrock_call(input_tokens, output_tokens)
                except Exception as e:
                    print(f"[COST] Failed to track Bedrock cost: {str(e)}")
            
            # Parse JSON response
            json_start = llm_response.find('{')
            json_end = llm_response.rfind('}')
            
            if json_start != -1 and json_end != -1:
                json_str = llm_response[json_start:json_end + 1]
                extracted_data = json.loads(json_str)
                
                # For non-page extraction (full document), flatten the response
                if custom_prompt and "documents" in extracted_data:
                    # Extract the first document's extracted_fields
                    docs = extracted_data.get("documents", [])
                    if docs and len(docs) > 0:
                        return docs[0].get("extracted_fields", {})
                
                return extracted_data
            else:
                return {"error": "No valid JSON in LLM response"}
                
        except Exception as e:
            print(f"[BG_PROCESSOR] LLM extraction failed: {str(e)}")
            return {"error": str(e)}
    

    
    def _update_stage_status(self, doc_id: str, stage: str, status: str, progress: int):
        """Update the status of a processing stage"""
        if doc_id in self.stage_progress:
            self.stage_progress[doc_id][stage] = {
                "status": status,
                "progress": progress,
                "timestamp": time.time()
            }
        
        # Also update document_status progress
        if doc_id in self.document_status:
            self.document_status[doc_id]["progress"] = progress
    
    def _update_extraction_progress(self, doc_id: str, pages_processed: int, total_pages: int):
        """Update LLM extraction progress"""
        progress = int((pages_processed / total_pages) * 100) if total_pages > 0 else 0
        self._update_stage_status(doc_id, DocumentProcessingStage.LLM_EXTRACTION, "processing", progress)
        
        if doc_id in self.document_status:
            self.document_status[doc_id]["pages_processed"] = pages_processed
    
    def get_document_status(self, doc_id: str) -> Optional[Dict]:
        """Get processing status for a document"""
        if doc_id not in self.document_status:
            return None
        
        status = self.document_status[doc_id].copy()
        status["stages"] = self.stage_progress.get(doc_id, {})
        return status
    
    def is_page_cached(self, doc_id: str, page_num: int) -> bool:
        """Check if a page is already processed and cached"""
        cache_key = f"page_data/{doc_id}/page_{page_num}.json"
        try:
            s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            return True
        except:
            return False
    
    def get_cached_page_data(self, doc_id: str, page_num: int) -> Optional[Dict]:
        """Get cached page data if available"""
        cache_key = f"page_data/{doc_id}/page_{page_num}.json"
        try:
            cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = json.loads(cached_result['Body'].read())
            return cached_data
        except:
            return None
    
    def _get_document_type(self, doc_id: str) -> str:
        """Get document type from the main document record"""
        try:
            global processed_documents
            doc = next((d for d in processed_documents if d["id"] == doc_id), None)
            if doc:
                return doc.get("document_type_info", {}).get("type", "unknown")
            return "unknown"
        except Exception as e:
            print(f"[BG_PROCESSOR] Error getting document type for {doc_id}: {str(e)}")
            return "unknown"
    
    def _stage_direct_llm_extraction(self, doc_id: str, full_text: str, doc_type: str) -> Dict:
        """Stage 4 (Alternative): Direct LLM extraction for non-loan documents"""
        print(f"[BG_PROCESSOR] ü§ñ DIRECT LLM: Extracting fields from {doc_type} document ({len(full_text)} chars)...")
        
        try:
            # Check if already cached
            cache_key = f"document_extraction_cache/{doc_id}/full_extraction.json"
            try:
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                print(f"[BG_PROCESSOR] ü§ñ DIRECT LLM: Using cached extraction ({len(cached_data['extracted_fields'])} fields)")
                return cached_data["extracted_fields"]
            except:
                pass  # Cache miss, proceed with extraction
            
            # Get appropriate prompt for document type
            if doc_type == "death_certificate":
                prompt = get_comprehensive_extraction_prompt()  # Use comprehensive prompt for death certificates
            else:
                prompt = get_comprehensive_extraction_prompt()  # Use comprehensive prompt for all document types
            
            # Extract data using LLM
            extracted_fields = self._extract_with_llm(full_text, "N/A", prompt, doc_id=doc_id)
            
            # Cache the result
            cache_data = {
                "extracted_fields": extracted_fields,
                "extraction_time": time.time(),
                "document_type": doc_type,
                "cache_version": "v2"
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                print(f"[BG_PROCESSOR] ü§ñ DIRECT LLM: Cached {len(extracted_fields)} fields to S3")
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ö†Ô∏è  DIRECT LLM: Failed to cache: {str(e)}")
            
            return extracted_fields
            
        except Exception as e:
            print(f"[BG_PROCESSOR] Direct LLM extraction failed for {doc_id}: {str(e)}")
            return {"error": str(e)}
    
    def _update_main_document_record(self, doc_id: str, accounts: List[Dict], total_pages: int, doc_type: str = "loan_document", extracted_fields: Dict = None):
        """Update the main document record with background processing results"""
        try:
            global processed_documents
            
            # Find the document in the main list
            doc_index = None
            for i, doc in enumerate(processed_documents):
                if doc["id"] == doc_id:
                    doc_index = i
                    break
            
            if doc_index is None:
                print(f"[BG_PROCESSOR] ‚ùå Document {doc_id} not found in processed_documents")
                return
            
            doc = processed_documents[doc_index]
            
            # Update the document with background processing results based on document type
            if doc_type == "loan_document" and len(accounts) > 0:
                # Update loan document with accounts
                if doc["documents"] and len(doc["documents"]) > 0:
                    doc["documents"][0].update({
                        "accounts": accounts,
                        "extracted_fields": {
                            "total_accounts": len(accounts),
                            "accounts_processed": len(accounts),
                            "processing_method": "Background processing completed"
                        },
                        "accuracy_score": 95,  # High score for background processing
                        "filled_fields": len(accounts) * 5,  # Estimate based on accounts
                        "total_fields": len(accounts) * 10,  # Estimate based on accounts
                        "needs_human_review": False,
                        "optimized": True,
                        "background_processed": True,
                        "processing_completed_at": datetime.now().isoformat()
                    })
                
                print(f"[BG_PROCESSOR] üíæ DATABASE: Updated loan document with {len(accounts)} accounts and processing metadata")
                
            elif extracted_fields and not extracted_fields.get("error"):
                # Update regular document (death certificate, etc.) with extracted fields
                if doc["documents"] and len(doc["documents"]) > 0:
                    # Calculate field statistics
                    filled_fields = sum(1 for v in extracted_fields.values() 
                                      if v and str(v).strip() and str(v) != "N/A" and not isinstance(v, dict))
                    total_fields = len(extracted_fields)
                    accuracy_score = int((filled_fields / total_fields) * 100) if total_fields > 0 else 0
                    
                    doc["documents"][0].update({
                        "extracted_fields": extracted_fields,
                        "accuracy_score": accuracy_score,
                        "filled_fields": filled_fields,
                        "total_fields": total_fields,
                        "needs_human_review": accuracy_score < 90,
                        "optimized": True,
                        "background_processed": True,
                        "processing_completed_at": datetime.now().isoformat()
                    })
                
                print(f"[BG_PROCESSOR] üíæ DATABASE: Updated {doc_type} document with {len(extracted_fields)} extracted fields and processing metadata")
            
            # Get and save cost information
            try:
                cost_tracker = get_cost_tracker(doc_id)
                cost_summary = cost_tracker.get_summary()
                doc["processing_cost"] = cost_summary
                print(f"[BG_PROCESSOR] üí∞ COST: Saved cost data - Total: ${cost_summary['total_cost']:.6f}")
            except Exception as e:
                print(f"[BG_PROCESSOR] ‚ö†Ô∏è Failed to save cost data: {str(e)}")
            
            # Update document status
            doc.update({
                "status": "completed",
                "background_processing_completed": True,
                "total_pages": total_pages,
                "processing_completion_time": datetime.now().isoformat()
            })
            
            # Save updated document list
            save_documents_db(processed_documents)
            
            print(f"[BG_PROCESSOR] üíæ DATABASE: Saved updated document to persistent storage")
            print(f"[BG_PROCESSOR] üéØ SUMMARY: Document {doc_id} ({doc_type}) fully processed and ready for UI access")
            
        except Exception as e:
            print(f"[BG_PROCESSOR] ‚ùå Failed to update main document record for {doc_id}: {str(e)}")
            import traceback
            traceback.print_exc()

def normalize_and_merge_accounts(accounts: List[Dict]) -> List[Dict]:
    """
    Normalize account numbers and merge duplicates that differ only by leading zeros
    Example: "0000927800" and "927800" should be treated as the same account
    """
    if not accounts:
        return accounts
    
    print(f"[ACCOUNT_MERGE] Processing {len(accounts)} accounts for duplicate normalization...")
    
    # Group accounts by normalized account number (without leading zeros)
    normalized_groups = {}
    
    for account in accounts:
        acc_num = account.get("accountNumber", "")
        if not acc_num:
            continue
        
        # Normalize by removing leading zeros
        normalized = acc_num.lstrip('0') or '0'  # Keep at least one zero if all zeros
        
        if normalized not in normalized_groups:
            normalized_groups[normalized] = []
        
        normalized_groups[normalized].append(account)
        print(f"[ACCOUNT_MERGE] Account {acc_num} -> normalized: {normalized}")
    
    # Merge duplicate accounts
    merged_accounts = []
    
    for normalized, group in normalized_groups.items():
        if len(group) == 1:
            # No duplicates, keep as is
            merged_accounts.append(group[0])
            print(f"[ACCOUNT_MERGE] Account {normalized}: no duplicates")
        else:
            # Multiple accounts with same normalized number - merge them
            print(f"[ACCOUNT_MERGE] Merging {len(group)} duplicate accounts for normalized number: {normalized}")
            
            # Choose the best account number format (prefer longer format with leading zeros)
            best_account = max(group, key=lambda acc: len(acc.get("accountNumber", "")))
            best_acc_num = best_account.get("accountNumber", "")
            
            # Merge all account data
            merged_result = {}
            all_fields = set()
            
            # Collect all fields from all duplicate accounts
            for account in group:
                result = account.get("result", {})
                if isinstance(result, dict):
                    merged_result.update(result)
                    all_fields.update(result.keys())
                
                print(f"[ACCOUNT_MERGE]   - Merging account: {account.get('accountNumber', 'N/A')}")
            
            # Create merged account using the best format
            merged_account = {
                "accountNumber": best_acc_num,
                "result": merged_result,
                "accuracy_score": best_account.get("accuracy_score"),
                "filled_fields": len([v for v in merged_result.values() if v and str(v).strip()]),
                "total_fields": len(merged_result),
                "fields_needing_review": best_account.get("fields_needing_review", []),
                "needs_human_review": best_account.get("needs_human_review", False),
                "optimized": True,
                "merged_from": [acc.get("accountNumber", "") for acc in group]  # Track what was merged
            }
            
            merged_accounts.append(merged_account)
            print(f"[ACCOUNT_MERGE] ‚úÖ Merged into account: {best_acc_num} (from: {[acc.get('accountNumber', '') for acc in group]})")
    
    print(f"[ACCOUNT_MERGE] ‚úÖ Reduced {len(accounts)} accounts to {len(merged_accounts)} unique accounts")
    return merged_accounts

# Global background processor instance
background_processor = BackgroundDocumentProcessor()

# Initialize and cleanup functions
def init_background_processor():
    """Initialize the background processor"""
    try:
        background_processor.start()
        print("[INIT] Background processor started")
    except Exception as e:
        print(f"[INIT] Failed to start background processor: {str(e)}")

def cleanup_background_processor():
    """Cleanup background processor on shutdown"""
    try:
        background_processor.stop()
        print("[CLEANUP] Background processor stopped")
    except Exception as e:
        print(f"[CLEANUP] Error stopping background processor: {str(e)}")

# Register cleanup
atexit.register(cleanup_background_processor)

app = Flask(__name__)

# AWS & Model Configuration
AWS_REGION = "us-east-1"
bedrock = boto3.client("bedrock-runtime", region_name=AWS_REGION)
textract = boto3.client("textract", region_name=AWS_REGION)
s3_client = boto3.client("s3", region_name=AWS_REGION)
MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"
S3_BUCKET = "awsidpdocs"

# Initialize OCR Cache Manager
ocr_cache_manager = OCRCacheManager(s3_client, S3_BUCKET)

# In-memory Job Tracker
job_status_map = {}

# Create output directory for OCR results
OUTPUT_DIR = "ocr_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Persistent storage for processed documents
DOCUMENTS_DB_FILE = "processed_documents.json"

def load_documents_db():
    """Load processed documents from file"""
    if os.path.exists(DOCUMENTS_DB_FILE):
        with open(DOCUMENTS_DB_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_documents_db(documents):
    """Save processed documents to file"""
    with open(DOCUMENTS_DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(documents, indent=2, fp=f)

# Load existing documents on startup
processed_documents = load_documents_db()

def find_existing_document_by_account(account_number):
    """Find existing document by account number"""
    if not account_number:
        return None
    
    # Normalize account number for comparison (remove spaces, dashes)
    normalized_search = re.sub(r'[\s\-]', '', str(account_number))
    
    for doc in processed_documents:
        # Check in basic_fields
        if doc.get("basic_fields", {}).get("account_number"):
            existing_acc = re.sub(r'[\s\-]', '', str(doc["basic_fields"]["account_number"]))
            if existing_acc == normalized_search:
                return doc
        
        # Check in documents array (for loan documents with accounts)
        for sub_doc in doc.get("documents", []):
            # Check extracted_fields
            if sub_doc.get("extracted_fields", {}).get("account_number"):
                existing_acc = re.sub(r'[\s\-]', '', str(sub_doc["extracted_fields"]["account_number"]))
                if existing_acc == normalized_search:
                    return doc
            
            # Check accounts array (for loan documents)
            for account in sub_doc.get("accounts", []):
                if account.get("accountNumber"):
                    existing_acc = re.sub(r'[\s\-]', '', str(account["accountNumber"]))
                    if existing_acc == normalized_search:
                        return doc
    
    return None

def merge_document_fields(existing_doc, new_doc):
    """Merge new document fields into existing document, tracking changes"""
    changes = []
    
    # Helper function to compare and merge fields
    def merge_fields(existing_fields, new_fields, path=""):
        field_changes = []
        for key, new_value in new_fields.items():
            # Skip empty values and metadata fields
            if new_value == "" or new_value is None:
                continue
            if key in ["total_accounts", "accounts_processed", "account_numbers"]:
                continue
                
            if key not in existing_fields:
                # New field added
                existing_fields[key] = new_value
                field_changes.append({
                    "field": f"{path}{key}",
                    "change_type": "added",
                    "new_value": new_value
                })
            elif existing_fields[key] != new_value:
                # Field value changed (skip if both are empty)
                old_value = existing_fields[key]
                if old_value == "" and new_value == "":
                    continue
                existing_fields[key] = new_value
                field_changes.append({
                    "field": f"{path}{key}",
                    "change_type": "updated",
                    "old_value": old_value,
                    "new_value": new_value
                })
        return field_changes
    
    # Merge basic_fields
    if new_doc.get("basic_fields"):
        if not existing_doc.get("basic_fields"):
            existing_doc["basic_fields"] = {}
        changes.extend(merge_fields(existing_doc["basic_fields"], new_doc["basic_fields"], "basic_fields."))
    
    # Merge documents array
    if new_doc.get("documents"):
        if not existing_doc.get("documents"):
            existing_doc["documents"] = []
        
        for new_sub_doc in new_doc["documents"]:
            # Find matching document by type
            doc_type = new_sub_doc.get("document_type")
            existing_sub_doc = next((d for d in existing_doc["documents"] if d.get("document_type") == doc_type), None)
            
            if existing_sub_doc:
                # Merge extracted_fields
                if new_sub_doc.get("extracted_fields"):
                    if not existing_sub_doc.get("extracted_fields"):
                        existing_sub_doc["extracted_fields"] = {}
                    changes.extend(merge_fields(existing_sub_doc["extracted_fields"], new_sub_doc["extracted_fields"], f"documents[{doc_type}].extracted_fields."))
                
                # Merge accounts array (for loan documents)
                if new_sub_doc.get("accounts"):
                    if not existing_sub_doc.get("accounts"):
                        existing_sub_doc["accounts"] = []
                    
                    for new_account in new_sub_doc["accounts"]:
                        acc_num = new_account.get("accountNumber")
                        existing_account = next((a for a in existing_sub_doc["accounts"] if a.get("accountNumber") == acc_num), None)
                        
                        if existing_account:
                            # Merge account result fields
                            if new_account.get("result"):
                                if not existing_account.get("result"):
                                    existing_account["result"] = {}
                                changes.extend(merge_fields(existing_account["result"], new_account["result"], f"accounts[{acc_num}].result."))
                        else:
                            # New account added
                            existing_sub_doc["accounts"].append(new_account)
                            changes.append({
                                "field": f"accounts[{acc_num}]",
                                "change_type": "added",
                                "new_value": acc_num
                            })
            else:
                # New document type added - add it to the existing document
                existing_doc["documents"].append(new_sub_doc)
                
                # Track all fields from the new document type as changes
                if new_sub_doc.get("extracted_fields"):
                    for field_name, field_value in new_sub_doc["extracted_fields"].items():
                        # Skip metadata fields
                        if field_name not in ["total_accounts", "accounts_processed", "account_numbers"]:
                            changes.append({
                                "field": f"documents[{doc_type}].extracted_fields.{field_name}",
                                "change_type": "added",
                                "new_value": field_value
                            })
    
    # Update metadata
    existing_doc["last_updated"] = datetime.now().isoformat()
    existing_doc["update_source_filename"] = new_doc.get("filename")
    existing_doc["needs_review"] = True
    existing_doc["changes"] = changes
    
    return existing_doc, changes

def _process_single_page_scan_optimized(args):
    """OPTIMIZED: Helper function to process a single page with cached OCR"""
    page_num, pdf_path, doc_id, accounts, cached_ocr_text = args
    import fitz
    import re
    
    try:
        # OPTIMIZATION: Use cached OCR text if available
        if cached_ocr_text:
            page_text = cached_ocr_text
            print(f"[SCAN_OPTIMIZED] üìã Page {page_num + 1}: Using cached OCR ({len(page_text)} chars)")
        else:
            # Extract text from PDF
            pdf_doc = fitz.open(pdf_path)
            page = pdf_doc[page_num]
            page_text = page.get_text()
            pdf_doc.close()
            
            # Check if page has watermark or needs OCR
            has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
            
            # If no text, has watermark, or very little text - do OCR
            if not page_text or len(page_text.strip()) < 20 or has_watermark:
                try:
                    pdf_doc = fitz.open(pdf_path)
                    page = pdf_doc[page_num]
                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                    pdf_doc.close()
                    
                    temp_image_path = os.path.join(OUTPUT_DIR, f"temp_scan_{doc_id}_{page_num}.png")
                    pix.save(temp_image_path)
                    
                    with open(temp_image_path, 'rb') as image_file:
                        image_bytes = image_file.read()
                    
                    print(f"[SCAN_OPTIMIZED] üîç Page {page_num + 1}: Running OCR (watermark: {has_watermark})")
                    
                    textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                    
                    ocr_text = ""
                    for block in textract_response.get('Blocks', []):
                        if block['BlockType'] == 'LINE':
                            ocr_text += block.get('Text', '') + "\n"
                    
                    if os.path.exists(temp_image_path):
                        os.remove(temp_image_path)
                    
                    if ocr_text.strip():
                        page_text = ocr_text
                        print(f"[SCAN_OPTIMIZED] ‚úÖ Page {page_num + 1}: OCR extracted {len(page_text)} chars")
                    
                except Exception as ocr_err:
                    print(f"[SCAN_OPTIMIZED] ‚ùå OCR failed on page {page_num + 1}: {str(ocr_err)}")
                    return page_num, None, None
        
        if not page_text or len(page_text.strip()) < 20:
            return page_num, page_text, None
        
        # Check which account appears on this page - use optimized matching
        matched_account = None
        for acc in accounts:
            acc_num = acc.get("accountNumber", "").strip()
            if not acc_num:
                continue
                
            # Quick exact match first (most common case)
            normalized_text = re.sub(r'[\s\-\.]', '', page_text)
            normalized_acc = re.sub(r'[\s\-\.]', '', acc_num)
            if normalized_acc in normalized_text:
                matched_account = acc_num
                print(f"[SCAN_OPTIMIZED] ‚úÖ Page {page_num + 1}: Found account {acc_num}")
                break
        
        return page_num, page_text, matched_account
        
    except Exception as e:
        print(f"[SCAN_OPTIMIZED] ‚ùå Failed to process page {page_num + 1}: {str(e)}")
        return page_num, None, None

def _process_single_page_scan(args):
    """Helper function to process a single page (for parallel processing)"""
    page_num, pdf_path, doc_id, accounts = args
    import fitz
    import re
    
    try:
        # Open PDF for this thread
        pdf_doc = fitz.open(pdf_path)
        page = pdf_doc[page_num]
        page_text = page.get_text()
        pdf_doc.close()
        
        # Check if page has watermark
        has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
        
        # If no text, has watermark, or very little text - do OCR
        if not page_text or len(page_text.strip()) < 20 or has_watermark:
            try:
                pdf_doc = fitz.open(pdf_path)
                page = pdf_doc[page_num]
                # Use higher resolution (2x) for better OCR accuracy
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                pdf_doc.close()
                
                temp_image_path = os.path.join(OUTPUT_DIR, f"temp_scan_{doc_id}_{page_num}.png")
                pix.save(temp_image_path)
                
                with open(temp_image_path, 'rb') as image_file:
                    image_bytes = image_file.read()
                
                print(f"[SCAN] Running OCR on page {page_num + 1} (watermark: {has_watermark}, little text: {len(page_text.strip()) < 20})")
                
                # Use detect_document_text for OCR
                textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                
                ocr_text = ""
                for block in textract_response.get('Blocks', []):
                    if block['BlockType'] == 'LINE':
                        ocr_text += block.get('Text', '') + "\n"
                
                if os.path.exists(temp_image_path):
                    os.remove(temp_image_path)
                
                if ocr_text.strip():
                    page_text = ocr_text
                    print(f"[SCAN] OCR extracted {len(page_text)} chars from page {page_num + 1}")
                else:
                    print(f"[SCAN] OCR returned no text for page {page_num + 1}")
                    
            except Exception as ocr_err:
                print(f"[ERROR] OCR failed on page {page_num + 1}: {str(ocr_err)}")
                return page_num, None, None
        
        if not page_text or len(page_text.strip()) < 20:
            return page_num, page_text, None
        
        # Check which account appears on this page - use more flexible matching
        matched_account = None
        for acc in accounts:
            acc_num = acc.get("accountNumber", "").strip()
            if not acc_num:
                continue
                
            # Try multiple matching strategies
            found = False
            
            # Strategy 1: Exact match (no spaces/dashes)
            normalized_text = re.sub(r'[\s\-\.]', '', page_text)
            normalized_acc = re.sub(r'[\s\-\.]', '', acc_num)
            if normalized_acc in normalized_text:
                found = True
                print(f"[SCAN] Found account {acc_num} on page {page_num + 1} (exact match)")
            
            # Strategy 2: Partial match (first 6 digits)
            if not found and len(acc_num) >= 6:
                partial_acc = acc_num[:6]
                if partial_acc in normalized_text:
                    found = True
                    print(f"[SCAN] Found account {acc_num} on page {page_num + 1} (partial match: {partial_acc})")
            
            # Strategy 3: Regex pattern matching for account-like numbers
            if not found:
                # Look for the account number with possible formatting
                pattern = r'\b' + re.escape(acc_num) + r'\b'
                if re.search(pattern, page_text, re.IGNORECASE):
                    found = True
                    print(f"[SCAN] Found account {acc_num} on page {page_num + 1} (regex match)")
            
            # Strategy 4: Look for any 9-digit number that matches
            if not found and len(acc_num) == 9:
                nine_digit_pattern = r'\b\d{9}\b'
                matches = re.findall(nine_digit_pattern, page_text)
                if acc_num in matches:
                    found = True
                    print(f"[SCAN] Found account {acc_num} on page {page_num + 1} (9-digit pattern)")
            
            if found:
                matched_account = acc_num
                break
        
        return page_num, page_text, matched_account
        
    except Exception as e:
        print(f"[ERROR] Failed to process page {page_num + 1}: {str(e)}")
        return page_num, None, None


def scan_and_map_pages(doc_id, pdf_path, accounts):
    """Scan pages and create a mapping of page_num -> account_number (OPTIMIZED + CACHED)"""
    import fitz
    
    pdf_doc = fitz.open(pdf_path)
    total_pages = len(pdf_doc)
    pdf_doc.close()
    
    page_to_account = {}
    accounts_found = set()
    
    print(f"[SCAN_OPTIMIZED] üöÄ FAST scanning {total_pages} pages to find account boundaries")
    print(f"[SCAN_OPTIMIZED] Looking for accounts: {[acc.get('accountNumber', 'N/A') for acc in accounts]}")
    
    # OPTIMIZATION 1: Check if OCR cache exists first
    ocr_text_cache = {}
    try:
        cache_key = f"ocr_cache/{doc_id}/text_cache.json"
        cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
        ocr_text_cache = json.loads(cached_result['Body'].read())
        print(f"[SCAN_OPTIMIZED] ‚úÖ Using cached OCR for {len(ocr_text_cache)} pages")
    except:
        print(f"[SCAN_OPTIMIZED] No OCR cache found, will extract as needed")
    
    # OPTIMIZATION 2: Process pages in parallel but use cached OCR when available
    max_workers = min(8, total_pages)  # Reduced workers to avoid overwhelming
    
    # Prepare arguments for parallel processing
    page_args = [(page_num, pdf_path, doc_id, accounts, ocr_text_cache.get(page_num)) for page_num in range(total_pages)]
    
    # Process pages in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(_process_single_page_scan_optimized, args) for args in page_args]
        
        for future in as_completed(futures):
            try:
                page_num, page_text, matched_account = future.result()
                
                if page_text and page_num not in ocr_text_cache:
                    ocr_text_cache[page_num] = page_text
                
                if matched_account:
                    page_to_account[page_num] = matched_account
                    accounts_found.add(matched_account)
                    print(f"[SCAN_OPTIMIZED] ‚úÖ Page {page_num + 1} -> Account {matched_account}")
                    
            except Exception as e:
                print(f"[SCAN_OPTIMIZED] ‚ùå Future failed: {str(e)}")
    
    print(f"[SCAN_OPTIMIZED] üéØ COMPLETE: Found {len(accounts_found)} accounts on {len(page_to_account)} pages")
    print(f"[SCAN_OPTIMIZED] Page mapping: {page_to_account}")
    
    # Save updated OCR cache
    try:
        cache_key = f"ocr_cache/{doc_id}/text_cache.json"
        s3_client.put_object(
            Bucket=S3_BUCKET,
            Key=cache_key,
            Body=json.dumps(ocr_text_cache),
            ContentType='application/json'
        )
        print(f"[SCAN_OPTIMIZED] üíæ Updated OCR cache with {len(ocr_text_cache)} pages")
    except Exception as e:
        print(f"[SCAN_OPTIMIZED] ‚ö†Ô∏è Failed to cache OCR: {str(e)}")
    
    return page_to_account








# Supported Document Types with Expected Fields
SUPPORTED_DOCUMENT_TYPES = {
    "marriage_certificate": {
        "name": "Marriage Certificate",
        "icon": "üíç",
        "description": "Official marriage registration document",
        "expected_fields": ["bride_name", "groom_name", "marriage_date", "location", "certificate_number", "county", "state", "officiant", "witness_names"],
        "keywords": ["marriage", "bride", "groom", "matrimony", "wedding", "spouse"]
    },
    "death_certificate": {
        "name": "Death Certificate",
        "icon": "üìú",
        "description": "Official death registration document",
        "expected_fields": ["deceased_name", "date_of_death", "place_of_death", "cause_of_death", "certificate_number", "account_number", "age", "date_of_birth", "social_security_number", "state_file_number", "registrar", "date_pronounced_dead", "time_of_death", "manner_of_death", "license_number_for"],
        "keywords": ["death", "deceased", "decedent", "demise", "passed away", "mortality", "certification of vital record", "certificate of death", "local registrar"]
    },
    "business_card": {
        "name": "Business Card / Card Order Form",
        "icon": "üíº",
        "description": "Professional contact information or card order form",
        "expected_fields": ["company_name", "contact_name", "job_title", "phone", "email", "address", "website", "card_details", "authorization"],
        "keywords": ["business card", "company", "phone", "email", "contact", "card order form", "business details", "mailing details", "atm", "debit card", "card request"]
    },
    "invoice": {
        "name": "Invoice / Withdrawal Form",
        "icon": "üßæ",
        "description": "Payment request or withdrawal document",
        "expected_fields": ["invoice_number", "vendor_name", "customer_name", "invoice_date", "due_date", "total_amount", "items", "tax", "payment_terms", "account_number", "withdrawal_amount"],
        "keywords": ["invoice", "bill", "payment", "amount due", "total", "vendor", "withdrawal", "funeral", "statement", "charges", "services"]
    },
    "loan_document": {
        "name": "Loan/Account Document",
        "icon": "üè¶",
        "description": "Banking or loan account information",
        "expected_fields": ["account_number", "account_holder_names", "account_type", "ownership_type", "ssn", "signers", "balance", "interest_rate"],
        "keywords": ["account", "loan", "bank", "balance", "account holder", "signature", "banking", "account number", "account documentation", "bank account", "checking", "savings", "deposit", "financial institution", "account opening", "account information", "signer", "ownership", "wsfs", "consumer", "business account", "account type", "ownership type", "account purpose", "wsfs account type", "account holder names", "signature card", "joint account"]
    },
    "drivers_license": {
        "name": "Driver's License / ID Card",
        "icon": "ü™™",
        "description": "Government-issued identification",
        "expected_fields": ["full_name", "license_number", "date_of_birth", "address", "issue_date", "expiration_date", "state", "class", "height", "weight", "eye_color"],
        "keywords": ["driver", "license", "identification", "ID", "DMV", "driver's license", "drivers license", "id card", "identification card"]
    },
    "passport": {
        "name": "Passport",
        "icon": "üõÇ",
        "description": "International travel document",
        "expected_fields": ["full_name", "passport_number", "nationality", "date_of_birth", "place_of_birth", "issue_date", "expiration_date", "sex"],
        "keywords": ["passport", "travel document", "nationality", "immigration"]
    },
    "contract": {
        "name": "Contract/Legal Document",
        "icon": "üìù",
        "description": "Legal agreement or official document",
        "expected_fields": ["contract_title", "parties", "effective_date", "expiration_date", "terms", "signatures", "contract_number", "estate_name", "decedent_name", "executor"],
        "keywords": ["contract", "agreement", "parties", "terms", "conditions", "hereby", "register of wills", "letters testamentary", "letters of administration", "affidavit", "small estate", "name change", "tax id change", "tin change"]
    },
    "receipt": {
        "name": "Receipt",
        "icon": "üßæ",
        "description": "Proof of purchase",
        "expected_fields": ["merchant_name", "date", "time", "items", "total_amount", "payment_method", "transaction_id"],
        "keywords": ["receipt", "purchase", "transaction", "paid", "merchant"]
    },
    "tax_form": {
        "name": "Tax Form",
        "icon": "üìã",
        "description": "Tax-related document",
        "expected_fields": ["form_type", "tax_year", "taxpayer_name", "ssn", "income", "deductions", "tax_owed", "refund"],
        "keywords": ["tax", "IRS", "W-2", "1099", "return", "federal", "tax identification"]
    },
    "medical_record": {
        "name": "Medical Record",
        "icon": "üè•",
        "description": "Healthcare document",
        "expected_fields": ["patient_name", "date_of_birth", "medical_record_number", "date_of_service", "diagnosis", "treatment", "provider_name"],
        "keywords": ["patient", "medical", "diagnosis", "treatment", "doctor", "hospital"]
    },
    "insurance_policy": {
        "name": "Insurance Policy",
        "icon": "üõ°Ô∏è",
        "description": "Insurance coverage document",
        "expected_fields": ["policy_number", "policyholder_name", "coverage_type", "effective_date", "expiration_date", "premium", "coverage_amount"],
        "keywords": ["insurance", "policy", "coverage", "premium", "insured", "beneficiary"]
    }
}

# Persistent storage for processed documents
DOCUMENTS_DB_FILE = "processed_documents.json"

def parse_combined_ocr_fields(text):
    """
    Parse combined OCR text that reads form labels and values together without spaces
    Examples: "PurposeConsumer Personal" ‚Üí Account_Purpose: "Consumer", Account_Category: "Personal"
    """
    results = {}
    print(f"[PARSE_COMBINED] Input text: '{text}'")
    
    # CRITICAL: Handle the exact case the user is experiencing
    # "Purpose Consumer Personal" ‚Üí Account_Purpose: "Consumer", Account_Category: "Personal"
    if "Purpose Consumer Personal" in text:
        print(f"[PARSE_COMBINED] Found 'Purpose Consumer Personal' pattern - EXACT USER CASE")
        results["Account_Purpose"] = "Consumer"
        results["Account_Category"] = "Personal"
        return results
    
    # Handle "Consumer Personal" pattern (space-separated)
    if "Consumer Personal" in text:
        print(f"[PARSE_COMBINED] Found 'Consumer Personal' pattern")
        results["Account_Purpose"] = "Consumer"
        results["Account_Category"] = "Personal"
        return results
    elif "Consumer Business" in text:
        print(f"[PARSE_COMBINED] Found 'Consumer Business' pattern")
        results["Account_Purpose"] = "Consumer"
        results["Account_Category"] = "Business"
        return results
    
    # Handle Purpose + Type combinations (no spaces)
    if "PurposeConsumer" in text:
        print(f"[PARSE_COMBINED] Found 'PurposeConsumer' pattern")
        results["Account_Purpose"] = "Consumer"
        # Check what comes after Consumer
        if "Personal" in text:
            results["Account_Category"] = "Personal"
            print(f"[PARSE_COMBINED] Also found 'Personal'")
        elif "Business" in text:
            results["Account_Category"] = "Business"
            print(f"[PARSE_COMBINED] Also found 'Business'")
    
    # Handle other Purpose types
    if "PurposeChecking" in text:
        print(f"[PARSE_COMBINED] Found 'PurposeChecking' pattern")
        results["Account_Purpose"] = "Checking"
    elif "PurposeSavings" in text:
        print(f"[PARSE_COMBINED] Found 'PurposeSavings' pattern")
        results["Account_Purpose"] = "Savings"
    
    # Handle Type patterns
    if "TypePersonal" in text:
        print(f"[PARSE_COMBINED] Found 'TypePersonal' pattern")
        results["Account_Category"] = "Personal"
    elif "TypeBusiness" in text:
        print(f"[PARSE_COMBINED] Found 'TypeBusiness' pattern")
        results["Account_Category"] = "Business"
    
    # Handle Ownership patterns
    if "OwnershipJoint" in text:
        print(f"[PARSE_COMBINED] Found 'OwnershipJoint' pattern")
        results["Ownership_Type"] = "Joint"
    elif "OwnershipIndividual" in text:
        print(f"[PARSE_COMBINED] Found 'OwnershipIndividual' pattern")
        results["Ownership_Type"] = "Individual"
    
    print(f"[PARSE_COMBINED] Results: {results}")
    return results


def extract_wsfs_product_from_text(text):
    """
    Extract WSFS product names from raw OCR text as a fallback
    """
    if not isinstance(text, str):
        return None
    
    # Common WSFS product patterns
    wsfs_products = [
        "WSFS Core Savings",
        "WSFS Checking Plus", 
        "WSFS Money Market",
        "WSFS Premier Checking",
        "Premier Checking",
        "Platinum Savings",
        "Gold CD",
        "Business Checking",
        "Money Market Account",
        "Certificate of Deposit"
    ]
    
    # Look for product names in the text
    for product in wsfs_products:
        if product in text:
            print(f"[WSFS_EXTRACT] Found product in text: {product}")
            return product
    
    return None


def ensure_consistent_field_structure(data, original_text=None):
    """
    Ensure consistent field structure by standardizing field names and values
    This is called after normalization to guarantee consistency
    """
    if not isinstance(data, dict):
        return data
    
    print(f"[CONSISTENCY] Input data: {data}")
    
    # Standard field mappings for loan documents
    standard_fields = {
        "Account_Number": None,
        "Account_Holders": None,
        "Account_Purpose": None,
        "Account_Category": None,
        "Account_Type": None,  # Keep for backward compatibility
        "WSFS_Account_Type": None,
        "Ownership_Type": None,
        "Address": None,
        "Phone_Number": None,
        "Work_Phone": None,
        "Date_Opened": None,
        "Date_Revised": None,
        "CIF_Number": None,
        "Branch": None,
        "Verified_By": None,
        "Opened_By": None,
        "Signatures_Required": None,
        "Special_Instructions": None,
        "Form_Number": None,
        "Reference_Number": None,
        "Stamp_Date": None,
        "Signer1_Name": None,
        "Signer1_SSN": None,
        "Signer1_DOB": None,
        "Signer1_Address": None,
        "Signer1_Phone": None,
        "Signer1_DriversLicense": None,
        "Signer2_Name": None,
        "Signer2_SSN": None,
        "Signer2_DOB": None,
        "Signer2_Address": None,
        "Signer2_Phone": None,
        "Signer2_DriversLicense": None
    }
    
    # Copy existing fields
    result = {}
    for key, value in data.items():
        # Skip empty values
        if value == "" or value is None:
            continue
        result[key] = value
    
    # Ensure we have the critical separated fields
    if "Account_Purpose" not in result and "Account_Category" not in result:
        # Look for any field that might contain combined values
        for key, value in result.items():
            actual_value = value
            if isinstance(value, dict) and "value" in value:
                actual_value = value["value"]
            
            if isinstance(actual_value, str):
                confidence_score = value.get("confidence", 100) if isinstance(value, dict) else 100
                
                if "Consumer Personal" in actual_value:
                    result["Account_Purpose"] = {"value": "Consumer", "confidence": confidence_score}
                    result["Account_Category"] = {"value": "Personal", "confidence": confidence_score}
                    # Remove the combined field
                    if key in ["Purpose", "Type", "Account_Type"]:
                        del result[key]
                    break
                elif "Consumer Business" in actual_value:
                    result["Account_Purpose"] = {"value": "Consumer", "confidence": confidence_score}
                    result["Account_Category"] = {"value": "Business", "confidence": confidence_score}
                    # Remove the combined field
                    if key in ["Purpose", "Type", "Account_Type"]:
                        del result[key]
                    break
    
    # CRITICAL: Try to extract WSFS product type if missing
    if "WSFS_Account_Type" not in result and original_text:
        wsfs_product = extract_wsfs_product_from_text(original_text)
        if wsfs_product:
            result["WSFS_Account_Type"] = {"value": wsfs_product, "confidence": 85}
            print(f"[CONSISTENCY] Added missing WSFS product: {wsfs_product}")
    
    print(f"[CONSISTENCY] Final result: {result}")
    return result


def normalize_extraction_result(data):
    """
    Normalize extraction results to ensure consistency across different extractions
    """
    if not data or not isinstance(data, dict):
        return data
    
    print(f"[NORMALIZE] Input data: {data}")
    normalized = {}
    
    # Field name mappings to standardize variations
    field_mappings = {
        # Address variations
        "Mailing_Address": "Address",
        "mailing_address": "Address", 
        "Street_Address": "Address",
        
        # Signature variations  
        "Required_Signatures": "Signatures_Required",
        "required_signatures": "Signatures_Required",
        "Signature_Required": "Signatures_Required",
        "Number_of_Signatures_Required": "Signatures_Required",
        
        # Phone variations - normalize format
        "phone_number": "Phone_Number",
        "contact_phone": "Phone_Number",
        "Home_Phone": "Phone_Number",
        "home_phone": "Phone_Number",
        
        # Account category
        "account_category": "Account_Category",
        "AccountCategory": "Account_Category",
        
        # Account holder variations
        "Account_Holder_Names": "Account_Holders",
        "account_holder_names": "Account_Holders",
        "AccountHolderNames": "Account_Holders",
        
        # CIF variations
        "CIF_Number": "CIF_Number",
        "cif_number": "CIF_Number",
        "CIFNumber": "CIF_Number",
        
        # Date variations
        "Date_Opened": "Date_Opened",
        "date_opened": "Date_Opened",
        "DateOpened": "Date_Opened",
        
        # Verification variations
        "Verified_By": "Verified_By",
        "verified_by": "Verified_By",
        "VerifiedBy": "Verified_By"
    }
    
    # Process each field
    for key, value in data.items():
        print(f"[NORMALIZE] Processing field: {key} = {value}")
        
        # CRITICAL: Handle confidence objects first
        actual_value = value
        confidence_score = 100  # Default confidence
        is_confidence_object = False
        
        if isinstance(value, dict) and "value" in value:
            actual_value = value["value"]
            confidence_score = value.get("confidence", 100)
            is_confidence_object = True
            print(f"[NORMALIZE] Extracted value from confidence object: {actual_value} (confidence: {confidence_score})")
        
        # CRITICAL FIX: Handle specific field names with combined values
        if key in ["Purpose", "Account_Purpose", "AccountPurpose"] and isinstance(actual_value, str):
            print(f"[NORMALIZE] Found Purpose field: {key} = {actual_value}")
            if "Consumer Personal" in actual_value:
                print(f"[NORMALIZE] Parsing 'Consumer Personal' into separate fields")
                normalized["Account_Purpose"] = {"value": "Consumer", "confidence": confidence_score}
                normalized["Account_Category"] = {"value": "Personal", "confidence": confidence_score}
                continue
            elif "Consumer Business" in actual_value:
                print(f"[NORMALIZE] Parsing 'Consumer Business' into separate fields")
                normalized["Account_Purpose"] = {"value": "Consumer", "confidence": confidence_score}
                normalized["Account_Category"] = {"value": "Business", "confidence": confidence_score}
                continue
            elif "Consumer" in actual_value:
                print(f"[NORMALIZE] Found Consumer, checking for additional category")
                normalized["Account_Purpose"] = {"value": "Consumer", "confidence": confidence_score}
                # Try to extract category from remaining text
                remaining = actual_value.replace("Consumer", "").strip()
                print(f"[NORMALIZE] Remaining text after removing 'Consumer': '{remaining}'")
                if remaining:
                    if "Personal" in remaining:
                        normalized["Account_Category"] = {"value": "Personal", "confidence": confidence_score}
                        print(f"[NORMALIZE] Found Personal in remaining text")
                    elif "Business" in remaining:
                        normalized["Account_Category"] = {"value": "Business", "confidence": confidence_score}
                        print(f"[NORMALIZE] Found Business in remaining text")
                continue
        
        # Handle Type field variations - map to Account_Category for consistency
        if key in ["Type", "Account_Type", "AccountType", "Account_Category", "AccountCategory"] and isinstance(actual_value, str):
            print(f"[NORMALIZE] Found Type/Category field: {key} = {actual_value}")
            if "Personal" in actual_value and "Consumer" not in actual_value:
                normalized["Account_Category"] = {"value": "Personal", "confidence": confidence_score}
                continue
            elif "Business" in actual_value and "Consumer" not in actual_value:
                normalized["Account_Category"] = {"value": "Business", "confidence": confidence_score}
                continue
            else:
                # If it's already Account_Type, map it to Account_Category for consistency
                if key in ["Account_Type", "AccountType"]:
                    normalized["Account_Category"] = {"value": actual_value, "confidence": confidence_score}
                else:
                    normalized["Account_Category"] = {"value": actual_value, "confidence": confidence_score}
                continue
        
        # CRITICAL FIX: Handle combined OCR field names and values
        combined_text = f"{key} {actual_value}" if isinstance(actual_value, str) else key
        parsed_fields = parse_combined_ocr_fields(combined_text)
        
        # If we parsed combined fields, add them and skip the original
        if parsed_fields:
            print(f"[NORMALIZE] Parsed combined fields: {parsed_fields}")
            normalized.update(parsed_fields)
            continue
        
        # Apply field name mapping
        normalized_key = field_mappings.get(key, key)
        
        # Normalize phone number format
        if "phone" in normalized_key.lower() and isinstance(actual_value, str):
            # Remove extra formatting and standardize
            phone = actual_value.replace("(", "").replace(")", "").replace("-", "").replace(" ", "")
            if len(phone) == 10:
                actual_value = f"({phone[:3]}) {phone[3:6]}-{phone[6:]}"
        
        print(f"[NORMALIZE] Adding field: {normalized_key} = {actual_value}")
        # Preserve confidence format
        if is_confidence_object or confidence_score != 100:
            normalized[normalized_key] = {"value": actual_value, "confidence": confidence_score}
        else:
            normalized[normalized_key] = actual_value
    
    # Ensure consistent field ordering
    ordered_fields = {}
    field_order = [
        "Account_Number", "Account_Category", "Account_Purpose", "Account_Type", 
        "Address", "Phone_Number", "Branch", "CIF_Number", "Date_Opened",
        "Form_Number", "Ownership_Type", "Signatures_Required", "Reference_Number",
        "Stamp_Date", "Verified_By", "Signer1_Name", "Signer1_SSN", "Signer1_DOB",
        "Signer1_DriversLicense", "Signer2_Name", "Signer2_SSN", "Signer2_DOB", 
        "Signer2_DriversLicense"
    ]
    
    # Add fields in preferred order
    for field in field_order:
        if field in normalized:
            ordered_fields[field] = normalized[field]
    
    # Add any remaining fields
    for key, value in normalized.items():
        if key not in ordered_fields:
            ordered_fields[key] = value
    
    # FINAL CLEANUP: Handle any remaining combined values that slipped through
    final_cleaned = {}
    for key, value in ordered_fields.items():
        # Extract actual value from confidence objects
        actual_value = value
        if isinstance(value, dict) and "value" in value:
            actual_value = value["value"]
        
        # Skip fields that contain combined values we should have parsed
        if isinstance(actual_value, str) and ("Consumer Personal" in actual_value or "Consumer Business" in actual_value):
            # These should have been parsed into separate fields already
            print(f"[NORMALIZE] Skipping combined field that should have been parsed: {key} = {actual_value}")
            continue
        final_cleaned[key] = value
    
    # Ensure we have the essential separated fields
    if "Account_Purpose" not in final_cleaned and "Account_Category" not in final_cleaned:
        # Look for any field that might contain the combined value
        for key, value in ordered_fields.items():
            actual_value = value
            if isinstance(value, dict) and "value" in value:
                actual_value = value["value"]
                
            if isinstance(actual_value, str) and "Consumer Personal" in actual_value:
                print(f"[NORMALIZE] Emergency parsing of combined field: {key} = {actual_value}")
                final_cleaned["Account_Purpose"] = "Consumer"
                final_cleaned["Account_Category"] = "Personal"
                break
            elif isinstance(actual_value, str) and "Consumer Business" in actual_value:
                print(f"[NORMALIZE] Emergency parsing of combined field: {key} = {actual_value}")
                final_cleaned["Account_Purpose"] = "Consumer"
                final_cleaned["Account_Category"] = "Business"
                break
    
    # FINAL SAFETY CHECK: Ensure no combined "Consumer Personal" fields remain
    safety_checked = {}
    for key, value in final_cleaned.items():
        # Extract actual value from confidence objects
        actual_value = value
        confidence_score = 100
        
        if isinstance(value, dict) and "value" in value:
            actual_value = value["value"]
            confidence_score = value.get("confidence", 100)
        
        # If we find ANY field with "Consumer Personal", force split it
        if isinstance(actual_value, str) and "Consumer Personal" in actual_value:
            print(f"[NORMALIZE] SAFETY CHECK: Found remaining combined field {key} = {actual_value}")
            # Don't add this field, instead add the split fields
            if "Account_Purpose" not in safety_checked:
                safety_checked["Account_Purpose"] = {"value": "Consumer", "confidence": confidence_score}
            if "Account_Category" not in safety_checked:
                safety_checked["Account_Category"] = {"value": "Personal", "confidence": confidence_score}
            print(f"[NORMALIZE] SAFETY CHECK: Forced split into Account_Purpose=Consumer, Account_Category=Personal")
        else:
            safety_checked[key] = value
    
    print(f"[NORMALIZE] Final output after safety check: {safety_checked}")
    return safety_checked


def normalize_confidence_format(data):
    """
    Normalize data to separate values and confidence scores.
    Input: {"Field": {"value": "text", "confidence": 95}}
    Output: ({"Field": "text"}, {"Field": 95})
    """
    if not isinstance(data, dict):
        return data, {}
    
    values = {}
    confidences = {}
    
    for key, value in data.items():
        if isinstance(value, dict) and "value" in value and "confidence" in value:
            # New format with confidence scores
            values[key] = value["value"]
            confidences[key] = value["confidence"]
        elif isinstance(value, dict) and "value" in value:
            # Has value but no confidence
            values[key] = value["value"]
            confidences[key] = 100  # Default to 100 if not specified
        else:
            # Old format without confidence scores
            values[key] = value
            confidences[key] = 100  # Default to 100 for backward compatibility
    
    return values, confidences


def is_confidence_object(obj):
    """Check if an object is a confidence object {value: X, confidence: Y}"""
    return (isinstance(obj, dict) and 
            "value" in obj and 
            "confidence" in obj and 
            len(obj) == 2)

def flatten_nested_objects(data):
    """
    Flatten nested objects like Signer1: {Name: "John"} to Signer1_Name: "John"
    Also handles arrays of signer objects and other nested structures
    CRITICAL: Preserves confidence objects {value: X, confidence: Y} at all levels
    """
    if not isinstance(data, dict):
        return data
    
    flattened = {}
    
    for key, value in data.items():
        # FIRST: Check if this value itself is a confidence object
        if is_confidence_object(value):
            flattened[key] = value
            print(f"[DEBUG] Preserved top-level confidence object: {key}")
            continue
        
        # Check if this is a signer object (Signer1, Signer2, etc.)
        if (key.startswith("Signer") and isinstance(value, dict) and 
            any(char.isdigit() for char in key)):
            # Flatten the signer object
            print(f"[DEBUG] Flattening signer object: {key} with {len(value)} fields")
            for sub_key, sub_value in value.items():
                flat_key = f"{key}_{sub_key}"
                # Preserve confidence objects
                if is_confidence_object(sub_value):
                    flattened[flat_key] = sub_value
                    print(f"[DEBUG] Preserved signer confidence: {flat_key}")
                else:
                    flattened[flat_key] = sub_value
                    print(f"[DEBUG] Flattened signer field: {flat_key}")
            continue
        
        # Handle arrays of signer objects
        if isinstance(value, list) and len(value) > 0:
            key_lower = key.lower().replace('_', '').replace(' ', '')
            is_signer_array = any(keyword in key_lower for keyword in ['signer', 'signature', 'accountholder'])
            first_item = value[0]
            is_object_array = isinstance(first_item, dict)
            
            if is_object_array and is_signer_array:
                print(f"[DEBUG] Found signer array '{key}' with {len(value)} signers")
                for idx, signer_obj in enumerate(value):
                    signer_num = idx + 1
                    for sub_key, sub_value in signer_obj.items():
                        flat_key = f"Signer{signer_num}_{sub_key}"
                        # Preserve confidence objects
                        if is_confidence_object(sub_value):
                            flattened[flat_key] = sub_value
                            print(f"[DEBUG] Preserved array signer confidence: {flat_key}")
                        else:
                            flattened[flat_key] = sub_value
                continue
            else:
                # Keep other arrays as-is
                flattened[key] = value
                continue
        
        # Keep primitives as-is
        if isinstance(value, (str, int, float, bool)) or value is None:
            flattened[key] = value
            continue
        
        # Handle nested dicts
        if isinstance(value, dict):
            # Check if it's a special structure to preserve
            if key in ["SupportingDocuments", "AccountHolderNames", "Supporting_Documents", "Account_Holders"]:
                flattened[key] = value
                continue
            
            # Flatten other nested objects
            for sub_key, sub_value in value.items():
                flat_key = f"{key}_{sub_key}"
                # Preserve confidence objects at nested level
                if is_confidence_object(sub_value):
                    flattened[flat_key] = sub_value
                    print(f"[DEBUG] Preserved nested confidence: {flat_key}")
                else:
                    flattened[flat_key] = sub_value
            continue
        
        # Default: keep as-is
        flattened[key] = value
    
    print(f"[DEBUG] Flattening complete: {len(data)} input fields -> {len(flattened)} output fields")
    return flattened


def call_bedrock(prompt: str, text: str, max_tokens: int = 8192):
    """Call AWS Bedrock with Claude - using maximum token limit"""
    payload = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": max_tokens,
        "temperature": 0,
        "messages": [
            {"role": "user", "content": [{"type": "text", "text": f"{prompt}\n\n{text}"}]}
        ],
    }
    resp = bedrock.invoke_model(modelId=MODEL_ID, body=json.dumps(payload))
    return json.loads(resp["body"].read())["content"][0]["text"]


def extract_basic_fields(text: str, num_fields: int = 100):
    """Extract ALL fields from any document (up to 100 fields) - BE THOROUGH"""
    # USE THE SAME PROMPT AS get_comprehensive_extraction_prompt() FOR CONSISTENCY
    prompt = get_comprehensive_extraction_prompt()
    
    try:
        # Ensure consistent text input by comprehensive normalization
        def normalize_text_for_consistency(text):
            """Normalize text to ensure identical processing regardless of source"""
            # Remove extra whitespace and normalize line endings
            text = re.sub(r'\r\n', '\n', text)  # Normalize line endings
            text = re.sub(r'\r', '\n', text)    # Handle old Mac line endings
            text = re.sub(r'\n+', '\n', text)   # Remove multiple consecutive newlines
            text = re.sub(r'[ \t]+', ' ', text) # Normalize spaces and tabs
            text = text.strip()                 # Remove leading/trailing whitespace
            return text
        
        normalized_text = normalize_text_for_consistency(text)[:10000]  # Consistent truncation
        
        # Create a deterministic prompt hash to ensure consistency
        import hashlib
        # Use a stable prompt identifier + normalized text for hashing
        prompt_stable = prompt.replace(str(num_fields), "NUM_FIELDS")  # Remove dynamic numbers
        text_hash = hashlib.md5(f"{prompt_stable[:200]}{normalized_text}".encode()).hexdigest()
        print(f"[EXTRACT_BASIC] Input hash: {text_hash[:8]} (for consistency tracking)")
        
        # Check if we have a cached result for this exact input
        cache_key = f"extraction_cache/{text_hash}.json"
        try:
            cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = json.loads(cached_result['Body'].read())
            print(f"[EXTRACT_BASIC] ‚úì Using cached result (hash: {text_hash[:8]}) - GUARANTEED CONSISTENT")
            return cached_data
        except:
            print(f"[EXTRACT_BASIC] No cache found, extracting fresh (hash: {text_hash[:8]}) - WILL CACHE FOR CONSISTENCY")
        
        response = call_bedrock(prompt, normalized_text, max_tokens=8192)  # Use maximum tokens for comprehensive extraction
        
        # Find JSON content
        json_start = response.find('{')
        json_end = response.rfind('}')
        
        if json_start == -1 or json_end == -1:
            raise ValueError("No JSON object found in response")
        
        json_str = response[json_start:json_end + 1]
        result = json.loads(json_str)
        
        # Log the number of fields extracted for consistency tracking
        field_count = len(result) if result else 0
        print(f"[EXTRACT_BASIC] ‚úì Extracted {field_count} fields (hash: {text_hash[:8]})")
        
        # Cache the result for future consistency
        try:
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(result),
                ContentType='application/json'
            )
            print(f"[EXTRACT_BASIC] ‚úì Cached result for future consistency")
        except Exception as cache_error:
            print(f"[EXTRACT_BASIC] ‚ö†Ô∏è Failed to cache result: {cache_error}")
        
        # Ensure we have at least some fields
        if not result or len(result) == 0:
            return {"document_content": "Unable to extract structured fields"}
        
        return result
    except Exception as e:
        return {
            "error": str(e),
            "note": "Failed to extract structured fields from document"
        }


def detect_and_extract_documents(text: str):
    """
    Dynamically detect document types and extract relevant fields
    AI decides what fields to extract based on document type
    """
    # First detect document type
    doc_type = detect_document_type(text)
    doc_info = SUPPORTED_DOCUMENT_TYPES.get(doc_type, {
        "name": "Unknown Document",
        "icon": "üìÑ",
        "description": "Unidentified document type",
        "expected_fields": []
    })
    
    # Use specialized prompts for specific document types
    if doc_type == "drivers_license":
        # Use specialized driver's license prompt
        prompt = get_drivers_license_prompt()
    elif doc_type == "loan_document":
        # Use specialized loan document prompt
        prompt = get_loan_document_prompt()
    else:
        # Build extraction prompt based on document type
        if doc_type != "unknown" and doc_info.get("expected_fields"):
            expected_fields_str = ", ".join(doc_info["expected_fields"])
            field_instructions = f"""
This is a {doc_info['name']}.
Extract ALL of these fields if present: {expected_fields_str}

Also extract any other relevant information you find in the document.
"""
        else:
            field_instructions = f"""
This appears to be a {doc_info['name']}.
Extract all relevant fields you can identify from the document.
"""
        
        prompt = f"""
{field_instructions}

YOU ARE A METICULOUS DATA EXTRACTION EXPERT. YOUR GOAL IS TO EXTRACT ABSOLUTELY EVERYTHING FROM THIS DOCUMENT.

CRITICAL: Use SIMPLE, SHORT field names. Do NOT copy the entire label text from the document.
- Example: "DATE PRONOUNCED DEAD" ‚Üí use "Death_Date" (NOT "Date_Pronounced_Dead")
- Example: "ACCOUNT HOLDER NAMES" ‚Üí use "Account_Holders" (NOT "Account_Holder_Names")
- Example: "DATE OF ISSUE" ‚Üí use "Issue_Date" (NOT "Date_Of_Issue")
- Simplify all verbose labels to their core meaning

CRITICAL EXTRACTION RULES - EXTRACT EVERYTHING:
1. ALL IDENTIFYING NUMBERS:
   - Certificate numbers, file numbers, case numbers, reference numbers
   - License numbers, registration numbers, document numbers
   - Social security numbers, tax IDs, account numbers
   - ANY number with a label or identifier (K1-0000608, K1-0011267, etc.)

2. ALL DATES AND TIMES:
   - Issue dates, filing dates, registration dates, effective dates
   - Birth dates, death dates, marriage dates, expiration dates
   - Timestamps (22:29, 14:30, etc.)
   - Date stamps (01/09/2016, January 9, 2016, etc.)
   - Extract in ORIGINAL format as shown

3. ALL NAMES:
   - Full names, first names, middle names, last names, maiden names
   - Witness names, registrar names, physician names, funeral director names
   - Father's name, mother's name, spouse's name
   - Informant names, certifier names, pronouncer names
   - ANY person's name mentioned ANYWHERE in the document

4. ALL LOCATIONS:
   - Cities, towns, townships, counties, states, countries
   - Place of birth, place of death, place of marriage, place of residence
   - Street addresses with house numbers, apartment numbers
   - Zip codes, postal codes
   - Hospital names, facility names, institution names

5. ALL FORM FIELDS WITH LABELS:
   - Look for patterns like "FIELD_NAME: value" or "FIELD_NAME value"
   - Extract checkbox fields (checked/unchecked, Yes/No)
   - Extract dropdown selections
   - Extract text fields, even if partially filled
   - Extract signature fields and who signed

6. ALL CODES AND CLASSIFICATIONS:
   - Cause of death codes, ICD codes, classification codes
   - Occupation codes, industry codes
   - Race codes, ethnicity codes, marital status codes
   - ANY coded information

7. ALL CONTACT INFORMATION:
   - Phone numbers, fax numbers, mobile numbers
   - Email addresses, websites
   - Mailing addresses, physical addresses

8. ALL ADMINISTRATIVE DATA:
   - Form numbers, version numbers, revision dates
   - Page numbers, section numbers
   - Barcode numbers, QR code data
   - Watermark text, stamp text
   - "LICENSE NUMBER FOR" fields
   - "SIGNATURE OF" fields
   - "DATE PRONOUNCED DEAD" fields
   - "ACTUAL OR PRESUMED DATE OF DEATH" fields
   - "CAUSE OF DEATH" fields
   - ANY field with a label, even if it seems minor

EXTRACTION STRATEGY:
- Read the document line by line, field by field
- Extract EVERY labeled field you see
- Extract EVERY number that has a label or context
- Extract EVERY date in any format
- Extract EVERY name in any context
- Do NOT skip fields because they seem unimportant
- Do NOT skip fields because they are partially visible
- Do NOT skip fields because they are handwritten
- Include fields even if the value is unclear (mark as "Illegible" or "Unclear")

FIELD NAMING:
- Use SIMPLE, SHORT field names (not the full label text from the document)
- Replace spaces with underscores
- Simplify verbose labels to their core meaning
- Examples:
  * "LICENSE NUMBER FOR" ‚Üí "License_Number"
  * "DATE PRONOUNCED DEAD" ‚Üí "Death_Date"
  * "ACTUAL OR PRESUMED DATE OF DEATH" ‚Üí "Death_Date"
  * "CAUSE OF DEATH" ‚Üí "Cause_Of_Death"
  * "K1-0011267" ‚Üí "Case_Number" or "File_Number"
  * "ACCOUNT HOLDER NAMES" ‚Üí "Account_Holders"
  * "DATE OF ISSUE" ‚Üí "Issue_Date"

CRITICAL NAMING FOR DEATH CERTIFICATES:
- The main certificate number (often handwritten, like "468431466" or "K1-0011267") MUST be extracted as "Account_Number"
- DO NOT use "Certificate_Number" - use "Account_Number" instead
- Example: If you see "468431466" or "K1-0011267" as the primary certificate identifier:
  * Extract it as "Account_Number": "468431466"
  * NOT as "Certificate_Number"

EXTRACT ABSOLUTELY EVERYTHING - MISS NOTHING:
- Read EVERY line of the document
- Extract EVERY field with a label
- Extract EVERY number (even if handwritten or unclear)
- Extract EVERY date and time
- Extract EVERY name (deceased, witnesses, physicians, funeral directors, registrars)
- Extract EVERY location (place of death, residence, city, county, state)
- Extract EVERY checkbox value (Yes/No, checked/unchecked)
- Extract EVERY signature field and date
- Extract license numbers, file numbers, reference numbers
- If a field is partially visible or unclear, still extract it and mark as "unclear" or "illegible"

Return ONLY valid JSON in this exact format where EVERY field has both value and confidence:
{{
  "documents": [
    {{
      "document_id": "doc_001",
      "document_type": "{doc_type}",
      "document_type_display": "{doc_info['name']}",
      "document_icon": "{doc_info['icon']}",
      "document_description": "{doc_info['description']}",
      "extracted_fields": {{
        "Field_Name": {{
          "value": "exact_value_from_document",
          "confidence": 95
        }},
        "Another_Field": {{
          "value": "another_value",
          "confidence": 85
        }},
        "SupportingDocuments": [
          {{
            "DocumentType": "type of document",
            "Details": "relevant details"
          }}
        ]
      }}
    }}
  ]
}}

CONFIDENCE SCORE GUIDELINES:
- 90-100: Text is clear, printed, and easily readable
- 70-89: Text is readable but slightly unclear (handwritten, faded, or small)
- 50-69: Text is partially unclear or ambiguous
- 30-49: Text is difficult to read or uncertain
- 0-29: Very uncertain or guessed

CRITICAL: EVERY field (except SupportingDocuments array) MUST have both "value" and "confidence"

For SupportingDocuments, ONLY include:
- Driver's License or State ID (with ID numbers)
- Death Certificates, Birth Certificates, Marriage Certificates
- OFAC checks or compliance verifications
- Passport or other government-issued IDs
- Attached copies of actual documents

DO NOT include as SupportingDocuments:
- Standard terms and conditions (e.g., "Deposit Account Agreement")
- Generic disclosures (e.g., "Regulation E Disclosure", "Privacy Policy")
- Legal disclaimers or authorization text
- Standard bank forms or agreements mentioned in fine print
- Any document that is just referenced in legal text but not actually attached

If there are NO actual supporting documents attached or specifically referenced with details, OMIT the SupportingDocuments field entirely.

CRITICAL: Do NOT use "N/A" or empty strings - ONLY include fields that have ACTUAL VALUES in the document.
If a field is not present or has no value, DO NOT include it in the JSON at all.
Only extract fields where you can see a clear, definite value in the document.
"""
    
    # For driver's license and loan documents, the prompt is already complete
    # For other documents, we need to wrap the response format
    if doc_type not in ["drivers_license", "loan_document"]:
        # The generic prompt needs the full format instructions which are already included above
        pass
    
    try:
        response = call_bedrock(prompt, text, max_tokens=8192)
        
        # Clean up response - remove markdown code blocks if present
        response = response.strip()
        
        # Find JSON content - look for the first { and last }
        json_start = response.find('{')
        json_end = response.rfind('}')
        
        if json_start == -1 or json_end == -1:
            raise ValueError("No JSON object found in response")
        
        json_str = response[json_start:json_end + 1]
        
        # Try to parse JSON
        result = json.loads(json_str)
        
        # Ensure documents array exists
        if "documents" not in result:
            result = {"documents": [result]}
        
        # Calculate accuracy for each document and identify fields needing review
        for doc in result.get("documents", []):
            # Ensure extracted_fields exists
            if "extracted_fields" not in doc:
                doc["extracted_fields"] = {}
            
            # CRITICAL: Flatten nested objects in extracted_fields
            doc["extracted_fields"] = flatten_nested_objects(doc["extracted_fields"])
            
            fields = doc.get("extracted_fields", {})
            
            # POST-PROCESSING: For death certificates, rename certificate_number to account_number
            doc_type = doc.get("document_type", "")
            if doc_type == "death_certificate":
                # If certificate_number exists but account_number doesn't, rename it
                if "certificate_number" in fields and "account_number" not in fields:
                    fields["account_number"] = fields["certificate_number"]
                    del fields["certificate_number"]
                # Also check for variations
                if "Certificate_Number" in fields and "Account_Number" not in fields:
                    fields["Account_Number"] = fields["Certificate_Number"]
                    del fields["Certificate_Number"]
            filled_fields = sum(1 for v in fields.values() if v and v != "N/A" and v != "")
            total_fields = len(fields) if fields else 1
            doc["accuracy_score"] = round((filled_fields / total_fields) * 100, 1)
            doc["total_fields"] = total_fields
            doc["filled_fields"] = filled_fields
            
            # Calculate average confidence score
            confidence_scores = []
            for field_name, value in fields.items():
                # Check if value is a confidence object
                if isinstance(value, dict) and "confidence" in value:
                    confidence_scores.append(value["confidence"])
            
            if confidence_scores:
                doc["confidence_score"] = round(sum(confidence_scores) / len(confidence_scores), 1)
            else:
                doc["confidence_score"] = None
            
            # Identify fields needing review
            fields_needing_review = []
            for field_name, value in fields.items():
                if not value or value == "N/A" or value == "" or (isinstance(value, list) and len(value) == 0):
                    fields_needing_review.append({
                        "field_name": field_name,
                        "reason": "Missing or not found in document",
                        "current_value": value if value else "Not extracted"
                    })
            
            doc["fields_needing_review"] = fields_needing_review
            doc["needs_human_review"] = doc["accuracy_score"] < 100
            
            # Ensure required fields exist
            if "document_id" not in doc:
                doc["document_id"] = f"doc_{int(time.time())}"
            if "document_type" not in doc:
                doc["document_type"] = "unknown"
            if "document_type_display" not in doc:
                doc["document_type_display"] = "Unknown Document"
        
        return result
    except json.JSONDecodeError as e:
        return {
            "documents": [{
                "document_id": "error_001",
                "document_type": "error",
                "document_type_display": "JSON Parse Error",
                "error": f"Failed to parse AI response: {str(e)}",
                "raw_response": response[:500] if 'response' in locals() else "No response",
                "extracted_fields": {},
                "accuracy_score": 0,
                "total_fields": 0,
                "filled_fields": 0
            }]
        }
    except Exception as e:
        return {
            "documents": [{
                "document_id": "error_002",
                "document_type": "error",
                "document_type_display": "Processing Error",
                "error": str(e),
                "extracted_fields": {},
                "accuracy_score": 0,
                "total_fields": 0,
                "filled_fields": 0
            }]
        }


def pre_cache_all_pages(job_id: str, pdf_path: str, accounts: list):
    """
    Pre-cache all page data during initial upload to avoid re-running OCR on every click.
    This extracts text and data from all pages once and stores in S3.
    OPTIMIZED: Reuses OCR text cache to avoid duplicate OCR calls.
    """
    import fitz
    import json
    
    if not pdf_path or not os.path.exists(pdf_path):
        print(f"[WARNING] PDF path not found for pre-caching: {pdf_path}")
        return
    
    print(f"[INFO] Starting pre-cache for all pages in document {job_id}")
    
    try:
        # OPTIMIZATION: Try to load OCR cache first
        ocr_text_cache = {}
        try:
            cache_key = f"ocr_cache/{job_id}/text_cache.json"
            cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            ocr_text_cache = json.loads(cache_response['Body'].read().decode('utf-8'))
            # Convert string keys to int
            ocr_text_cache = {int(k): v for k, v in ocr_text_cache.items()}
            print(f"[INFO] Loaded OCR cache with {len(ocr_text_cache)} pages - will reuse to save costs")
        except Exception as cache_err:
            print(f"[INFO] No OCR cache found, will extract text: {str(cache_err)}")
        
        pdf_doc = fitz.open(pdf_path)
        total_pages = len(pdf_doc)
        
        # First, scan and map pages to accounts
        page_to_account = {}
        accounts_found = set()
        
        print(f"[INFO] Scanning {total_pages} pages to map accounts...")
        
        for page_num in range(total_pages):
            # OPTIMIZATION: Check cache first
            if page_num in ocr_text_cache:
                page_text = ocr_text_cache[page_num]
                print(f"[DEBUG] Reusing cached OCR for page {page_num + 1} ({len(page_text)} chars)")
            else:
                page = pdf_doc[page_num]
                page_text = page.get_text()
                
                # Check if page needs OCR
                has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
                
                if not page_text or len(page_text.strip()) < 20 or has_watermark:
                    # Extract with OCR
                    try:
                        pix = page.get_pixmap(matrix=fitz.Matrix(1, 1))
                        temp_image_path = os.path.join(OUTPUT_DIR, f"temp_precache_{job_id}_{page_num}.png")
                        pix.save(temp_image_path)
                        
                        with open(temp_image_path, 'rb') as image_file:
                            image_bytes = image_file.read()
                        
                        textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                        
                        page_text = ""
                        for block in textract_response.get('Blocks', []):
                            if block['BlockType'] == 'LINE':
                                page_text += block.get('Text', '') + "\n"
                        
                        if os.path.exists(temp_image_path):
                            os.remove(temp_image_path)
                            
                    except Exception as ocr_err:
                        print(f"[ERROR] OCR failed on page {page_num + 1}: {str(ocr_err)}")
                        continue
                
                # Cache for later reuse
                ocr_text_cache[page_num] = page_text
            
            # Map page to account
            for acc_idx, acc in enumerate(accounts):
                acc_num = acc.get("accountNumber", "").strip()
                normalized_text = re.sub(r'[\s\-]', '', page_text)
                normalized_acc = re.sub(r'[\s\-]', '', acc_num)
                
                if normalized_acc and normalized_acc in normalized_text:
                    page_to_account[page_num] = (acc_idx, acc_num)
                    accounts_found.add(acc_num)
                    break
        
        print(f"[INFO] Mapped {len(page_to_account)} pages to {len(accounts_found)} accounts")
        
        # SPEED OPTIMIZATION: Extract and cache data for pages in PARALLEL
        def _extract_page_data(page_info):
            """Helper to extract data from a single page"""
            page_num, account_index, account_number = page_info
            try:
                print(f"[INFO] Pre-caching page {page_num + 1} for account {account_number}")
                
                # OPTIMIZATION: Reuse cached OCR text
                if page_num in ocr_text_cache:
                    page_text = ocr_text_cache[page_num]
                    print(f"[DEBUG] Reusing cached text for page {page_num + 1} - saved OCR call!")
                else:
                    # Fallback: extract if not in cache
                    import fitz
                    pdf_doc_local = fitz.open(pdf_path)
                    page = pdf_doc_local[page_num]
                    page_text = page.get_text()
                    pdf_doc_local.close()
                    
                    # Check if needs OCR
                    has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
                    
                    if not page_text or len(page_text.strip()) < 20 or has_watermark:
                        pdf_doc_local = fitz.open(pdf_path)
                        page = pdf_doc_local[page_num]
                        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                        pdf_doc_local.close()
                        
                        temp_image_path = os.path.join(OUTPUT_DIR, f"temp_extract_{job_id}_{page_num}.png")
                        pix.save(temp_image_path)
                        
                        with open(temp_image_path, 'rb') as image_file:
                            image_bytes = image_file.read()
                        
                        textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                        
                        page_text = ""
                        for block in textract_response.get('Blocks', []):
                            if block['BlockType'] == 'LINE':
                                page_text += block.get('Text', '') + "\n"
                        
                        if os.path.exists(temp_image_path):
                            os.remove(temp_image_path)
                
                # Detect document type on this page
                detected_type = detect_document_type(page_text)
                
                # Use appropriate prompt based on detected type
                if detected_type == "drivers_license":
                    page_extraction_prompt = get_drivers_license_prompt()
                else:
                    page_extraction_prompt = get_comprehensive_extraction_prompt()
                
                # Extract data using AI
                response = call_bedrock(page_extraction_prompt, page_text, max_tokens=8192)
                
                # Parse JSON
                json_start = response.find('{')
                json_end = response.rfind('}')
                
                if json_start != -1 and json_end != -1:
                    json_str = response[json_start:json_end + 1]
                    parsed = json.loads(json_str)
                    
                    # Handle driver's license format
                    if detected_type == "drivers_license" and "documents" in parsed:
                        if len(parsed["documents"]) > 0:
                            doc_data = parsed["documents"][0]
                            if "extracted_fields" in doc_data:
                                parsed = doc_data["extracted_fields"]
                            else:
                                parsed = doc_data
                    
                    # CRITICAL: Flatten nested objects
                    parsed = flatten_nested_objects(parsed)
                    
                    parsed["Account_Number"] = account_number
                    
                    # Cache to S3
                    cache_key = f"page_data/{job_id}/account_{account_index}/page_{page_num}.json"
                    cache_data = {
                        "account_number": account_number,
                        "data": parsed,
                        "extracted_at": datetime.now().isoformat(),
                        "pre_cached": True,
                        "prompt_version": "v5_enhanced_verified"  # Version to invalidate old cache
                    }
                    
                    s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=cache_key,
                        Body=json.dumps(cache_data),
                        ContentType='application/json'
                    )
                    
                    print(f"[INFO] Cached page {page_num + 1} data to S3: {cache_key}")
                    return True
                    
            except Exception as page_error:
                print(f"[ERROR] Failed to pre-cache page {page_num + 1}: {str(page_error)}")
                return False
        
        # Prepare page info for parallel processing
        page_infos = [(page_num, account_index, account_number) 
                      for page_num, (account_index, account_number) in page_to_account.items()]
        
        # PARALLEL PROCESSING: Extract data from multiple pages simultaneously
        # Use up to 5 workers for LLM calls (to avoid rate limits)
        max_workers = min(5, len(page_infos))
        print(f"[INFO] PARALLEL extraction: Processing {len(page_infos)} pages with {max_workers} workers")
        
        success_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(_extract_page_data, page_info) for page_info in page_infos]
            
            for future in as_completed(futures):
                try:
                    if future.result():
                        success_count += 1
                except Exception as e:
                    print(f"[ERROR] Future failed: {str(e)}")
        
        print(f"[INFO] PARALLEL pre-caching completed: {success_count}/{len(page_infos)} pages cached successfully")
        
    except Exception as e:
        print(f"[ERROR] Pre-caching failed: {str(e)}")


def process_job(job_id: str, file_bytes: bytes, filename: str, use_ocr: bool, document_name: str = None, original_file_path: str = None):
    """Background worker to process documents - FAST upload with placeholder creation"""
    global processed_documents
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Use document_name if provided, otherwise use filename
        if not document_name:
            document_name = filename
        
        # Save PDF file locally for viewing
        saved_pdf_path = None
        if filename.lower().endswith('.pdf'):
            saved_pdf_path = f"{OUTPUT_DIR}/{timestamp}_{filename}"
            with open(saved_pdf_path, 'wb') as f:
                f.write(file_bytes)
            print(f"[INFO] Saved PDF to: {saved_pdf_path}")
        
        # Log processing start
        print(f"[INFO] Starting FAST upload for job {job_id}: {filename}")
        print(f"[INFO] File size: {len(file_bytes) / 1024:.2f} KB")
        
        # Initialize job status
        job_status_map[job_id] = {
            "status": "Creating placeholder document...",
            "progress": 20
        }
        
        # FAST UPLOAD: Create placeholder document immediately with document type detection
        detected_doc_type = "unknown"
        doc_icon = "üìÑ"
        doc_description = "Document uploaded - extraction will start when opened"
        
        if filename.lower().endswith('.pdf') and saved_pdf_path:
            job_status_map[job_id].update({
                "status": "Detecting document type...",
                "progress": 40
            })
            
            # Quick document type detection using first page text
            try:
                import fitz
                pdf_doc = fitz.open(saved_pdf_path)
                if len(pdf_doc) > 0:
                    first_page_text = pdf_doc[0].get_text()
                    print(f"[DEBUG] First page text extracted: {len(first_page_text)} characters")
                    print(f"[DEBUG] First 200 chars: {first_page_text[:200]}")
                    
                    # If insufficient text found (scanned PDF or poor text extraction), use OCR on first page only
                    if len(first_page_text.strip()) < 300:
                        print(f"[INFO] First page has little text ({len(first_page_text)} chars), using OCR for document type detection...")
                        
                        try:
                            # Convert first page to image and OCR it
                            page = pdf_doc[0]
                            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x resolution
                            
                            temp_image_path = os.path.join(OUTPUT_DIR, f"temp_detection_{job_id}_page0.png")
                            pix.save(temp_image_path)
                            
                            with open(temp_image_path, 'rb') as image_file:
                                image_bytes = image_file.read()
                            
                            # Use Textract for OCR on first page only
                            textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                            
                            ocr_text = ""
                            for block in textract_response.get('Blocks', []):
                                if block['BlockType'] == 'LINE':
                                    ocr_text += block.get('Text', '') + "\n"
                            
                            # Clean up temp file
                            if os.path.exists(temp_image_path):
                                os.remove(temp_image_path)
                            
                            if ocr_text.strip():
                                first_page_text = ocr_text
                                print(f"[INFO] ‚úì OCR extracted {len(first_page_text)} characters for document type detection")
                            else:
                                print(f"[WARNING] OCR failed to extract text from first page")
                                
                        except Exception as ocr_error:
                            print(f"[WARNING] OCR failed for document type detection: {str(ocr_error)}")
                    
                    # Detect document type using the extracted text
                    if len(first_page_text.strip()) > 10:
                        detected_doc_type = detect_document_type(first_page_text)
                        
                        # Get document info
                        if detected_doc_type in SUPPORTED_DOCUMENT_TYPES:
                            doc_info = SUPPORTED_DOCUMENT_TYPES[detected_doc_type]
                            doc_icon = doc_info["icon"]
                            doc_description = doc_info["description"]
                        
                        print(f"[INFO] ‚úÖ Detected document type: {detected_doc_type}")
                    else:
                        print(f"[WARNING] Insufficient text for document type detection ({len(first_page_text)} chars)")
                    
                pdf_doc.close()
            except Exception as detection_error:
                print(f"[WARNING] Document type detection failed: {str(detection_error)}")
                detected_doc_type = "unknown"
        
        # Create placeholder document immediately - NO OCR, NO EXTRACTION
        job_status_map[job_id].update({
            "status": "Creating placeholder document...",
            "progress": 80
        })
        
        # Create placeholder document structure
        if detected_doc_type == "loan_document":
            # For loan documents, create simple placeholder - accounts will be extracted when document is opened
            print(f"[UPLOAD] Creating loan document placeholder - accounts will be extracted when opened")
            placeholder_doc = {
                "document_id": "loan_doc_001",
                "document_type": "loan_document",
                "document_type_display": "Loan/Account Document",
                "document_icon": "üè¶",
                "document_description": "Banking or loan account information",
                "extracted_fields": {
                    "total_accounts": 0,
                    "accounts_processed": 0
                },
                "accounts": [],  # Empty - will be populated when document is opened
                "accuracy_score": None,
                "filled_fields": 0,
                "total_fields": 0,
                "fields_needing_review": [],
                "needs_human_review": False,
                "optimized": True
            }
        else:
            # For other documents, create simple placeholder
            placeholder_doc = {
                "document_type": detected_doc_type,
                "document_type_display": SUPPORTED_DOCUMENT_TYPES.get(detected_doc_type, {}).get("name", "Unknown Document"),
                "document_icon": doc_icon,
                "document_description": doc_description,
                "extracted_fields": {},
                "total_fields": 0,
                "filled_fields": 0,
                "needs_human_review": False
            }
        
        # Create document record
        document_record = {
            "id": job_id,
            "filename": filename,
            "document_name": document_name,
            "timestamp": timestamp,
            "processed_date": datetime.now().isoformat(),
            "ocr_file": None,  # No OCR file yet
            "ocr_method": "Deferred - will extract when opened",
            "basic_fields": {},
            "documents": [placeholder_doc],
            "document_type_info": {
                "type": detected_doc_type,
                "name": SUPPORTED_DOCUMENT_TYPES.get(detected_doc_type, {}).get("name", "Unknown Document"),
                "icon": doc_icon,
                "description": doc_description,
                "expected_fields": SUPPORTED_DOCUMENT_TYPES.get(detected_doc_type, {}).get("expected_fields", []),
                "is_supported": detected_doc_type in SUPPORTED_DOCUMENT_TYPES
            },
            "use_ocr": use_ocr,
            "pdf_path": saved_pdf_path,
            "status": "extracting",  # Show as extracting
            "can_view": True  # Allow immediate viewing
        }
        
        # Add document to database immediately
        processed_documents.append(document_record)
        save_documents_db(processed_documents)
        
        # Mark job as processing (not complete yet - background processing will continue)
        # Keep progress at 40 (from upload phase) instead of resetting to 15
        job_status_map[job_id] = {
            "status": "üì§ Document uploaded - starting background processing...",
            "progress": 40,
            "document_id": job_id,
            "is_complete": False
        }
        
        # üöÄ START BACKGROUND PROCESSING IMMEDIATELY
        if saved_pdf_path and detected_doc_type == "loan_document":
            print(f"[BG_PROCESSOR] üöÄ Starting background processing for loan document {job_id}")
            background_processor.queue_document_for_processing(job_id, saved_pdf_path, priority=1)
        elif saved_pdf_path:
            print(f"[BG_PROCESSOR] üöÄ Starting background processing for document {job_id}")
            background_processor.queue_document_for_processing(job_id, saved_pdf_path, priority=2)
        
        print(f"[INFO] ‚úÖ FAST upload completed - document {job_id} ready for viewing")
        print(f"[INFO] Document type: {detected_doc_type} - background processing started")
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        
        job_status_map[job_id] = {
            "status": f"‚ùå Error: {str(e)}",
            "progress": 0,
            "error": str(e),
            "error_details": error_details
        }
        
        # Log error to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        error_log = f"{OUTPUT_DIR}/{timestamp}_processing_error.log"
        with open(error_log, 'w', encoding='utf-8') as f:
            f.write(f"Job ID: {job_id}\n")
            f.write(f"Filename: {filename}\n")
            f.write(f"Error: {str(e)}\n\n")
            f.write(f"Traceback:\n{error_details}\n")


@app.route("/")
def index():
    """Main page - Skills Catalog Dashboard"""
    return render_template("skills_catalog.html")


@app.route("/dashboard")
def dashboard():
    """Dashboard - Shows all processed documents as skills"""
    return render_template("skills_catalog.html")


@app.route("/codebase")
def codebase():
    """Codebase documentation"""
    return render_template("codebase_docs.html")


@app.route("/test_account_display.html")
def test_account_display():
    """Test page for account display"""
    return send_file("test_account_display.html")


@app.route("/api/documents")
def get_all_documents():
    """API endpoint to get all processed documents"""
    # Return documents with their saved cost information
    response = jsonify({"documents": processed_documents, "total": len(processed_documents)})
    
    # Add cache-busting headers to ensure fresh data
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    
    return response


@app.route("/api/document/<doc_id>")
def get_document_detail(doc_id):
    """Get details of a specific document with real-time account availability"""
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if doc:
        # Check if background processing has found accounts even if not fully complete
        bg_status = background_processor.get_document_status(doc_id)
        if bg_status and bg_status.get("accounts"):
            # Update document with latest accounts from background processing
            if doc.get("documents") and len(doc["documents"]) > 0:
                doc["documents"][0]["accounts"] = bg_status["accounts"]
                print(f"[API] üîÑ Updated document {doc_id} with {len(bg_status['accounts'])} accounts from background processing")
        
        # Cost information is already saved in the document record
        # No need to fetch from in-memory tracker
        
        return jsonify({"success": True, "document": doc})
    return jsonify({"success": False, "message": "Document not found"}), 404


@app.route("/api/document/<doc_id>/process-loan", methods=["POST"])
def process_loan_document_endpoint(doc_id):
    """Process loan document to split into accounts - called when loan document is first opened"""
    try:
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Check if it's a loan document
        doc_type = doc.get("document_type_info", {}).get("type")
        if doc_type != "loan_document":
            return jsonify({"success": False, "message": "Not a loan document"}), 400
        
        # Check if already processed (has accounts)
        doc_data = doc.get("documents", [{}])[0]
        existing_accounts = doc_data.get("accounts", [])
        
        # üöÄ PRIORITY: Check background processing status
        bg_status = background_processor.get_document_status(doc_id)
        
        # If background processing is still running, wait for it or return pending status
        if bg_status and bg_status.get("stage") and bg_status.get("stage") != "completed":
            print(f"[LOAN_PROCESSING] ‚è≥ Background processing still running (stage: {bg_status.get('stage')})")
            return jsonify({
                "success": True,
                "message": "Background processing in progress",
                "accounts": [],
                "total_accounts": 0,
                "source": "background_processing_pending",
                "stage": bg_status.get("stage")
            })
        
        # If background processing completed, use those results
        if bg_status and bg_status.get("accounts") and len(bg_status.get("accounts", [])) > 0:
            bg_accounts = bg_status["accounts"]
            print(f"[LOAN_PROCESSING] ‚úÖ Found {len(bg_accounts)} accounts from background processing")
            
            # Update the document with background results if not already updated
            if not existing_accounts or len(existing_accounts) == 0:
                doc_data["accounts"] = bg_accounts
                doc_data["extracted_fields"] = {
                    "total_accounts": len(bg_accounts),
                    "accounts_processed": len(bg_accounts),
                    "processing_method": "Background processing"
                }
                doc_data["background_processed"] = True
                save_documents_db(processed_documents)
                print(f"[LOAN_PROCESSING] ‚úÖ Updated document with background processing results")
            
            return jsonify({
                "success": True, 
                "message": "Processed by background system", 
                "accounts": bg_accounts,
                "total_accounts": len(bg_accounts),
                "source": "background_processing"
            })
        
        if existing_accounts and len(existing_accounts) > 0:
            return jsonify({
                "success": True, 
                "message": "Already processed", 
                "accounts": existing_accounts,
                "total_accounts": len(existing_accounts),
                "source": "previous_processing"
            })
        
        # Get PDF path
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        print(f"[LOAN_PROCESSING] Processing loan document {doc_id} for account splitting...")
        
        # üöÄ CHECK OCR STATUS FIRST - Prevent duplicate OCR calls
        ocr_status = ocr_cache_manager.get_ocr_status(doc_id)
        
        if ocr_status and ocr_status.get("ocr_completed"):
            print(f"[LOAN_PROCESSING] ‚úÖ OCR already completed for {doc_id}, skipping OCR")
            return jsonify({
                "success": True,
                "message": "OCR already completed, waiting for background processing",
                "accounts": [],
                "total_accounts": 0,
                "source": "ocr_already_done"
            })
        
        if ocr_status and ocr_status.get("ocr_in_progress"):
            print(f"[LOAN_PROCESSING] ‚è≥ OCR already in progress for {doc_id}, skipping duplicate OCR")
            return jsonify({
                "success": True,
                "message": "OCR already in progress",
                "accounts": [],
                "total_accounts": 0,
                "source": "ocr_in_progress"
            })
        
        # Mark OCR as in progress
        ocr_cache_manager.mark_ocr_in_progress(doc_id)
        
        # Extract text from entire PDF using fast OCR
        try:
            # Read PDF file bytes
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
            
            # Use fast OCR extraction
            print(f"[LOAN_PROCESSING] Using fast OCR for account detection...")
            full_text, _ = extract_text_with_textract(pdf_bytes, os.path.basename(pdf_path))
            
            # Fallback to PyPDF if OCR fails
            if not full_text or len(full_text.strip()) < 100:
                print(f"[LOAN_PROCESSING] OCR failed, trying PyPDF as fallback...")
                full_text, _ = try_extract_pdf_with_pypdf(pdf_bytes, os.path.basename(pdf_path))
            
            print(f"[LOAN_PROCESSING] Extracted {len(full_text)} characters from PDF")
            
            # Mark OCR as completed
            ocr_cache_manager.mark_ocr_completed(doc_id, full_text, {"method": "textract"})
            
        except Exception as text_error:
            print(f"[LOAN_PROCESSING] Text extraction failed: {str(text_error)}")
            return jsonify({"success": False, "message": f"Text extraction failed: {str(text_error)}"}), 500
        
        if not full_text or len(full_text.strip()) < 100:
            return jsonify({"success": False, "message": "Insufficient text extracted from PDF"}), 400
        
        # Process with loan processor to split into accounts
        try:
            loan_result = process_loan_document(full_text)
            
            if not loan_result or "documents" not in loan_result:
                return jsonify({"success": False, "message": "Loan processing failed"}), 500
            
            loan_doc_data = loan_result["documents"][0]
            raw_accounts = loan_doc_data.get("accounts", [])
            
            # Normalize and merge duplicate accounts (e.g., "0000927800" and "927800")
            accounts = normalize_and_merge_accounts(raw_accounts)
            print(f"[LOAN_PROCESSING] Account normalization: {len(raw_accounts)} -> {len(accounts)} accounts")
            
            print(f"[LOAN_PROCESSING] ‚úì Found {len(accounts)} accounts")
            print(f"[LOAN_PROCESSING] Loan result structure: {loan_result}")
            print(f"[LOAN_PROCESSING] Accounts to save: {accounts}")
            
            # Update the document with account information
            update_data = {
                "extracted_fields": loan_doc_data.get("extracted_fields", {}),
                "accounts": accounts,
                "total_fields": loan_doc_data.get("total_fields", 0),
                "filled_fields": loan_doc_data.get("filled_fields", 0),
                "needs_human_review": loan_doc_data.get("needs_human_review", False),
                "optimized": True
            }
            
            print(f"[LOAN_PROCESSING] Update data: {update_data}")
            print(f"[LOAN_PROCESSING] Document before update: {doc['documents'][0]}")
            
            doc["documents"][0].update(update_data)
            
            print(f"[LOAN_PROCESSING] Document after update: {doc['documents'][0]}")
            
            # Save updated document
            save_documents_db(processed_documents)
            print(f"[LOAN_PROCESSING] ‚úì Document saved to database")
            
            return jsonify({
                "success": True,
                "message": f"Successfully processed {len(accounts)} accounts",
                "accounts": accounts,
                "total_accounts": len(accounts)
            })
            
        except Exception as process_error:
            print(f"[LOAN_PROCESSING] Processing failed: {str(process_error)}")
            return jsonify({"success": False, "message": f"Processing failed: {str(process_error)}"}), 500
        
    except Exception as e:
        print(f"[LOAN_PROCESSING] Endpoint error: {str(e)}")
        return jsonify({"success": False, "message": f"Server error: {str(e)}"}), 500


@app.route("/document/<doc_id>/pages")
def view_document_pages(doc_id):
    """View document with unified page-by-page viewer"""
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if doc:
        return render_template("unified_page_viewer.html", document=doc)
    return "Document not found", 404


@app.route("/document/<doc_id>/accounts")
def view_account_based(doc_id):
    """View document with account-based interface"""
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if doc:
        return render_template("account_based_viewer.html", document=doc)
    return "Document not found", 404


@app.route("/api/document/<doc_id>/changes", methods=["GET"])
def get_document_changes(doc_id):
    """Get the list of changes for a document"""
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    changes = doc.get("changes", [])
    return jsonify({
        "success": True,
        "changes": changes,
        "needs_review": doc.get("needs_review", False),
        "update_source": doc.get("update_source_filename", "Unknown")
    })


@app.route("/api/document/<doc_id>/apply-changes", methods=["POST"])
def apply_selected_changes(doc_id):
    """Apply only the selected changes to the document"""
    global processed_documents
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    try:
        data = request.get_json()
        selected_indices = data.get("selected_changes", [])
        
        if not selected_indices:
            return jsonify({"success": False, "message": "No changes selected"}), 400
        
        changes = doc.get("changes", [])
        applied_changes = []
        
        # Apply only selected changes
        for idx in selected_indices:
            if 0 <= idx < len(changes):
                change = changes[idx]
                applied_changes.append(change)
                
                # Apply the change to the document
                field_path = change["field"].split(".")
                
                # Navigate to the field and update it
                current = doc
                for i, key in enumerate(field_path[:-1]):
                    # Handle array indices like "accounts[468869904]"
                    if "[" in key and "]" in key:
                        base_key = key.split("[")[0]
                        array_key = key.split("[")[1].split("]")[0]
                        
                        if base_key not in current:
                            current[base_key] = []
                        
                        # Find the item in array
                        if base_key == "accounts":
                            item = next((a for a in current[base_key] if a.get("accountNumber") == array_key), None)
                            if item:
                                current = item
                    else:
                        if key not in current:
                            current[key] = {}
                        current = current[key]
                
                # Set the final value
                final_key = field_path[-1]
                if change["change_type"] == "added" or change["change_type"] == "updated":
                    current[final_key] = change["new_value"]
        
        # Mark as reviewed and move to history
        doc["needs_review"] = False
        doc["reviewed_at"] = datetime.now().isoformat()
        doc["changes_history"] = doc.get("changes_history", [])
        doc["changes_history"].append({
            "applied_changes": applied_changes,
            "rejected_changes": [c for i, c in enumerate(changes) if i not in selected_indices],
            "reviewed_at": doc["reviewed_at"]
        })
        doc["changes"] = []
        
        save_documents_db(processed_documents)
        
        return jsonify({
            "success": True,
            "message": f"Applied {len(applied_changes)} changes",
            "applied_count": len(applied_changes)
        })
    
    except Exception as e:
        print(f"[ERROR] Failed to apply changes: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "message": f"Failed: {str(e)}"}), 500


@app.route("/api/document/<doc_id>/mark-reviewed", methods=["POST"])
def mark_document_reviewed(doc_id):
    """Mark a document as reviewed without applying changes (reject all)"""
    global processed_documents
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    try:
        # Clear review flags
        doc["needs_review"] = False
        doc["reviewed_at"] = datetime.now().isoformat()
        
        # Keep changes history but mark as reviewed
        if "changes" in doc:
            doc["changes_history"] = doc.get("changes_history", [])
            doc["changes_history"].append({
                "rejected_changes": doc["changes"],
                "reviewed_at": doc["reviewed_at"]
            })
            doc["changes"] = []
        
        save_documents_db(processed_documents)
        
        return jsonify({"success": True, "message": "Document marked as reviewed (all changes rejected)"})
    
    except Exception as e:
        print(f"[ERROR] Failed to mark document as reviewed: {str(e)}")
        return jsonify({"success": False, "message": f"Failed: {str(e)}"}), 500


@app.route("/api/document/<doc_id>/pages")
def get_document_pages(doc_id):
    """Get all pages of a document as images using PyMuPDF with account mapping"""
    import fitz  # PyMuPDF
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    try:
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # Create pages directory if it doesn't exist
        pages_dir = os.path.join(OUTPUT_DIR, "pages", doc_id)
        os.makedirs(pages_dir, exist_ok=True)
        
        # Check if pages already exist
        existing_pages = sorted([f for f in os.listdir(pages_dir) if f.endswith('.png')])
        
        if not existing_pages:
            # Convert PDF to images using PyMuPDF
            print(f"[INFO] Converting PDF to images for document {doc_id}")
            pdf_document = fitz.open(pdf_path)
            
            # Convert each page to image
            for page_num in range(len(pdf_document)):
                page = pdf_document[page_num]
                
                # Render page to image (zoom=2 for 200 DPI equivalent)
                mat = fitz.Matrix(2, 2)  # 2x zoom = ~200 DPI
                pix = page.get_pixmap(matrix=mat)
                
                # Save as PNG
                page_path = os.path.join(pages_dir, f"page_{page_num+1}.png")
                pix.save(page_path)
                existing_pages.append(f"page_{page_num+1}.png")
            
            pdf_document.close()
            print(f"[INFO] Created {len(existing_pages)} page images")
        
        # Check if this is a loan document with accounts
        doc_data = doc.get("documents", [{}])[0]
        has_accounts = doc_data.get("accounts") is not None and len(doc_data.get("accounts", [])) > 0
        
        # Create page-to-account mapping for loan documents
        page_account_mapping = {}
        if has_accounts:
            accounts = doc_data.get("accounts", [])
            # Simple mapping: distribute pages evenly across accounts
            # For better accuracy, you could analyze page content to detect account numbers
            pages_per_account = max(1, len(existing_pages) // len(accounts))
            
            for i, page_file in enumerate(existing_pages):
                account_index = min(i // pages_per_account, len(accounts) - 1)
                page_account_mapping[i] = {
                    "account_index": account_index,
                    "account_number": accounts[account_index].get("accountNumber", "Unknown")
                }
        
        # Return page URLs with account mapping
        pages = [
            {
                "page_number": i + 1,
                "url": f"/api/document/{doc_id}/page/{i}",
                "thumbnail": f"/api/document/{doc_id}/page/{i}/thumbnail",
                "account_index": page_account_mapping.get(i, {}).get("account_index"),
                "account_number": page_account_mapping.get(i, {}).get("account_number")
            }
            for i in range(len(existing_pages))
        ]
        
        return jsonify({
            "success": True,
            "pages": pages,
            "total_pages": len(pages),
            "has_accounts": has_accounts,
            "total_accounts": len(doc_data.get("accounts", [])) if has_accounts else 0
        })
        
    except Exception as e:
        print(f"[ERROR] Failed to get pages for {doc_id}: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "message": f"Failed to get pages: {str(e)}"}), 500


@app.route("/api/document/<doc_id>/page/<int:page_num>")
@app.route("/api/document/<doc_id>/page/<int:page_num>/image")
def get_document_page(doc_id, page_num):
    """Get a specific page image"""
    from flask import send_file
    
    pages_dir = os.path.join(OUTPUT_DIR, "pages", doc_id)
    page_path = os.path.join(pages_dir, f"page_{page_num}.png")  # Fixed: URL is already 1-based
    
    if os.path.exists(page_path):
        return send_file(page_path, mimetype='image/png')
    
    return "Page not found", 404


@app.route("/api/document/<doc_id>/page/<int:page_num>/thumbnail")
def get_document_page_thumbnail(doc_id, page_num):
    """Get a thumbnail of a specific page"""
    from flask import send_file
    from PIL import Image
    import tempfile
    
    pages_dir = os.path.join(OUTPUT_DIR, "pages", doc_id)
    page_path = os.path.join(pages_dir, f"page_{page_num}.png")  # Fixed: URL is already 1-based
    
    if not os.path.exists(page_path):
        return "Page not found", 404
    
    try:
        # Create thumbnail
        img = Image.open(page_path)
        img.thumbnail((150, 200))
        
        # Save to temp file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
        img.save(temp_file.name, 'PNG')
        temp_file.close()
        
        return send_file(temp_file.name, mimetype='image/png')
    except Exception as e:
        print(f"[ERROR] Failed to create thumbnail: {str(e)}")
        return "Failed to create thumbnail", 500


@app.route("/api/document/<doc_id>/account/<int:account_index>/pages")
def get_account_pages(doc_id, account_index):
    """Get pages for a specific account by detecting account numbers on pages"""
    import fitz
    import re
    import json
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    try:
        # Get PDF info
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # Get total pages from document metadata (avoid opening PDF if possible)
        total_pages = doc.get("total_pages")
        if not total_pages:
            # Fallback: open PDF to get page count
            import fitz
            pdf_doc = fitz.open(pdf_path)
            total_pages = len(pdf_doc)
            pdf_doc.close()
            print(f"[CACHE_LOAD] üìÑ Got total pages from PDF: {total_pages}")
        else:
            print(f"[CACHE_LOAD] üìÑ Got total pages from metadata: {total_pages}")
        
        # Get account info
        doc_data = doc.get("documents", [{}])[0]
        accounts = doc_data.get("accounts", [])
        
        if not accounts or len(accounts) == 0:
            return jsonify({"success": False, "message": "No accounts found"}), 404
        
        if account_index >= len(accounts):
            return jsonify({"success": False, "message": "Account index out of range"}), 400
        
        target_account_number = accounts[account_index].get("accountNumber", "").strip()
        
        # CRITICAL: Normalize account number to match background processing format
        # Background processing removes leading zeros, so API must do the same
        normalized_target_account = target_account_number.lstrip('0') or '0'
        print(f"[BOUNDARY] Target account: {target_account_number} -> normalized: {normalized_target_account}")
        
        # Check cache first
        cache_key = f"page_mapping/{doc_id}/mapping.json"
        page_to_account = None
        
        try:
            print(f"[CACHE_LOAD] üîç Checking cache for page mapping: {cache_key}")
            cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = cache_response['Body'].read().decode('utf-8')
            cached_mapping = json.loads(cached_data)
            
            # Handle both old format (direct mapping) and new format (with metadata)
            if isinstance(cached_mapping, dict):
                # Try to parse as page mapping - handle string keys properly
                try:
                    # Filter out non-numeric keys and convert to int
                    page_to_account = {}
                    for k, v in cached_mapping.items():
                        if str(k).isdigit():
                            page_to_account[int(k)] = v
                    
                    if page_to_account:
                        print(f"[CACHE_LOAD] ‚úÖ Loaded cached page mapping with {len(page_to_account)} pages: {page_to_account}")
                    else:
                        print(f"[CACHE_LOAD] ‚ö†Ô∏è Cache exists but no valid page mappings found")
                        page_to_account = None
                        
                except (ValueError, TypeError) as parse_error:
                    print(f"[CACHE_LOAD] ‚ùå Cache parsing failed: {str(parse_error)}, will scan pages")
                    page_to_account = None
            else:
                print(f"[CACHE_LOAD] ‚ùå Invalid cache format (not dict), will scan pages")
                page_to_account = None
        except s3_client.exceptions.NoSuchKey:
            print(f"[CACHE_LOAD] ‚ÑπÔ∏è No cached mapping found, will scan pages")
        except Exception as cache_error:
            print(f"[CACHE_LOAD] ‚ùå Cache load failed: {str(cache_error)}, will scan pages")
        
        # PRIORITY 1: Check if background processing has completed and has page_data
        if doc_data.get("background_processed") and accounts:
            account = accounts[account_index] if account_index < len(accounts) else None
            if account and account.get("page_data"):
                print(f"[CACHE_LOAD] ‚úÖ Background processing completed, using page_data from account")
                # Get pages directly from account's page_data (much faster than scanning)
                page_data = account.get("page_data", {})
                account_pages = []
                
                for page_key in page_data.keys():
                    if page_key.isdigit():
                        page_num = int(page_key)  # page_data uses 1-based keys
                        account_pages.append(page_num)
                
                if account_pages:
                    account_pages.sort()
                    print(f"[CACHE_LOAD] ‚úÖ Found {len(account_pages)} pages from background processing: {account_pages}")
                    
                    # Return immediately without any OCR scanning
                    return jsonify({
                        "success": True,
                        "total_pages": len(account_pages),
                        "pages": account_pages,  # Already 1-based
                        "account_number": normalized_target_account,
                        "source": "background_processing_cache"
                    })
                else:
                    print(f"[CACHE_LOAD] ‚ö†Ô∏è Account has page_data but no valid page numbers found")
            else:
                print(f"[CACHE_LOAD] ‚ö†Ô∏è Background processing completed but no page_data found for account {account_index}")
        else:
            print(f"[CACHE_LOAD] ‚ö†Ô∏è Background processing not completed or no accounts found")
        
        # PRIORITY 2: Check S3 cache for page mapping
        cache_key = f"page_mapping/{doc_id}/mapping.json"
        page_to_account = None
        
        try:
            print(f"[CACHE_LOAD] üîç Checking S3 cache for page mapping: {cache_key}")
            cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = cache_response['Body'].read().decode('utf-8')
            cached_mapping = json.loads(cached_data)
            
            # Handle both old format (direct mapping) and new format (with metadata)
            if isinstance(cached_mapping, dict):
                # Filter out non-numeric keys and convert to int
                page_to_account = {}
                for k, v in cached_mapping.items():
                    if str(k).isdigit():
                        page_to_account[int(k)] = v
                
                if page_to_account:
                    print(f"[CACHE_LOAD] ‚úÖ Loaded cached page mapping with {len(page_to_account)} pages: {page_to_account}")
                else:
                    print(f"[CACHE_LOAD] ‚ö†Ô∏è Cache exists but no valid page mappings found")
                    page_to_account = None
            else:
                print(f"[CACHE_LOAD] ‚ùå Invalid cache format (not dict), will scan pages")
                page_to_account = None
        except s3_client.exceptions.NoSuchKey:
            print(f"[CACHE_LOAD] ‚ÑπÔ∏è No S3 cache found, will scan pages")
        except Exception as cache_error:
            print(f"[CACHE_LOAD] ‚ùå S3 cache load failed: {str(cache_error)}, will scan pages")
        
        # PRIORITY 3: Check if background processing is still running
        if page_to_account is None:
            # Check if background processing is still running
            bg_status = background_processor.document_status.get(doc_id, {})
            if bg_status.get("stage") and bg_status.get("stage") != "completed":
                print(f"[CACHE_LOAD] ‚è≥ Background processing still running (stage: {bg_status.get('stage')}), using fallback")
                # Return a simple fallback mapping - all pages to this account
                fallback_pages = list(range(total_pages))
                return jsonify({
                    "success": True,
                    "total_pages": len(fallback_pages),
                    "pages": [p + 1 for p in fallback_pages],  # Convert to 1-based
                    "account_number": normalized_target_account,
                    "note": "Background processing in progress, showing all pages"
                })
        
        # If still no cache and background processing is complete, scan pages and create mapping (with lock to prevent duplicates)
        if page_to_account is None:
            # Check if another process is already scanning
            lock_key = f"scan_lock/{doc_id}/scanning.lock"
            try:
                # Try to create a lock
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=lock_key,
                    Body=json.dumps({"start_time": time.time(), "process_id": os.getpid()}),
                    ContentType='application/json'
                )
                
                print(f"[SCAN_LOCK] üîí Acquired scan lock for {doc_id}")
                
                try:
                    page_to_account = scan_and_map_pages(doc_id, pdf_path, accounts)
                finally:
                    # Always release the lock
                    try:
                        s3_client.delete_object(Bucket=S3_BUCKET, Key=lock_key)
                        print(f"[SCAN_LOCK] üîì Released scan lock for {doc_id}")
                    except:
                        pass
                        
            except Exception as lock_error:
                # Lock might already exist, wait and retry cache
                print(f"[SCAN_LOCK] ‚è≥ Another process is scanning, waiting...")
                time.sleep(2)
                
                # Try cache again
                try:
                    cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                    cached_data = cache_response['Body'].read().decode('utf-8')
                    cached_mapping = json.loads(cached_data)
                    page_to_account = {int(k): v for k, v in cached_mapping.items() if k.isdigit()}
                    print(f"[SCAN_LOCK] ‚úÖ Got cached result after waiting")
                except:
                    # If still no cache, do a quick fallback scan
                    print(f"[SCAN_LOCK] ‚ö†Ô∏è No cache after waiting, doing fallback scan")
                    page_to_account = scan_and_map_pages(doc_id, pdf_path, accounts)
            
            # Save to cache
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(page_to_account),
                    ContentType='application/json'
                )
                print(f"[INFO] Cached page mapping to S3: {cache_key}")
            except Exception as s3_error:
                print(f"[WARNING] Failed to cache mapping: {str(s3_error)}")
        
        # Now assign pages to the target account using IMPROVED BOUNDARY LOGIC
        # (total_pages already calculated above)
        
        account_pages = []
        
        print(f"[BOUNDARY] Page to account mapping: {page_to_account}")
        print(f"[BOUNDARY] Looking for account: {normalized_target_account}")
        print(f"[BOUNDARY] Total pages in document: {total_pages}")
        
        # IMPROVED ALGORITHM: Assign ALL pages between account boundaries
        if page_to_account:
            # Get all account boundary pages (where account numbers are found)
            boundary_pages = sorted(page_to_account.keys())
            print(f"[BOUNDARY] Account boundary pages: {boundary_pages}")
            
            # Find account boundaries for this specific account (using normalized numbers)
            account_boundaries = []
            for page_num in boundary_pages:
                page_account = page_to_account[page_num]
                # Normalize the page account number for comparison
                normalized_page_account = page_account.lstrip('0') or '0'
                if normalized_page_account == normalized_target_account:
                    account_boundaries.append(page_num)
            
            print(f"[BOUNDARY] Account {normalized_target_account} found on pages: {account_boundaries}")
            
            if account_boundaries:
                # For each boundary where this account appears, assign pages until next different account
                for boundary_page in account_boundaries:
                    start_page = boundary_page
                    
                    # Find the end boundary (next different account or end of document)
                    end_page = total_pages  # Default to end of document
                    
                    # Look for the next page with a different account number
                    for check_page in range(boundary_page + 1, total_pages):
                        if check_page in page_to_account:
                            check_page_account = page_to_account[check_page]
                            normalized_check_account = check_page_account.lstrip('0') or '0'
                            if normalized_check_account != normalized_target_account:
                                end_page = check_page
                                print(f"[BOUNDARY] Found next different account at page {check_page + 1}")
                                break
                    
                    # Assign ALL pages in this range (including pages without account numbers)
                    range_pages = list(range(start_page, end_page))
                    account_pages.extend(range_pages)
                    
                    print(f"[BOUNDARY] Assigned pages {start_page + 1} to {end_page} to account {target_account_number}")
                
                # Remove duplicates and sort
                account_pages = sorted(list(set(account_pages)))
                
                print(f"[BOUNDARY] ‚úÖ FINAL ASSIGNMENT: Account {target_account_number} gets pages {[p+1 for p in account_pages]}")
                print(f"[BOUNDARY] This includes ALL pages between boundaries (driver licenses, forms, etc.)")
            else:
                print(f"[BOUNDARY] ‚ùå Account {target_account_number} not found on any page")
        else:
            print(f"[BOUNDARY] ‚ùå No account boundaries found - page_to_account is empty")
        
        # If no pages found, fall back to even distribution but exclude obvious non-account pages
        if not account_pages:
            print(f"[WARNING] No pages found for account {target_account_number}, using smart distribution")
            
            # Check for pages that should be excluded (document prep, cover pages, etc.)
            excluded_pages = set()
            
            # Check each page for exclusion criteria
            pdf_doc = fitz.open(pdf_path)
            for page_num in range(total_pages):
                try:
                    page = pdf_doc[page_num]
                    page_text = page.get_text()
                    
                    # If no text or watermarked, use OCR
                    has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
                    if not page_text or len(page_text.strip()) < 50 or has_watermark:
                        print(f"[SMART] Page {page_num + 1} needs OCR for exclusion check")
                        try:
                            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                            temp_image_path = os.path.join(OUTPUT_DIR, f"temp_exclude_{doc_id}_{page_num}.png")
                            pix.save(temp_image_path)
                            
                            with open(temp_image_path, 'rb') as image_file:
                                image_bytes = image_file.read()
                            
                            textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                            ocr_text = ""
                            for block in textract_response.get('Blocks', []):
                                if block['BlockType'] == 'LINE':
                                    ocr_text += block.get('Text', '') + "\n"
                            
                            if os.path.exists(temp_image_path):
                                os.remove(temp_image_path)
                            
                            if ocr_text.strip():
                                page_text = ocr_text
                                print(f"[SMART] OCR extracted {len(page_text)} chars for exclusion check on page {page_num + 1}")
                        
                        except Exception as ocr_err:
                            print(f"[WARNING] OCR failed for exclusion check on page {page_num + 1}: {str(ocr_err)}")
                    
                    # Check if this looks like a document prep or cover page
                    prep_indicators = [
                        "document prep", "step #1", "cis work", "associate:",
                        "# of documents", "count includes separator", 
                        "cover sheet", "preparation", "processing", "scanning process"
                    ]
                    
                    page_text_lower = page_text.lower()
                    is_prep_page = any(indicator in page_text_lower for indicator in prep_indicators)
                    
                    if is_prep_page:
                        excluded_pages.add(page_num)
                        print(f"[SMART] Excluding page {page_num + 1} (document prep/cover page)")
                        print(f"[SMART] Found indicators: {[ind for ind in prep_indicators if ind in page_text_lower]}")
                
                except Exception as e:
                    print(f"[WARNING] Could not check page {page_num} for exclusion: {str(e)}")
            
            pdf_doc.close()
            
            # Calculate available pages (excluding prep pages)
            available_pages = [p for p in range(total_pages) if p not in excluded_pages]
            
            if available_pages:
                pages_per_account = max(1, len(available_pages) // len(accounts))
                start_idx = account_index * pages_per_account
                end_idx = start_idx + pages_per_account if account_index < len(accounts) - 1 else len(available_pages)
                account_pages = available_pages[start_idx:end_idx]
                print(f"[SMART] Assigned {len(account_pages)} pages to account {target_account_number}: {[p+1 for p in account_pages]}")
            else:
                # Fallback to original logic if no pages available
                pages_per_account = max(1, total_pages // len(accounts))
                start_page = account_index * pages_per_account
                end_page = start_page + pages_per_account if account_index < len(accounts) - 1 else total_pages
                account_pages = list(range(start_page, end_page))
                print(f"[FALLBACK] No available pages after exclusion, using original distribution: {[p+1 for p in account_pages]}")
            
            # Also clear the cache since it's not working properly
            try:
                s3_client.delete_object(Bucket=S3_BUCKET, Key=cache_key)
                print(f"[FALLBACK] Cleared faulty cache: {cache_key}")
            except:
                pass
        
        # Display page numbers as 1-based for clarity
        display_pages = [p + 1 for p in account_pages]
        print(f"[INFO] Account {target_account_number} has {len(account_pages)} page(s): {display_pages}")
        
        response_data = {
            "success": True,
            "total_pages": len(account_pages),
            "pages": [p + 1 for p in account_pages],  # Convert to 1-based page numbers for frontend
            "account_number": target_account_number
        }
        print(f"[INFO] Final account_pages (0-based): {account_pages}")
        print(f"[INFO] Returning response: {response_data}")
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f"[ERROR] Failed to get account pages: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/clear-page-cache", methods=["POST"])
def clear_page_cache(doc_id):
    """Clear page mapping cache for a document"""
    try:
        cache_key = f"page_mapping/{doc_id}/mapping.json"
        s3_client.delete_object(Bucket=S3_BUCKET, Key=cache_key)
        print(f"[CACHE] Cleared page mapping cache: {cache_key}")
        return jsonify({"success": True, "message": "Page cache cleared"})
    except s3_client.exceptions.NoSuchKey:
        return jsonify({"success": True, "message": "No cache to clear"})
    except Exception as e:
        print(f"[ERROR] Failed to clear cache: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/account/<int:account_index>/page/<int:page_num>")
def get_account_page_image(doc_id, account_index, page_num):
    """Get specific page image for an account"""
    import fitz
    from flask import send_file
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return "Document not found", 404
    
    pages_dir = os.path.join(OUTPUT_DIR, "pages", doc_id)
    page_path = os.path.join(pages_dir, f"page_{page_num}.png")  # Fixed: URL is already 1-based
    
    # If page doesn't exist, generate it
    if not os.path.exists(page_path):
        try:
            pdf_path = doc.get("pdf_path")
            if not pdf_path or not os.path.exists(pdf_path):
                return "PDF file not found", 404
            
            os.makedirs(pages_dir, exist_ok=True)
            
            # Open PDF and render the specific page
            pdf_doc = fitz.open(pdf_path)
            if page_num > len(pdf_doc):  # page_num is 1-based, so use > instead of >=
                return "Page number out of range", 404
            
            page = pdf_doc[page_num - 1]  # Convert 1-based page_num to 0-based PDF index
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            pix.save(page_path)
            pdf_doc.close()
            
        except Exception as e:
            print(f"[ERROR] Failed to generate page {page_num}: {str(e)}")
            return f"Failed to generate page: {str(e)}", 500
    
    if os.path.exists(page_path):
        return send_file(page_path, mimetype='image/png')
    
    return "Page not found", 404


@app.route("/api/document/<doc_id>/account/<int:account_index>/page/<int:page_num>/data")
def get_account_page_data(doc_id, account_index, page_num):
    """Extract data for a specific page of an account - with S3 caching"""
    import fitz
    import json
    
    # CRITICAL FIX: page_num from URL is 1-based (from frontend), convert to 0-based for PDF operations
    page_num_0based = page_num - 1
    
    print(f"[API] üìÑ Page data request: doc_id={doc_id}, account={account_index}, page={page_num} (0-based: {page_num_0based})")
    
    # üöÄ PRIORITY 0: Check account's page_data first (from background processing)
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if doc:
        doc_data = doc.get("documents", [{}])[0]
        accounts = doc_data.get("accounts", [])
        
        if account_index < len(accounts):
            account = accounts[account_index]
            page_data = account.get("page_data", {})
            
            # Check if this page has data in the account's page_data (1-based keys)
            page_key = str(page_num)
            if page_key in page_data:
                print(f"[CACHE] ‚úÖ Serving page {page_num} from account page_data (account {account_index})")
                print(f"[CACHE] üìä Page data contains {len(page_data[page_key])} fields")
                response = jsonify({
                    "success": True,
                    "data": page_data[page_key],
                    "account_number": account.get("accountNumber", "Unknown"),
                    "cache_source": "account_page_data",
                    "cached": True
                })
                response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
                response.headers['Pragma'] = 'no-cache'
                response.headers['Expires'] = '0'
                return response
            else:
                print(f"[DEBUG] Page {page_num} not found in account page_data. Available pages: {list(page_data.keys())}")
    
    # üöÄ PRIORITY 1: Check background processor cache first (convert 1-based to 0-based)
    if background_processor.is_page_cached(doc_id, page_num_0based):
        cached_data = background_processor.get_cached_page_data(doc_id, page_num_0based)
        if cached_data and cached_data.get("extracted_data"):
            print(f"[CACHE] ‚úÖ Serving page {page_num} from background processing cache (account {account_index})")
            print(f"[CACHE] üìä Cache contains {len(cached_data.get('extracted_data', {}))} extracted fields")
            response = jsonify({
                "success": True,
                "data": cached_data["extracted_data"],
                "account_number": cached_data.get("account_number", "Unknown"),
                "cache_source": "background_processor",
                "extraction_time": cached_data.get("extraction_time"),
                "cached": True
            })
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            return response
    
    # Check background processing status
    bg_status = background_processor.get_document_status(doc_id)
    if bg_status and bg_status.get("stage") != DocumentProcessingStage.COMPLETED:
        print(f"[DEBUG] Document {doc_id} is being processed in background (stage: {bg_status.get('stage')})")
        return jsonify({
            "success": True,
            "processing_in_background": True,
            "stage": bg_status.get("stage"),
            "progress": bg_status.get("progress", 0),
            "pages_processed": bg_status.get("pages_processed", 0),
            "total_pages": bg_status.get("total_pages", 0),
            "message": "Page is being processed in background. Please wait..."
        })
    
    # CONSISTENCY FIX: Check for document-level extraction cache first
    doc_cache_key = f"document_extraction_cache/{doc_id}_account_{account_index}_page_{page_num}.json"
    try:
        cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=doc_cache_key)
        cached_data = json.loads(cached_result['Body'].read())
        print(f"[DEBUG] ‚úì Using document-level cached result for account {account_index} page {page_num} - GUARANTEED CONSISTENT")
        
        # Add cache headers to prevent browser caching issues
        response = jsonify(cached_data)
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        return response
    except Exception as e:
        print(f"[DEBUG] No document-level cache found for account {account_index} page {page_num}, extracting fresh: {str(e)}")
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        print(f"[ERROR] Document not found: {doc_id}")
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    print(f"[DEBUG] Document found, pdf_path={doc.get('pdf_path')}")
    
    try:
        # Check S3 cache first
        cache_key = f"page_data/{doc_id}/account_{account_index}/page_{page_num}.json"
        
        try:
            print(f"[DEBUG] Checking S3 cache: {cache_key}")
            cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = json.loads(cache_response['Body'].read().decode('utf-8'))
            
            # Check prompt version - invalidate old cache
            cached_version = cached_data.get("prompt_version", "v1")
            current_version = "v6_loan_document_prompt_fix"  # Updated to force re-extraction with correct prompt
            
            if cached_version != current_version:
                print(f"[DEBUG] Cache version mismatch ({cached_version} vs {current_version}) - will re-extract")
                raise Exception("Cache version outdated")
            
            print(f"[DEBUG] Found cached data in S3 (version {cached_version})")
            
            # CRITICAL: Apply flattening to cached data too!
            cached_fields = cached_data.get("data", {})
            cached_fields = flatten_nested_objects(cached_fields)
            print(f"[DEBUG] Applied flattening to cached data")
            
            response = jsonify({
                "success": True,
                "page_number": page_num + 1,
                "account_number": cached_data.get("account_number"),
                "data": cached_fields,
                "cached": True,
                "prompt_version": cached_version
            })
            # Prevent browser caching - always fetch fresh data
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            return response
        except s3_client.exceptions.NoSuchKey:
            print(f"[DEBUG] No cache found, will extract data")
        except Exception as cache_error:
            print(f"[DEBUG] Cache check failed: {str(cache_error)}, will extract data")
        
        # Get PDF path
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # OPTIMIZATION: Try to load OCR text from cache first
        page_text = None
        try:
            cache_key = f"ocr_cache/{doc_id}/text_cache.json"
            cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            ocr_cache = json.loads(cache_response['Body'].read().decode('utf-8'))
            # OCR cache uses 0-based page numbers
            page_text = ocr_cache.get(str(page_num_0based))
            if page_text:
                print(f"[DEBUG] Loaded page {page_num} (0-based: {page_num_0based}) text from OCR cache ({len(page_text)} chars)")
        except Exception as cache_err:
            print(f"[DEBUG] No OCR cache found, will extract text: {str(cache_err)}")
        
        # If not in cache, extract text from PDF
        if not page_text:
            print(f"[DEBUG] Opening PDF: {pdf_path}")
            pdf_doc = fitz.open(pdf_path)
            print(f"[DEBUG] PDF has {len(pdf_doc)} pages")
            
            if page_num_0based >= len(pdf_doc):  # page_num_0based is 0-based
                print(f"[ERROR] Page number {page_num} (0-based: {page_num_0based}) out of range (total pages: {len(pdf_doc)})")
                return jsonify({"success": False, "message": "Page number out of range"}), 404
            
            page = pdf_doc[page_num_0based]  # Use 0-based page number
            page_text = page.get_text()
            
            print(f"[DEBUG] Extracted {len(page_text)} characters from page {page_num}")
            print(f"[DEBUG] Page text preview: {page_text[:200]}")
            
            # Check if page has watermark or is mostly garbage text
            has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
            is_mostly_single_chars = page_text.count('\n') > len(page_text) / 3  # Too many line breaks
            
            # If no text found, has watermark, or is likely an image - use OCR
            if not page_text or len(page_text.strip()) < 50 or has_watermark or is_mostly_single_chars:
                print(f"[DEBUG] Page {page_num} has no text layer, using OCR...")
                
                # Save page as image temporarily
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                temp_image_path = os.path.join(OUTPUT_DIR, f"temp_page_{doc_id}_{page_num}.png")
                pix.save(temp_image_path)
                
                # Use Textract to extract text from the image
                try:
                    with open(temp_image_path, 'rb') as image_file:
                        image_bytes = image_file.read()
                    
                    textract_response = textract.detect_document_text(
                        Document={'Bytes': image_bytes}
                    )
                    
                    # Extract text from Textract response
                    page_text = ""
                    for block in textract_response.get('Blocks', []):
                        if block['BlockType'] == 'LINE':
                            page_text += block.get('Text', '') + "\n"
                    
                    print(f"[DEBUG] OCR extracted {len(page_text)} characters from page {page_num}")
                    
                    # Clean up temp file
                    if os.path.exists(temp_image_path):
                        os.remove(temp_image_path)
                        
                except Exception as ocr_error:
                    print(f"[ERROR] OCR failed: {str(ocr_error)}")
                    pdf_doc.close()
                    return jsonify({"success": False, "message": f"OCR failed: {str(ocr_error)}"}), 500
            else:
                print(f"[DEBUG] Extracted {len(page_text)} characters from page {page_num}")
            
            pdf_doc.close()
        
        # Get account info to know what account number this page belongs to
        doc_data = doc.get("documents", [{}])[0]
        accounts = doc_data.get("accounts", [])
        
        if account_index >= len(accounts):
            return jsonify({"success": False, "message": "Account index out of range"}), 400
        
        account = accounts[account_index]
        account_number = account.get("accountNumber", "Unknown")
        
        # CONSISTENCY FIX: Create content-based cache key for deterministic results
        import hashlib
        content_hash = hashlib.md5(page_text.encode('utf-8')).hexdigest()[:12]
        content_cache_key = f"content_extraction_cache/{content_hash}_account_{account_index}_page_{page_num}.json"
        
        # Check content-based cache first
        try:
            cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=content_cache_key)
            cached_data = json.loads(cached_result['Body'].read())
            print(f"[DEBUG] ‚úì Using content-based cached result (hash: {content_hash}) - GUARANTEED SAME CONTENT = SAME RESULT")
            
            # Still cache it in document-level cache for faster access
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=doc_cache_key,
                    Body=json.dumps(cached_data),
                    ContentType='application/json'
                )
            except:
                pass
            
            response = jsonify(cached_data)
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            return response
        except Exception as e:
            print(f"[DEBUG] No content-based cache found (hash: {content_hash}), will extract fresh")
        
        # Extract data from this specific page using AI
        print(f"[DEBUG] Calling AI to extract data from page {page_num}")
        
        # Use document-level type instead of page-level detection
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        print(f"[DEBUG] Using document-level type for page {page_num}: {doc_type}")
        
        # Use appropriate prompt based on document type (not page-level detection)
        if doc_type == "drivers_license":
            page_extraction_prompt = get_drivers_license_prompt()
            print(f"[DEBUG] Using specialized DL prompt for page {page_num}")
        elif doc_type == "loan_document":
            page_extraction_prompt = get_loan_document_prompt()
            print(f"[DEBUG] Using specialized LOAN DOCUMENT prompt for page {page_num}")
        else:
            page_extraction_prompt = get_comprehensive_extraction_prompt()
            print(f"[DEBUG] Using comprehensive prompt for page {page_num} (doc_type: {doc_type})")
        
        print(f"[DEBUG] Got page extraction prompt, calling Bedrock...")
        response = call_bedrock(page_extraction_prompt, page_text, max_tokens=8192)
        print(f"[DEBUG] Got response from Bedrock, length: {len(response)}")
        
        # Parse JSON response
        json_start = response.find('{')
        json_end = response.rfind('}')
        
        if json_start != -1 and json_end != -1:
            json_str = response[json_start:json_end + 1]
            try:
                parsed = json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"[ERROR] JSON parse error: {str(e)}")
                print(f"[ERROR] AI Response: {response[:500]}")
                return jsonify({
                    "success": False,
                    "message": f"Failed to parse AI response: {str(e)}",
                    "raw_response": response[:500]
                }), 500
            
            # Handle driver's license format: unwrap documents array if present
            if doc_type == "drivers_license" and "documents" in parsed:
                if len(parsed["documents"]) > 0:
                    doc_data = parsed["documents"][0]
                    # Extract the fields from extracted_fields
                    if "extracted_fields" in doc_data:
                        parsed = doc_data["extracted_fields"]
                        print(f"[DEBUG] Unwrapped driver's license data: {len(parsed)} fields")
                    else:
                        parsed = doc_data
            
            # MANDATORY: Always check for VERIFIED text using regex fallback
            import re
            
            # Check for VERIFIED text in the page content
            verified_patterns = [
                r'\bVERIFIED\b',
                r'\bVERIFICATION\b', 
                r'\bVERIFY\b',
                r'‚úì\s*VERIFIED',
                r'VERIFIED\s*[-‚Äì]\s*([A-Z\s]+)',  # VERIFIED - NAME pattern
            ]
            
            verification_found = False
            verified_by_name = None
            
            for pattern in verified_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    verification_found = True
                    print(f"[REGULAR] FALLBACK: Found VERIFIED text with pattern '{pattern}': {matches}")
                    
                    # Check for name after VERIFIED
                    name_match = re.search(r'VERIFIED\s*[-‚Äì]\s*([A-Z][A-Z\s]{2,20})', page_text, re.IGNORECASE)
                    if name_match:
                        verified_by_name = name_match.group(1).strip()
                        print(f"[REGULAR] FALLBACK: Found verified by name: {verified_by_name}")
                    break
            
            # Add VERIFIED fields if found but not extracted by Claude
            if verification_found:
                if not any(key.lower().startswith('verified') for key in parsed.keys()):
                    print(f"[REGULAR] FALLBACK: Adding missing VERIFIED field")
                    parsed["Verified"] = {
                        "value": "VERIFIED",
                        "confidence": 90
                    }
                    
                if verified_by_name and not any('verified_by' in key.lower() for key in parsed.keys()):
                    print(f"[REGULAR] FALLBACK: Adding missing Verified_By field: {verified_by_name}")
                    parsed["Verified_By"] = {
                        "value": verified_by_name,
                        "confidence": 85
                    }

            # CRITICAL: Flatten nested objects (Signer1: {Name: "John"} -> Signer1_Name: "John")
            parsed = flatten_nested_objects(parsed)
            print(f"[DEBUG] Flattened nested objects in parsed data")
            
            # Add account number to the result
            parsed["Account_Number"] = account_number
            
            # Cache the result in S3
            cache_data = {
                "account_number": account_number,
                "data": parsed,
                "extracted_at": datetime.now().isoformat()
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                print(f"[DEBUG] Cached data to S3: {cache_key}")
            except Exception as s3_error:
                print(f"[WARNING] Failed to cache to S3: {str(s3_error)}")
            
            result_data = {
                "success": True,
                "page_number": page_num + 1,
                "account_number": account_number,
                "data": parsed,
                "cached": False,
                "prompt_version": "v5_enhanced_verified"
            }
            
            # CONSISTENCY FIX: Normalize field names and values before caching
            print(f"[DEBUG] RAW AI RESPONSE BEFORE NORMALIZATION: {parsed}")
            normalized_data = normalize_extraction_result(parsed)
            print(f"[DEBUG] NORMALIZED DATA AFTER PROCESSING: {normalized_data}")
            
            # CONSISTENCY FIX: Ensure consistent field structure
            consistent_data = ensure_consistent_field_structure(normalized_data, page_text)
            print(f"[DEBUG] CONSISTENT DATA AFTER STRUCTURE CHECK: {consistent_data}")
            
            # CONSISTENCY FIX: Log field count for debugging
            field_count = len(consistent_data)
            print(f"[DEBUG] FINAL FIELD COUNT: {field_count} fields")
            print(f"[DEBUG] FIELD NAMES: {list(consistent_data.keys())}")
            
            result_data["data"] = consistent_data
            
            # Cache the result at document level for future consistency
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=doc_cache_key,
                    Body=json.dumps(result_data),
                    ContentType='application/json'
                )
                print(f"[DEBUG] ‚úì Cached document-level result for account {account_index} page {page_num} - ENSURES CONSISTENCY")
            except Exception as cache_error:
                print(f"[WARNING] Failed to cache document-level result: {cache_error}")
            
            # CONSISTENCY FIX: Also cache by content hash for deterministic results
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=content_cache_key,
                    Body=json.dumps(result_data),
                    ContentType='application/json'
                )
                print(f"[DEBUG] ‚úì Cached content-based result (hash: {content_hash}) - ENSURES SAME CONTENT = SAME RESULT")
            except Exception as cache_error:
                print(f"[WARNING] Failed to cache content-based result: {cache_error}")
            
            response = jsonify(result_data)
            # Prevent browser caching - always fetch fresh data
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
            return response
        else:
            return jsonify({
                "success": False,
                "message": "Failed to parse AI response"
            }), 500
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"[ERROR] Failed to extract page data: {str(e)}")
        print(f"[ERROR] Traceback: {error_trace}")
        return jsonify({"success": False, "message": str(e), "traceback": error_trace}), 500


@app.route("/api/document/<doc_id>/page/<int:page_num>/extract")
def extract_page_data(doc_id, page_num):
    """Extract data from a specific page - works for any document type"""
    import fitz
    import json
    
    print(f"[DEBUG] extract_page_data called: doc_id={doc_id}, page_num={page_num}")
    
    # üöÄ PRIORITY 1: Check death certificate cache first (for death certificates)
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if doc:
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        
        if doc_type == "death_certificate":
            try:
                # Check death certificate cache (convert 1-based to 0-based)
                cache_key = f"death_cert_page_data/{doc_id}/page_{page_num - 1}.json"
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                
                print(f"[DEBUG] ‚úÖ Using DEATH CERTIFICATE cached result for page {page_num}")
                return jsonify({
                    "success": True,
                    "extracted_fields": cached_data.get("extracted_data", {}),
                    "page_number": cached_data.get("page_number", page_num - 1),
                    "document_type": cached_data.get("document_type", "death_certificate"),
                    "cache_source": "death_certificate_cache",
                    "extraction_time": cached_data.get("extraction_time"),
                    "cached": True
                })
            except Exception as e:
                print(f"[DEBUG] No death certificate cache found for page {page_num}: {str(e)}")
    
    # üöÄ PRIORITY 2: Check background processor cache (convert 1-based to 0-based)
    if background_processor.is_page_cached(doc_id, page_num - 1):
        cached_data = background_processor.get_cached_page_data(doc_id, page_num - 1)
        if cached_data and cached_data.get("extracted_data"):
            print(f"[DEBUG] ‚úÖ Using BACKGROUND PROCESSOR cached result for page {page_num}")
            return jsonify({
                "success": True,
                "extracted_fields": cached_data["extracted_data"],
                "account_number": cached_data.get("account_number", "Unknown"),
                "cache_source": "background_processor",
                "extraction_time": cached_data.get("extraction_time"),
                "cached": True
            })
    
    # Check background processing status
    bg_status = background_processor.get_document_status(doc_id)
    if bg_status and bg_status.get("stage") != DocumentProcessingStage.COMPLETED:
        print(f"[DEBUG] Document {doc_id} is being processed in background (stage: {bg_status.get('stage')})")
        # Return processing status instead of extracting
        return jsonify({
            "success": True,
            "processing_in_background": True,
            "stage": bg_status.get("stage"),
            "progress": bg_status.get("progress", 0),
            "pages_processed": bg_status.get("pages_processed", 0),
            "total_pages": bg_status.get("total_pages", 0),
            "message": "Page is being processed in background. Please wait..."
        })
    
    # CONSISTENCY FIX: Check for document-level extraction cache first
    doc_cache_key = f"document_extraction_cache/{doc_id}_page_{page_num}.json"
    try:
        cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=doc_cache_key)
        cached_data = json.loads(cached_result['Body'].read())
        print(f"[DEBUG] ‚úì Using document-level cached result for page {page_num} - GUARANTEED CONSISTENT")
        return jsonify(cached_data)
    except:
        print(f"[DEBUG] No document-level cache found for page {page_num}, extracting fresh")
    
    # Check if force re-extraction is requested
    force = request.args.get('force', 'false').lower() == 'true'
    
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    try:
        # For single-page documents or first page, use the document's extracted_fields if available
        # This ensures all data from initial processing is shown
        # BUT: If cache was cleared, force re-extraction
        cache_was_cleared = doc.get("cache_cleared", False)
        
        if page_num == 0 and not force and not cache_was_cleared:
            doc_data = doc.get("documents", [{}])[0] if doc.get("documents") else doc
            extracted_fields = doc_data.get("extracted_fields", {})
            
            if extracted_fields and len(extracted_fields) > 0:
                print(f"[DEBUG] Using document's extracted_fields for page 0 ({len(extracted_fields)} fields)")
                
                # POST-PROCESSING: For death certificates, rename certificate_number to account_number
                doc_type = doc_data.get("document_type", "")
                if doc_type == "death_certificate" or "death" in doc.get("document_name", "").lower():
                    if "certificate_number" in extracted_fields and "account_number" not in extracted_fields:
                        extracted_fields["account_number"] = extracted_fields["certificate_number"]
                        del extracted_fields["certificate_number"]
                        print(f"[DEBUG] Renamed certificate_number to account_number: {extracted_fields['account_number']}")
                    if "Certificate_Number" in extracted_fields and "Account_Number" not in extracted_fields:
                        extracted_fields["Account_Number"] = extracted_fields["Certificate_Number"]
                        del extracted_fields["Certificate_Number"]
                        print(f"[DEBUG] Renamed Certificate_Number to Account_Number: {extracted_fields['Account_Number']}")
                
                # Cache this data to S3 for consistency (convert 1-based to 0-based)
                cache_key = f"page_data/{doc_id}/page_{page_num - 1}.json"
                cache_data = {
                    "data": extracted_fields,
                    "extracted_at": datetime.now().isoformat(),
                    "source": "document_fields"
                }
                
                try:
                    s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=cache_key,
                        Body=json.dumps(cache_data),
                        ContentType='application/json'
                    )
                    print(f"[DEBUG] Cached document fields to S3: {cache_key}")
                except Exception as s3_error:
                    print(f"[WARNING] Failed to cache to S3: {str(s3_error)}")
                
                return jsonify({
                    "success": True,
                    "page_number": page_num + 1,
                    "data": extracted_fields,
                    "cached": False,
                    "source": "document_fields"
                })
        
        # Check S3 cache first (unless force=true) - convert 1-based to 0-based
        cache_key = f"page_data/{doc_id}/page_{page_num - 1}.json"
        
        if not force:
            try:
                print(f"[DEBUG] Checking S3 cache: {cache_key}")
                cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cache_response['Body'].read().decode('utf-8'))
                print(f"[DEBUG] Found cached data in S3")
                
                # CRITICAL: Apply flattening to cached data too!
                cached_fields = cached_data.get("data", {})
                cached_fields = flatten_nested_objects(cached_fields)
                print(f"[DEBUG] Applied flattening to cached data")
                
                return jsonify({
                    "success": True,
                    "page_number": page_num + 1,
                    "data": cached_fields,
                    "cached": True,
                    "edited": cached_data.get("edited", False)
                })
            except s3_client.exceptions.NoSuchKey:
                print(f"[DEBUG] No cache found, will extract data")
            except Exception as cache_error:
                print(f"[DEBUG] Cache check failed: {str(cache_error)}, will extract data")
        else:
            print(f"[DEBUG] Force re-extraction requested, skipping cache")
        
        # Get PDF path
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # Extract text from the specific page
        print(f"[DEBUG] Opening PDF: {pdf_path}")
        pdf_doc = fitz.open(pdf_path)
        
        if page_num > len(pdf_doc):  # page_num is 1-based from URL
            return jsonify({"success": False, "message": "Page number out of range"}), 404
        
        page = pdf_doc[page_num - 1]  # Convert 1-based page_num to 0-based PDF index
        page_text = page.get_text()
        
        # If no text found, use OCR
        if not page_text or len(page_text.strip()) < 50:
            print(f"[DEBUG] Page {page_num} has no text layer, using OCR...")
            
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
            temp_image_path = os.path.join(OUTPUT_DIR, f"temp_page_{doc_id}_{page_num}.png")
            pix.save(temp_image_path)
            
            try:
                with open(temp_image_path, 'rb') as image_file:
                    image_bytes = image_file.read()
                
                textract_response = textract.detect_document_text(
                    Document={'Bytes': image_bytes}
                )
                
                page_text = ""
                for block in textract_response.get('Blocks', []):
                    if block['BlockType'] == 'LINE':
                        page_text += block.get('Text', '') + "\n"
                
                print(f"[DEBUG] OCR extracted {len(page_text)} characters from page {page_num}")
                
                if os.path.exists(temp_image_path):
                    os.remove(temp_image_path)
                    
            except Exception as ocr_error:
                print(f"[ERROR] OCR failed: {str(ocr_error)}")
                pdf_doc.close()
                return jsonify({"success": False, "message": f"OCR failed: {str(ocr_error)}"}), 500
        
        pdf_doc.close()
        
        # Extract data using AI
        print(f"[DEBUG] Calling AI to extract data from page {page_num}")
        
        # Detect document type on this page
        detected_type = detect_document_type(page_text)
        print(f"[DEBUG] Detected document type: {detected_type}")
        
        # Use appropriate prompt
        if detected_type == "drivers_license":
            page_extraction_prompt = get_drivers_license_prompt()
            print(f"[DEBUG] Using specialized driver's license prompt")
        else:
            page_extraction_prompt = get_comprehensive_extraction_prompt()
            print(f"[DEBUG] Using comprehensive extraction prompt")
        
        response = call_bedrock(page_extraction_prompt, page_text, max_tokens=8192)
        print(f"[DEBUG] Got response from Bedrock, length: {len(response)}")
        
        # Parse JSON response
        json_start = response.find('{')
        json_end = response.rfind('}')
        
        if json_start != -1 and json_end != -1:
            json_str = response[json_start:json_end + 1]
            parsed = json.loads(json_str)
            
            # Handle driver's license format: unwrap documents array if present
            if detected_type == "drivers_license" and "documents" in parsed:
                if len(parsed["documents"]) > 0:
                    doc_data = parsed["documents"][0]
                    # Extract the fields from extracted_fields
                    if "extracted_fields" in doc_data:
                        parsed = doc_data["extracted_fields"]
                        print(f"[DEBUG] Unwrapped driver's license data: {len(parsed)} fields")
                    else:
                        parsed = doc_data
            
            # KEEP confidence format intact - don't normalize
            # The frontend expects {value: "X", confidence: 95} format
            print(f"[DEBUG] Keeping confidence format intact for frontend processing")
            
            # POST-PROCESSING: For death certificates, rename certificate_number to account_number
            if doc.get("document_type") == "death_certificate" or "death" in doc.get("document_name", "").lower():
                if "certificate_number" in parsed and "account_number" not in parsed:
                    parsed["account_number"] = parsed["certificate_number"]
                    del parsed["certificate_number"]
                if "Certificate_Number" in parsed and "Account_Number" not in parsed:
                    parsed["Account_Number"] = parsed["Certificate_Number"]
                    del parsed["Certificate_Number"]
            
            # Cache the result in S3
            cache_data = {
                "data": parsed,
                "extracted_at": datetime.now().isoformat()
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=cache_key,
                    Body=json.dumps(cache_data),
                    ContentType='application/json'
                )
                print(f"[DEBUG] Cached data to S3: {cache_key}")
            except Exception as s3_error:
                print(f"[WARNING] Failed to cache to S3: {str(s3_error)}")
            
            # CONSISTENCY FIX: Cache the result at document level for future consistency
            # CONSISTENCY FIX: Normalize field names and values before caching
            normalized_data = normalize_extraction_result(parsed)
            
            result_data = {
                "success": True,
                "page_number": page_num + 1,
                "data": normalized_data,
                "cached": False
            }
            
            try:
                s3_client.put_object(
                    Bucket=S3_BUCKET,
                    Key=doc_cache_key,
                    Body=json.dumps(result_data),
                    ContentType='application/json'
                )
                print(f"[DEBUG] ‚úì Cached document-level result for page {page_num} - ENSURES CONSISTENCY")
            except Exception as cache_error:
                print(f"[WARNING] Failed to cache document-level result: {cache_error}")
            
            return jsonify(result_data)
        else:
            return jsonify({
                "success": False,
                "message": "Failed to parse AI response"
            }), 500
            
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"[ERROR] Failed to extract page data: {str(e)}")
        print(f"[ERROR] Traceback: {error_trace}")
        return jsonify({"success": False, "message": str(e)}), 500



@app.route("/api/document/<doc_id>/account/<int:account_index>/page/<int:page_index>/extract-progressive", methods=["POST"])
def extract_page_progressive(doc_id, account_index, page_index):
    """Progressive page extraction - extract one page at a time in background"""
    import json
    import time
    from datetime import datetime
    
    print(f"[PROGRESSIVE] Starting extraction for doc {doc_id}, account {account_index}, page {page_index}")
    
    try:
        # Get request data
        data = request.get_json() or {}
        priority = data.get('priority', 2)
        
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Get account info
        doc_data = doc.get("documents", [{}])[0]
        accounts = doc_data.get("accounts", [])
        
        if account_index >= len(accounts):
            return jsonify({"success": False, "message": "Account index out of range"}), 400
        
        account = accounts[account_index]
        account_number = account.get("accountNumber", "N/A")
        
        # Get PDF path
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # Use the actual PDF page number for cache key to match regular /data endpoint
        # The page_index parameter is the actual PDF page number from the frontend
        cache_key = f"page_data/{doc_id}/account_{account_index}/page_{page_index}.json"
        try:
            cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = json.loads(cached_result['Body'].read())
            
            # Check prompt version - invalidate old cache
            cached_version = cached_data.get("prompt_version", "v1")
            current_version = "v5_enhanced_verified"
            
            if cached_version != current_version:
                print(f"[PROGRESSIVE] Cache version mismatch ({cached_version} vs {current_version}) - will re-extract")
                raise Exception("Cache version outdated")
            
            # Check if cached data contains only errors or is invalid
            cached_fields = cached_data.get("data", {})
            if isinstance(cached_fields, dict):
                # Skip cache if it only contains error messages
                if len(cached_fields) == 1 and "error" in cached_fields:
                    print(f"[PROGRESSIVE] Cached data contains only error, re-extracting for account {account_number}, page {page_index}")
                    raise Exception("Cached data is invalid, re-extract")
                
                # Skip cache if error message mentions watermarks
                error_msg = cached_fields.get("error", "")
                if "watermark" in str(error_msg).lower() or "pdf-xchange" in str(error_msg).lower():
                    print(f"[PROGRESSIVE] Cached data contains watermark error, re-extracting for account {account_number}, page {page_index}")
                    raise Exception("Cached data contains watermark error, re-extract")
            
            # Always re-extract to get fresh data with enhanced VERIFIED detection
            print(f"[PROGRESSIVE] Forcing fresh extraction for account {account_number}, page {page_index}")
            raise Exception("Force fresh extraction")
                
        except Exception as e:
            print(f"[PROGRESSIVE] Extracting fresh for account {account_number}, page {page_index}: {str(e)}")
        
        # Extract text from the specific page
        import fitz
        pdf_doc = fitz.open(pdf_path)
        
        print(f"[PROGRESSIVE] PDF has {len(pdf_doc)} pages, extracting page {page_index} (0-based)")
        
        if page_index >= len(pdf_doc):
            pdf_doc.close()
            return jsonify({"success": False, "message": f"Page index {page_index} out of range (PDF has {len(pdf_doc)} pages)"}), 400
        
        page = pdf_doc[page_index]
        page_text = page.get_text()
        
        print(f"[PROGRESSIVE] Extracted {len(page_text)} characters from page {page_index}")
        print(f"[PROGRESSIVE] First 200 chars: {page_text[:200]}")
        
        # Check for watermark content and apply OCR if needed
        has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
        has_little_text = len(page_text.strip()) < 100
        
        if has_watermark or has_little_text:
            print(f"[PROGRESSIVE] Page {page_index} needs OCR (watermark: {has_watermark}, little text: {has_little_text})")
            
            try:
                # Convert page to image with higher resolution for better OCR
                # Note: PDF is still open, so we can access the page
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x resolution
                
                temp_image_path = os.path.join(OUTPUT_DIR, f"temp_progressive_{doc_id}_{page_index}.png")
                pix.save(temp_image_path)
                
                with open(temp_image_path, 'rb') as image_file:
                    image_bytes = image_file.read()
                
                print(f"[PROGRESSIVE] Running Textract OCR on page {page_index}...")
                
                # Use Textract for OCR
                textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                
                # Track Textract cost
                try:
                    cost_tracker = get_cost_tracker(doc_id)
                    cost_tracker.track_textract_sync(pages=1)
                    print(f"[COST] ‚úÖ Tracked Textract cost for page {page_index}")
                except Exception as e:
                    print(f"[COST] Failed to track Textract cost: {str(e)}")
                
                ocr_text = ""
                for block in textract_response.get('Blocks', []):
                    if block['BlockType'] == 'LINE':
                        ocr_text += block.get('Text', '') + "\n"
                
                # Clean up temp file
                if os.path.exists(temp_image_path):
                    os.remove(temp_image_path)
                
                if ocr_text.strip() and len(ocr_text.strip()) > len(page_text.strip()):
                    page_text = ocr_text
                    print(f"[PROGRESSIVE] ‚úì OCR extracted {len(page_text)} characters from page {page_index}")
                    print(f"[PROGRESSIVE] OCR first 200 chars: {page_text[:200]}")
                else:
                    print(f"[PROGRESSIVE] OCR didn't improve text for page {page_index}")
                    
            except Exception as ocr_error:
                print(f"[PROGRESSIVE] OCR error on page {page_index}: {str(ocr_error)}")
        
        # Close PDF after text extraction and OCR
        pdf_doc.close()
        
        if not page_text.strip():
            return jsonify({"success": False, "message": "No text found on page"}), 400
        
        # Use Claude AI to extract data from this page
        import boto3
        bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
        MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"
        
        # Get document type for appropriate prompt
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        
        # Use comprehensive extraction prompt with enhanced VERIFIED detection
        prompt = """
You are a data extraction expert. Extract ALL structured data from this document page.

üî¥üî¥üî¥ CRITICAL PRIORITY #1: VERIFICATION DETECTION üî¥üî¥üî¥
**THIS IS THE MOST IMPORTANT TASK - NEVER SKIP VERIFICATION DETECTION**

**STEP 1: MANDATORY VERIFICATION SCAN (DO THIS FIRST):**
Before extracting any other data, you MUST scan the ENTIRE page for verification indicators:

1. **SEARCH FOR "VERIFIED" TEXT EVERYWHERE:**
   - Look for "VERIFIED" stamps, seals, or text ANYWHERE on the page
   - Look for "VERIFICATION" text or stamps  
   - Look for "VERIFY" or "VERIFIED BY" text
   - Look for checkboxes or boxes marked with "VERIFIED"
   - Look for "‚úì VERIFIED" or similar checkmark combinations
   - Search in margins, corners, stamps, seals, form fields, and document body
   - Extract as: Verified: {"value": "VERIFIED", "confidence": 95}

2. **SEARCH FOR NAMES NEAR VERIFICATION:**
   - Look for names immediately after "VERIFIED" (like "VERIFIED - RENDA", "VERIFIED BRENDA HALLSTEAT")
   - Look for "VERIFIED BY: [NAME]" patterns
   - Look for names in verification stamps or seals
   - Extract as: Verified_By: {"value": "Name", "confidence": 85}
   
3. **SEARCH FOR VERIFICATION DATES:**
   - Look for dates on or near verification stamps
   - Look for "VERIFIED ON: [DATE]" patterns
   - Extract as: Verified_Date: {"value": "Date", "confidence": 85}

üö® **VERIFICATION DETECTION RULES:**
- **NEVER** skip verification detection - it must be checked on EVERY page
- **ALWAYS** scan the ENTIRE page text for "VERIFIED" (case-insensitive)
- **ALWAYS** extract verification fields if found, even if unclear
- If you find ANY verification indicator, you MUST extract it
- Look in ALL parts of the page: headers, footers, margins, stamps, seals, form fields

üî¥üî¥üî¥ VERIFICATION EXAMPLES - STUDY THESE PATTERNS üî¥üî¥üî¥

**COMMON VERIFICATION PATTERNS TO LOOK FOR:**
- "VERIFIED" (standalone stamp)
- "VERIFIED - [NAME]" (stamp with name)
- "VERIFIED BY: [NAME]" (formal verification)
- "VERIFIED [FULL NAME]" (like "VERIFIED BRENDA HALLSTEAT")
- "‚úì VERIFIED" (checkmark with verified)
- "VERIFICATION COMPLETE" (process completion)
- "DOCUMENT VERIFIED" (document validation)
- "IDENTITY VERIFIED" (identity confirmation)
- "SIGNATURE VERIFIED" (signature validation)

**EXAMPLE EXTRACTIONS:**
- Text: "VERIFIED" ‚Üí Extract: Verified: {"value": "VERIFIED", "confidence": 95}
- Text: "VERIFIED - RENDA" ‚Üí Extract: Verified: {"value": "VERIFIED", "confidence": 95}, Verified_By: {"value": "RENDA", "confidence": 90}
- Text: "VERIFIED BRENDA HALLSTEAT" ‚Üí Extract: Verified: {"value": "VERIFIED", "confidence": 95}, Verified_By: {"value": "BRENDA HALLSTEAT", "confidence": 90}
- Text: "VERIFIED BY: MARIA SANTOS" ‚Üí Extract: Verified: {"value": "VERIFIED", "confidence": 95}, Verified_By: {"value": "MARIA SANTOS", "confidence": 90}
- Text: "‚úì VERIFIED 03/15/2024" ‚Üí Extract: Verified: {"value": "VERIFIED", "confidence": 95}, Verified_Date: {"value": "03/15/2024", "confidence": 90}
- Text: "VERIFICATION COMPLETE" ‚Üí Extract: Verified: {"value": "VERIFIED", "confidence": 90}

üö® **CRITICAL REMINDER:**
- Verification detection is MANDATORY on every page
- Even if the text is unclear, extract it with lower confidence
- Look in ALL areas of the page, not just the main content
- If you're unsure, extract it anyway - better to have false positives than miss verification

**STEP 2: OTHER CRITICAL FIELDS (AFTER VERIFICATION):**
After completing verification detection, extract these fields:

4. **IDENTIFYING NUMBERS** - Extract ALL significant numbers:
   - Certificate Numbers (like "22156777") - Extract as Certificate_Number
   - Account Numbers - Extract as Account_Number
   - File Numbers - Extract as File_Number
   - State File Numbers - Extract as State_File_Number
   - Any 6-12 digit numbers - Extract as Account_Number

5. **NAMES** - All person names:
   - Main person name - Extract as Full_Name or Deceased_Name
   - Registrar names - Extract as Registrar_Name
   - Any other names - Extract with appropriate field names

6. **ADDRESSES** - Full addresses - Extract as Address, Residence_Address, etc.

7. **DATES** - All dates:
   - Birth dates - Extract as Date_of_Birth
   - Death dates - Extract as Date_of_Death
   - Issue dates - Extract as Issue_Date
   - Stamp dates - Extract as Stamp_Date

8. **OTHER INFORMATION:**
   - Phone numbers - Extract as Phone_Number
   - SSN - Extract as SSN
   - Places - Extract as Place_of_Birth, Place_of_Death, etc.

Return ONLY valid JSON in this format:
{
  "Field_Name": {
    "value": "extracted value",
    "confidence": 95
  }
}

CONFIDENCE SCORE GUIDELINES:
- 90-100: Text is clear, printed, and easily readable
- 70-89: Text is readable but slightly unclear (handwritten, faded, or small)
- 50-69: Text is partially unclear or ambiguous
- 30-49: Text is difficult to read or uncertain

üî¥ REMEMBER: ALWAYS check for "VERIFIED" text on EVERY page - this is mandatory!
"""
        
        # Call Claude AI
        claude_request = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4000,
            "temperature": 0,
            "messages": [{
                "role": "user",
                "content": f"{prompt}\n\nPage text:\n{page_text}"
            }]
        }
        
        start_time = time.time()
        response = bedrock.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(claude_request)
        )
        
        response_body = json.loads(response['body'].read())
        claude_response = response_body['content'][0]['text']
        
        # Track Bedrock cost
        try:
            input_tokens = response_body.get('usage', {}).get('input_tokens', 0)
            output_tokens = response_body.get('usage', {}).get('output_tokens', 0)
            
            if input_tokens > 0 or output_tokens > 0:
                cost_tracker = get_cost_tracker(doc_id)
                cost_tracker.track_bedrock_call(input_tokens, output_tokens)
                print(f"[COST] ‚úÖ Tracked Bedrock cost for progressive extraction: {input_tokens} input + {output_tokens} output tokens")
        except Exception as e:
            print(f"[COST] Failed to track Bedrock cost: {str(e)}")
        
        # Parse extracted data with better error handling
        try:
            print(f"[PROGRESSIVE] Claude response length: {len(claude_response)}")
            print(f"[PROGRESSIVE] Claude response preview: {claude_response[:300]}...")
            
            if not claude_response.strip():
                print(f"[PROGRESSIVE] Empty response from Claude AI")
                return jsonify({"success": False, "message": "Empty response from Claude AI"}), 500
            
            # Clean up Claude response - sometimes it includes extra text
            claude_response_clean = claude_response.strip()
            
            # Try to extract JSON from the response
            json_start = claude_response_clean.find('{')
            json_end = claude_response_clean.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = claude_response_clean[json_start:json_end]
                extracted_fields = json.loads(json_text)
            else:
                # Fallback: try to parse the entire response
                extracted_fields = json.loads(claude_response_clean)
            
            # MANDATORY: Always check for VERIFIED text using regex fallback with enhanced name detection
            import re
            
            # ENHANCED VERIFIED DETECTION - Check for VERIFIED text in the page content with comprehensive patterns
            verified_patterns = [
                r'\bVERIFIED\b',
                r'\bVERIFICATION\b', 
                r'\bVERIFY\b',
                r'‚úì\s*VERIFIED',
                r'VERIFIED\s*[-‚Äì]\s*([A-Z][A-Z\s]{2,30})',  # VERIFIED - NAME pattern
                r'VERIFIED\s*BY\s*:?\s*([A-Z][A-Z\s]{2,30})',  # VERIFIED BY: NAME pattern
                r'VERIFIED\s*:\s*([A-Z][A-Z\s]{2,30})',  # VERIFIED: NAME pattern
                r'VERIFIED\s+([A-Z][A-Z\s]{2,30})',        # VERIFIED FULL NAME (like "VERIFIED BRENDA HALLSTEAT")
                r'‚òë\s*VERIFIED',  # Checkbox with VERIFIED
                r'‚úì.*VERIFIED',   # Checkmark with VERIFIED
                r'VERIFIED.*‚úì',   # VERIFIED with checkmark
            ]
            
            verification_found = False
            verified_by_name = None
            
            # Check each pattern and log what we find
            for pattern in verified_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    verification_found = True
                    print(f"[PROGRESSIVE] FALLBACK: Found VERIFIED text with pattern '{pattern}': {matches}")
                    
                    # Try to extract name from the match if it's a capturing group
                    if matches and isinstance(matches[0], str) and len(matches[0]) > 1:
                        potential_name = matches[0].strip()
                        if len(potential_name) > 2 and not potential_name.upper() == "VERIFIED":
                            verified_by_name = potential_name
                            print(f"[PROGRESSIVE] FALLBACK: Extracted name from pattern: '{verified_by_name}'")
                    break
            
            # Additional comprehensive search for VERIFIED text
            if not verification_found:
                # Case-insensitive search for any occurrence of "verified"
                if re.search(r'verified', page_text, re.IGNORECASE):
                    verification_found = True
                    print(f"[PROGRESSIVE] FALLBACK: Found 'verified' text (case-insensitive)")
                
                # Search for common verification phrases
                verification_phrases = [
                    r'verification\s+complete',
                    r'document\s+verified',
                    r'identity\s+verified',
                    r'signature\s+verified',
                    r'verified\s+copy',
                    r'verified\s+true',
                    r'verified\s+correct'
                ]
                
                for phrase in verification_phrases:
                    if re.search(phrase, page_text, re.IGNORECASE):
                        verification_found = True
                        print(f"[PROGRESSIVE] FALLBACK: Found verification phrase: '{phrase}'")
                        break
            
            # Enhanced name extraction - try multiple patterns to get complete names like "BRENDA HALLSTEAT"
            if verification_found and not verified_by_name:
                name_patterns = [
                    r'VERIFIED\s*[-‚Äì]\s*([A-Z][A-Z\s]{2,30})',  # VERIFIED - FULL NAME (extended length)
                    r'VERIFIED\s+([A-Z][A-Z\s]{2,30})',        # VERIFIED FULL NAME (no dash) - like "VERIFIED BRENDA HALLSTEAT"
                    r'VERIFIED\s*:\s*([A-Z][A-Z\s]{2,30})',    # VERIFIED: FULL NAME
                    r'VERIFIED\s*BY\s*:?\s*([A-Z][A-Z\s]{2,30})', # VERIFIED BY: FULL NAME
                    r'VERIFIED\s*[-‚Äì]\s*([A-Z]+\s+[A-Z]+)',    # VERIFIED - FIRSTNAME LASTNAME
                    r'VERIFIED\s*BY\s*([A-Z]+)',               # VERIFIED BY NAME (single word)
                    r'VERIFIED\s*-\s*([A-Z]+)',                # VERIFIED - NAME (single word)
                ]
                
                for name_pattern in name_patterns:
                    name_match = re.search(name_pattern, page_text, re.IGNORECASE)
                    if name_match:
                        full_name = name_match.group(1).strip()
                        # Clean up the name - remove extra spaces and validate
                        full_name = re.sub(r'\s+', ' ', full_name)  # Replace multiple spaces with single space
                        
                        # Accept names that look valid (1+ words, reasonable length)
                        if len(full_name) >= 2 and len(full_name) <= 30:
                            verified_by_name = full_name
                            print(f"[PROGRESSIVE] FALLBACK: Found complete verified by name: '{verified_by_name}' using pattern '{name_pattern}'")
                            break
                
                # If no name found with patterns, try a broader search around VERIFIED
                if not verified_by_name:
                    # Look for names within 100 characters after VERIFIED (increased range)
                    verified_context = re.search(r'VERIFIED.{0,100}', page_text, re.IGNORECASE | re.DOTALL)
                    if verified_context:
                        context_text = verified_context.group(0)
                        print(f"[PROGRESSIVE] FALLBACK: Searching for names in context: '{context_text[:100]}...'")
                        
                        # Extract potential names (capitalized words)
                        name_candidates = re.findall(r'\b[A-Z][A-Z\s]{1,25}\b', context_text)
                        for candidate in name_candidates:
                            candidate = candidate.strip()
                            # Skip if it's just "VERIFIED" or common words
                            skip_words = ["VERIFIED", "BY", "DATE", "STAMP", "SEAL", "DOCUMENT", "COPY", "TRUE", "CORRECT"]
                            if candidate not in skip_words and len(candidate) >= 2:
                                verified_by_name = candidate
                                print(f"[PROGRESSIVE] FALLBACK: Found verified by name from context: '{verified_by_name}'")
                                break
                    
                    # Also try looking before VERIFIED
                    if not verified_by_name:
                        verified_context_before = re.search(r'.{0,50}VERIFIED', page_text, re.IGNORECASE | re.DOTALL)
                        if verified_context_before:
                            context_text = verified_context_before.group(0)
                            # Look for names right before VERIFIED
                            name_candidates = re.findall(r'\b[A-Z][A-Z\s]{1,25}\b', context_text)
                            if name_candidates:
                                # Take the last name candidate (closest to VERIFIED)
                                candidate = name_candidates[-1].strip()
                                skip_words = ["VERIFIED", "BY", "DATE", "STAMP", "SEAL", "DOCUMENT", "COPY", "TRUE", "CORRECT"]
                                if candidate not in skip_words and len(candidate) >= 2:
                                    verified_by_name = candidate
                                    print(f"[PROGRESSIVE] FALLBACK: Found verified by name before VERIFIED: '{verified_by_name}'")
            
            # CRITICAL: Always add VERIFIED fields if found, even if Claude extracted them
            # This ensures consistency and prevents missing VERIFIED detection
            if verification_found:
                # Always add or update the Verified field
                verified_field_exists = any(key.lower().startswith('verified') and not 'by' in key.lower() for key in extracted_fields.keys())
                
                if not verified_field_exists:
                    print(f"[PROGRESSIVE] FALLBACK: Adding missing VERIFIED field")
                    extracted_fields["Verified"] = {
                        "value": "VERIFIED",
                        "confidence": 95
                    }
                else:
                    print(f"[PROGRESSIVE] FALLBACK: VERIFIED field already exists, ensuring it's marked as found")
                    # Update confidence if Claude found it with lower confidence
                    for key in extracted_fields.keys():
                        if key.lower().startswith('verified') and not 'by' in key.lower():
                            if extracted_fields[key].get("confidence", 0) < 95:
                                extracted_fields[key]["confidence"] = 95
                                print(f"[PROGRESSIVE] FALLBACK: Updated {key} confidence to 95")
                
                # Add verified by name if found
                if verified_by_name:
                    verified_by_exists = any('verified_by' in key.lower() or 'verifiedby' in key.lower() for key in extracted_fields.keys())
                    
                    if not verified_by_exists:
                        print(f"[PROGRESSIVE] FALLBACK: Adding missing Verified_By field: {verified_by_name}")
                        extracted_fields["Verified_By"] = {
                            "value": verified_by_name,
                            "confidence": 85
                        }
                    else:
                        print(f"[PROGRESSIVE] FALLBACK: Verified_By field already exists")
                
                # Log final verification status
                print(f"[PROGRESSIVE] VERIFICATION SUMMARY:")
                print(f"  - Verification found: {verification_found}")
                print(f"  - Verified by name: {verified_by_name}")
                print(f"  - Total VERIFIED fields in result: {len([k for k in extracted_fields.keys() if 'verified' in k.lower()])}")
                
                # Show all verification-related fields
                for key, value in extracted_fields.items():
                    if 'verified' in key.lower():
                        print(f"  - {key}: {value}")
            else:
                print(f"[PROGRESSIVE] VERIFICATION SUMMARY: No VERIFIED text found on this page")
                print(f"[PROGRESSIVE] Page text sample for verification check: {page_text[:300]}...")

            print(f"[PROGRESSIVE] Successfully parsed {len(extracted_fields)} fields")
            
        except json.JSONDecodeError as e:
            print(f"[PROGRESSIVE] JSON parse error: {e}")
            print(f"[PROGRESSIVE] Raw Claude response: '{claude_response}'")
            return jsonify({"success": False, "message": f"Failed to parse extraction result: {str(e)}"}), 500
        
        extraction_time = time.time() - start_time
        
        # Cache the result in the same format as regular /data endpoint
        cache_data = {
            "success": True,
            "page_number": page_index + 1,
            "account_number": account_number,
            "data": extracted_fields,
            "cached": False,
            "prompt_version": "v5_enhanced_verified",  # Updated version to invalidate old cache
            "extraction_method": "progressive",
            "extracted_at": datetime.now().isoformat(),
            "extraction_time_seconds": round(extraction_time, 2),
            "priority": priority,
            "doc_type": doc_type
        }
        
        try:
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
            print(f"[PROGRESSIVE] ‚úì Cached extraction result: {cache_key}")
        except Exception as s3_error:
            print(f"[PROGRESSIVE] Warning: Failed to cache result: {s3_error}")
        
        fields_count = len(extracted_fields)
        print(f"[PROGRESSIVE] ‚úÖ Extracted {fields_count} fields from account {account_number}, page {page_index} in {extraction_time:.2f}s")
        
        return jsonify({
            "success": True,
            "cached": False,
            "fieldsExtracted": fields_count,
            "extractedAt": datetime.now().isoformat(),
            "accountNumber": account_number,
            "extractionTime": round(extraction_time, 2),
            "extractedFields": extracted_fields
        })
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"[PROGRESSIVE] Error extracting page: {str(e)}")
        print(f"[PROGRESSIVE] Traceback: {error_trace}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/page/<int:page_index>/extract-progressive", methods=["POST"])
def extract_regular_page_progressive(doc_id, page_index):
    """Progressive page extraction for regular documents (non-account-based) - extract one page at a time in background"""
    import json
    import time
    from datetime import datetime
    
    print(f"[PROGRESSIVE-REGULAR] Starting regular page extraction for doc {doc_id}, page {page_index}")
    
    try:
        # Get request data
        data = request.get_json() or {}
        priority = data.get('priority', 2)
        
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Get PDF path
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # Use regular page cache key (no account index)
        cache_key = f"page_data/{doc_id}/page_{page_index}.json"
        try:
            cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            cached_data = json.loads(cached_result['Body'].read())
            
            # Check prompt version - invalidate old cache
            cached_version = cached_data.get("prompt_version", "v1")
            current_version = "v5_enhanced_verified"
            
            if cached_version != current_version:
                print(f"[PROGRESSIVE-REGULAR] Cache version mismatch ({cached_version} vs {current_version}) - will re-extract")
                raise Exception("Cache version outdated")
            
            # Check if cached data contains only errors or is invalid
            cached_fields = cached_data.get("data", {})
            if isinstance(cached_fields, dict):
                # Skip cache if it only contains error messages
                if len(cached_fields) == 1 and "error" in cached_fields:
                    print(f"[PROGRESSIVE-REGULAR] Cached data contains only error, re-extracting page {page_index}")
                    raise Exception("Cached data is invalid, re-extract")
                
                # Skip cache if error message mentions watermarks
                error_msg = cached_fields.get("error", "")
                if "watermark" in str(error_msg).lower() or "pdf-xchange" in str(error_msg).lower():
                    print(f"[PROGRESSIVE-REGULAR] Cached data contains watermark error, re-extracting page {page_index}")
                    raise Exception("Cached data contains watermark error, re-extract")
            
            # Always re-extract to get fresh data with enhanced VERIFIED detection
            print(f"[PROGRESSIVE-REGULAR] Forcing fresh extraction for page {page_index}")
            raise Exception("Force fresh extraction")
                
        except Exception as e:
            print(f"[PROGRESSIVE-REGULAR] Extracting fresh for page {page_index}: {str(e)}")
        
        # Extract text from the specific page
        import fitz
        pdf_doc = fitz.open(pdf_path)
        
        print(f"[PROGRESSIVE-REGULAR] PDF has {len(pdf_doc)} pages, extracting page {page_index} (0-based)")
        
        if page_index >= len(pdf_doc):
            pdf_doc.close()
            return jsonify({"success": False, "message": f"Page index {page_index} out of range (PDF has {len(pdf_doc)} pages)"}), 400
        
        page = pdf_doc[page_index]
        page_text = page.get_text()
        
        print(f"[PROGRESSIVE-REGULAR] Extracted {len(page_text)} characters from page {page_index}")
        print(f"[PROGRESSIVE-REGULAR] First 200 chars: {page_text[:200]}")
        
        # Check for watermark content and apply OCR if needed
        has_watermark = "PDF-XChange" in page_text or "Click to BUY NOW" in page_text
        has_little_text = len(page_text.strip()) < 100
        
        if has_watermark or has_little_text:
            print(f"[PROGRESSIVE-REGULAR] Page {page_index} needs OCR (watermark: {has_watermark}, little text: {has_little_text})")
            
            try:
                # Convert page to image with higher resolution for better OCR
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x resolution
                
                temp_image_path = os.path.join(OUTPUT_DIR, f"temp_progressive_regular_{doc_id}_{page_index}.png")
                pix.save(temp_image_path)
                
                with open(temp_image_path, 'rb') as image_file:
                    image_bytes = image_file.read()
                
                print(f"[PROGRESSIVE-REGULAR] Running Textract OCR on page {page_index}...")
                
                # Use Textract for OCR
                textract_response = textract.detect_document_text(Document={'Bytes': image_bytes})
                
                # Track Textract cost
                try:
                    cost_tracker = get_cost_tracker(doc_id)
                    cost_tracker.track_textract_sync(pages=1)
                    print(f"[COST] ‚úÖ Tracked Textract cost for page {page_index}")
                except Exception as e:
                    print(f"[COST] Failed to track Textract cost: {str(e)}")
                
                ocr_text = ""
                for block in textract_response.get('Blocks', []):
                    if block['BlockType'] == 'LINE':
                        ocr_text += block.get('Text', '') + "\n"
                
                # Clean up temp file
                if os.path.exists(temp_image_path):
                    os.remove(temp_image_path)
                
                if ocr_text.strip() and len(ocr_text.strip()) > len(page_text.strip()):
                    page_text = ocr_text
                    print(f"[PROGRESSIVE-REGULAR] ‚úì OCR extracted {len(page_text)} characters from page {page_index}")
                    print(f"[PROGRESSIVE-REGULAR] OCR first 200 chars: {page_text[:200]}")
                else:
                    print(f"[PROGRESSIVE-REGULAR] OCR didn't improve text for page {page_index}")
                    
            except Exception as ocr_error:
                print(f"[PROGRESSIVE-REGULAR] OCR error on page {page_index}: {str(ocr_error)}")
        
        # Close PDF after text extraction and OCR
        pdf_doc.close()
        
        if not page_text.strip():
            return jsonify({"success": False, "message": "No text found on page"}), 400
        
        # Use Claude AI to extract data from this page
        import boto3
        bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
        MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"
        
        # Get document type for appropriate prompt
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        
        # Use comprehensive extraction prompt with enhanced VERIFIED detection
        prompt = get_comprehensive_extraction_prompt()
        
        # Call Claude AI
        claude_request = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4000,
            "temperature": 0,
            "messages": [{
                "role": "user",
                "content": f"{prompt}\n\nPage text:\n{page_text}"
            }]
        }
        
        start_time = time.time()
        response = bedrock.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(claude_request)
        )
        
        response_body = json.loads(response['body'].read())
        claude_response = response_body['content'][0]['text']
        
        # Track Bedrock cost
        try:
            input_tokens = response_body.get('usage', {}).get('input_tokens', 0)
            output_tokens = response_body.get('usage', {}).get('output_tokens', 0)
            
            if input_tokens > 0 or output_tokens > 0:
                cost_tracker = get_cost_tracker(doc_id)
                cost_tracker.track_bedrock_call(input_tokens, output_tokens)
                print(f"[COST] ‚úÖ Tracked Bedrock cost for progressive extraction: {input_tokens} input + {output_tokens} output tokens")
        except Exception as e:
            print(f"[COST] Failed to track Bedrock cost: {str(e)}")
        
        # Parse extracted data with better error handling
        try:
            print(f"[PROGRESSIVE-REGULAR] Claude response length: {len(claude_response)}")
            print(f"[PROGRESSIVE-REGULAR] Claude response preview: {claude_response[:300]}...")
            
            if not claude_response.strip():
                print(f"[PROGRESSIVE-REGULAR] Empty response from Claude AI")
                return jsonify({"success": False, "message": "Empty response from Claude AI"}), 500
            
            # Clean up Claude response - sometimes it includes extra text
            claude_response_clean = claude_response.strip()
            
            # Try to extract JSON from the response
            json_start = claude_response_clean.find('{')
            json_end = claude_response_clean.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = claude_response_clean[json_start:json_end]
                extracted_fields = json.loads(json_text)
            else:
                # Fallback: try to parse the entire response
                extracted_fields = json.loads(claude_response_clean)
            
            # MANDATORY: Always check for VERIFIED text using regex fallback (same as account-based)
            import re
            
            # ENHANCED VERIFIED DETECTION - Check for VERIFIED text in the page content with comprehensive patterns
            verified_patterns = [
                r'\bVERIFIED\b',
                r'\bVERIFICATION\b', 
                r'\bVERIFY\b',
                r'‚úì\s*VERIFIED',
                r'VERIFIED\s*[-‚Äì]\s*([A-Z\s]+)',  # VERIFIED - NAME pattern
                r'VERIFIED\s*BY\s*:?\s*([A-Z\s]+)',  # VERIFIED BY: NAME pattern
                r'VERIFIED\s*:\s*([A-Z\s]+)',  # VERIFIED: NAME pattern
                r'‚òë\s*VERIFIED',  # Checkbox with VERIFIED
                r'‚úì.*VERIFIED',   # Checkmark with VERIFIED
                r'VERIFIED.*‚úì',   # VERIFIED with checkmark
            ]
            
            verification_found = False
            verified_by_name = None
            
            # Check each pattern and log what we find
            for pattern in verified_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    verification_found = True
                    print(f"[PROGRESSIVE-REGULAR] FALLBACK: Found VERIFIED text with pattern '{pattern}': {matches}")
                    
                    # Try to extract name from the match if it's a capturing group
                    if matches and isinstance(matches[0], str) and len(matches[0]) > 1:
                        potential_name = matches[0].strip()
                        if len(potential_name) > 2 and not potential_name.upper() == "VERIFIED":
                            verified_by_name = potential_name
                            print(f"[PROGRESSIVE-REGULAR] FALLBACK: Extracted name from pattern: '{verified_by_name}'")
                    break
            
            # Additional comprehensive search for VERIFIED text
            if not verification_found:
                # Case-insensitive search for any occurrence of "verified"
                if re.search(r'verified', page_text, re.IGNORECASE):
                    verification_found = True
                    print(f"[PROGRESSIVE-REGULAR] FALLBACK: Found 'verified' text (case-insensitive)")
                
                # Search for common verification phrases
                verification_phrases = [
                    r'verification\s+complete',
                    r'document\s+verified',
                    r'identity\s+verified',
                    r'signature\s+verified',
                    r'verified\s+copy',
                    r'verified\s+true',
                    r'verified\s+correct'
                ]
                
                for phrase in verification_phrases:
                    if re.search(phrase, page_text, re.IGNORECASE):
                        verification_found = True
                        print(f"[PROGRESSIVE-REGULAR] FALLBACK: Found verification phrase: '{phrase}'")
                        break
            
            # Enhanced name extraction - try multiple patterns to get complete names
            if verification_found and not verified_by_name:
                name_patterns = [
                    r'VERIFIED\s*[-‚Äì]\s*([A-Z][A-Z\s]{2,30})',  # VERIFIED - FULL NAME (extended length)
                    r'VERIFIED\s+([A-Z][A-Z\s]{2,30})',        # VERIFIED FULL NAME (no dash)
                    r'VERIFIED\s*:\s*([A-Z][A-Z\s]{2,30})',    # VERIFIED: FULL NAME
                    r'VERIFIED\s*BY\s*:?\s*([A-Z][A-Z\s]{2,30})', # VERIFIED BY: FULL NAME
                    r'VERIFIED\s*[-‚Äì]\s*([A-Z]+\s+[A-Z]+)',    # VERIFIED - FIRSTNAME LASTNAME
                    r'VERIFIED\s*BY\s*([A-Z]+)',               # VERIFIED BY NAME (single word)
                    r'VERIFIED\s*-\s*([A-Z]+)',                # VERIFIED - NAME (single word)
                ]
                
                for name_pattern in name_patterns:
                    name_match = re.search(name_pattern, page_text, re.IGNORECASE)
                    if name_match:
                        full_name = name_match.group(1).strip()
                        # Clean up the name - remove extra spaces and validate
                        full_name = re.sub(r'\s+', ' ', full_name)  # Replace multiple spaces with single space
                        
                        # Accept names that look valid (1+ words, reasonable length)
                        if len(full_name) >= 2 and len(full_name) <= 30:
                            verified_by_name = full_name
                            print(f"[PROGRESSIVE-REGULAR] FALLBACK: Found complete verified by name: '{verified_by_name}' using pattern '{name_pattern}'")
                            break
                
                # If no name found with patterns, try a broader search around VERIFIED
                if not verified_by_name:
                    # Look for names within 100 characters after VERIFIED (increased range)
                    verified_context = re.search(r'VERIFIED.{0,100}', page_text, re.IGNORECASE | re.DOTALL)
                    if verified_context:
                        context_text = verified_context.group(0)
                        print(f"[PROGRESSIVE-REGULAR] FALLBACK: Searching for names in context: '{context_text[:100]}...'")
                        
                        # Extract potential names (capitalized words)
                        name_candidates = re.findall(r'\b[A-Z][A-Z\s]{1,25}\b', context_text)
                        for candidate in name_candidates:
                            candidate = candidate.strip()
                            # Skip if it's just "VERIFIED" or common words
                            skip_words = ["VERIFIED", "BY", "DATE", "STAMP", "SEAL", "DOCUMENT", "COPY", "TRUE", "CORRECT"]
                            if candidate not in skip_words and len(candidate) >= 2:
                                verified_by_name = candidate
                                print(f"[PROGRESSIVE-REGULAR] FALLBACK: Found verified by name from context: '{verified_by_name}'")
                                break
                    
                    # Also try looking before VERIFIED
                    if not verified_by_name:
                        verified_context_before = re.search(r'.{0,50}VERIFIED', page_text, re.IGNORECASE | re.DOTALL)
                        if verified_context_before:
                            context_text = verified_context_before.group(0)
                            # Look for names right before VERIFIED
                            name_candidates = re.findall(r'\b[A-Z][A-Z\s]{1,25}\b', context_text)
                            if name_candidates:
                                # Take the last name candidate (closest to VERIFIED)
                                candidate = name_candidates[-1].strip()
                                skip_words = ["VERIFIED", "BY", "DATE", "STAMP", "SEAL", "DOCUMENT", "COPY", "TRUE", "CORRECT"]
                                if candidate not in skip_words and len(candidate) >= 2:
                                    verified_by_name = candidate
                                    print(f"[PROGRESSIVE-REGULAR] FALLBACK: Found verified by name before VERIFIED: '{verified_by_name}'")
            
            # CRITICAL: Always add VERIFIED fields if found, even if Claude extracted them
            # This ensures consistency and prevents missing VERIFIED detection
            if verification_found:
                # Always add or update the Verified field
                verified_field_exists = any(key.lower().startswith('verified') and not 'by' in key.lower() for key in extracted_fields.keys())
                
                if not verified_field_exists:
                    print(f"[PROGRESSIVE-REGULAR] FALLBACK: Adding missing VERIFIED field")
                    extracted_fields["Verified"] = {
                        "value": "VERIFIED",
                        "confidence": 95
                    }
                else:
                    print(f"[PROGRESSIVE-REGULAR] FALLBACK: VERIFIED field already exists, ensuring it's marked as found")
                    # Update confidence if Claude found it with lower confidence
                    for key in extracted_fields.keys():
                        if key.lower().startswith('verified') and not 'by' in key.lower():
                            if extracted_fields[key].get("confidence", 0) < 95:
                                extracted_fields[key]["confidence"] = 95
                                print(f"[PROGRESSIVE-REGULAR] FALLBACK: Updated {key} confidence to 95")
                
                # Add verified by name if found
                if verified_by_name:
                    verified_by_exists = any('verified_by' in key.lower() or 'verifiedby' in key.lower() for key in extracted_fields.keys())
                    
                    if not verified_by_exists:
                        print(f"[PROGRESSIVE-REGULAR] FALLBACK: Adding missing Verified_By field: {verified_by_name}")
                        extracted_fields["Verified_By"] = {
                            "value": verified_by_name,
                            "confidence": 85
                        }
                    else:
                        print(f"[PROGRESSIVE-REGULAR] FALLBACK: Verified_By field already exists")
                
                # Log final verification status
                print(f"[PROGRESSIVE-REGULAR] VERIFICATION SUMMARY:")
                print(f"  - Verification found: {verification_found}")
                print(f"  - Verified by name: {verified_by_name}")
                print(f"  - Total VERIFIED fields in result: {len([k for k in extracted_fields.keys() if 'verified' in k.lower()])}")
                
                # Show all verification-related fields
                for key, value in extracted_fields.items():
                    if 'verified' in key.lower():
                        print(f"  - {key}: {value}")
            else:
                print(f"[PROGRESSIVE-REGULAR] VERIFICATION SUMMARY: No VERIFIED text found on this page")
                print(f"[PROGRESSIVE-REGULAR] Page text sample for verification check: {page_text[:300]}...")
            
            print(f"[PROGRESSIVE-REGULAR] Successfully parsed {len(extracted_fields)} fields")
            
        except json.JSONDecodeError as e:
            print(f"[PROGRESSIVE-REGULAR] JSON parse error: {e}")
            print(f"[PROGRESSIVE-REGULAR] Raw Claude response: '{claude_response}'")
            return jsonify({"success": False, "message": f"Failed to parse extraction result: {str(e)}"}), 500
        
        extraction_time = time.time() - start_time
        
        # Cache the result in the same format as regular /data endpoint
        cache_data = {
            "success": True,
            "page_number": page_index + 1,
            "data": extracted_fields,
            "cached": False,
            "prompt_version": "v5_enhanced_verified",  # Updated version to invalidate old cache
            "extraction_method": "progressive_regular",
            "extracted_at": datetime.now().isoformat(),
            "extraction_time_seconds": round(extraction_time, 2),
            "priority": priority,
            "doc_type": doc_type
        }
        
        try:
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
            print(f"[PROGRESSIVE-REGULAR] ‚úì Cached extraction result: {cache_key}")
        except Exception as s3_error:
            print(f"[PROGRESSIVE-REGULAR] Warning: Failed to cache result: {s3_error}")
        
        fields_count = len(extracted_fields)
        print(f"[PROGRESSIVE-REGULAR] ‚úÖ Extracted {fields_count} fields from page {page_index} in {extraction_time:.2f}s")
        
        return jsonify({
            "success": True,
            "cached": False,
            "fieldsExtracted": fields_count,
            "extractedAt": datetime.now().isoformat(),
            "extractionTime": round(extraction_time, 2),
            "extractedFields": extracted_fields
        })
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"[PROGRESSIVE-REGULAR] Error extracting page: {str(e)}")
        print(f"[PROGRESSIVE-REGULAR] Traceback: {error_trace}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/page/<int:page_num>/update", methods=["POST"])
def update_page_data(doc_id, page_num):
    """Update page data and save to S3 cache"""
    import json
    
    try:
        data = request.get_json()
        page_data = data.get("page_data")
        account_index = data.get("account_index")  # Get account index if provided
        
        if not page_data:
            return jsonify({"success": False, "message": "No page data provided"}), 400
        
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Determine cache key based on whether this is an account-based document
        if account_index is not None:
            # Account-based document (loan documents)
            cache_key = f"page_data/{doc_id}/account_{account_index}/page_{page_num}.json"
            print(f"[INFO] Updating account-based cache: {cache_key}")
        else:
            # Regular document (convert 1-based to 0-based)
            cache_key = f"page_data/{doc_id}/page_{page_num - 1}.json"
            print(f"[INFO] Updating regular cache: {cache_key}")
        
        # Get existing cache to preserve metadata
        try:
            cache_response = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
            existing_cache = json.loads(cache_response['Body'].read().decode('utf-8'))
            account_number = existing_cache.get("account_number")
        except:
            account_number = None
        
        cache_data = {
            "data": page_data,
            "extracted_at": datetime.now().isoformat(),
            "edited": True,
            "edited_at": datetime.now().isoformat()
        }
        
        if account_number:
            cache_data["account_number"] = account_number
        
        try:
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=cache_key,
                Body=json.dumps(cache_data),
                ContentType='application/json'
            )
            print(f"[INFO] Updated cache: {cache_key}")
            return jsonify({"success": True, "message": "Page data updated successfully"})
        except Exception as s3_error:
            print(f"[ERROR] Failed to update cache: {str(s3_error)}")
            return jsonify({"success": False, "message": f"Failed to update cache: {str(s3_error)}"}), 500
    
    except Exception as e:
        print(f"[ERROR] Failed to update page data: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/delete", methods=["DELETE", "POST"])
def delete_document(doc_id):
    """Delete a processed document"""
    global processed_documents
    
    # Find document
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"success": False, "message": "Document not found"}), 404
    
    try:
        # Delete OCR file if exists
        if "ocr_file" in doc and doc["ocr_file"] and os.path.exists(doc["ocr_file"]):
            os.remove(doc["ocr_file"])
            print(f"[INFO] Deleted OCR file: {doc['ocr_file']}")
        
        # Delete PDF file if exists
        if "pdf_path" in doc and doc["pdf_path"] and os.path.exists(doc["pdf_path"]):
            os.remove(doc["pdf_path"])
            print(f"[INFO] Deleted PDF file: {doc['pdf_path']}")
        
        # Remove from processed documents
        processed_documents = [d for d in processed_documents if d["id"] != doc_id]
        save_documents_db(processed_documents)
        
        print(f"[INFO] Deleted document: {doc_id}")
        return jsonify({"success": True, "message": "Document deleted successfully"})
    
    except Exception as e:
        print(f"[ERROR] Failed to delete document {doc_id}: {str(e)}")
        return jsonify({"success": False, "message": f"Failed to delete: {str(e)}"}), 500


@app.route("/api/document/<doc_id>/pdf")
def serve_pdf(doc_id):
    """Serve the PDF file for viewing"""
    doc = next((d for d in processed_documents if d["id"] == doc_id), None)
    if not doc:
        return jsonify({"error": "Document not found"}), 404
    
    pdf_path = doc.get("pdf_path")
    if not pdf_path or not os.path.exists(pdf_path):
        return jsonify({"error": "PDF file not found"}), 404
    
    return send_file(pdf_path, as_attachment=False, mimetype='application/pdf')


@app.route("/api/documents/cleanup", methods=["POST"])
def cleanup_old_documents():
    """Delete all old uploaded documents and OCR results"""
    global processed_documents
    
    try:
        deleted_count = 0
        
        # Delete all OCR result files
        if os.path.exists(OUTPUT_DIR):
            for filename in os.listdir(OUTPUT_DIR):
                file_path = os.path.join(OUTPUT_DIR, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        deleted_count += 1
                except Exception as e:
                    print(f"[WARNING] Failed to delete {file_path}: {str(e)}")
        
        # Clear processed documents database
        doc_count = len(processed_documents)
        processed_documents = []
        save_documents_db(processed_documents)
        
        # Clear job status map
        job_status_map.clear()
        
        print(f"[INFO] Cleanup completed: {deleted_count} files deleted, {doc_count} documents cleared")
        
        return jsonify({
            "success": True,
            "message": f"Cleanup completed successfully",
            "files_deleted": deleted_count,
            "documents_cleared": doc_count
        })
    
    except Exception as e:
        print(f"[ERROR] Cleanup failed: {str(e)}")
        return jsonify({"success": False, "message": f"Cleanup failed: {str(e)}"}), 500


@app.route("/upload", methods=["POST"])
@app.route("/process", methods=["POST"])
def upload_file():
    """Handle file upload and start processing"""
    if "file" not in request.files:
        return jsonify({"success": False, "message": "No file provided"}), 400
    
    file = request.files["file"]
    if file.filename == "":
        return jsonify({"success": False, "message": "No file selected"}), 400
    
    # Get optional document name from form
    document_name = request.form.get("document_name", "").strip()
    
    # Generate unique job ID
    job_id = hashlib.md5(f"{file.filename}{time.time()}".encode()).hexdigest()[:12]
    
    # Read file content
    file_bytes = file.read()
    
    # Start background processing
    thread = threading.Thread(
        target=process_job,
        args=(job_id, file_bytes, file.filename, True, document_name),
        daemon=True
    )
    thread.start()
    
    return jsonify({
        "success": True,
        "job_id": job_id,
        "message": "File uploaded successfully. Processing started."
    })


@app.route("/status/<job_id>")
def get_status(job_id):
    """Get processing status for a job with detailed progress messages"""
    
    # Check background processor status FIRST (prioritize live processing status)
    bg_status = background_processor.get_document_status(job_id)
    
    if bg_status:
        stage = bg_status.get("stage", "unknown")
        stage_progress = bg_status.get("progress", 0)
        
        # If background processing hasn't actually started yet (stage is initial but progress is 0),
        # fall back to job_status_map to show upload progress
        if stage == DocumentProcessingStage.OCR_EXTRACTION and stage_progress == 0 and job_id in job_status_map:
            job_status = job_status_map[job_id]
            return jsonify(job_status)
        
        # Calculate continuous overall progress across all stages
        # Upload detection: 0-15% (already done before background processing starts)
        # Progress ranges (continuous from upload phase):
        # Upload phase: 0-40% (document type detection)
        # OCR extraction: 40-55% (15% range)
        # Account splitting: 55-75% (20% range)
        # LLM extraction: 75-95% (20% range)
        # Completion: 95-100% (5% range)
        
        if stage == "ocr_extraction":
            # OCR continues from 40% to 55%
            overall_progress = 40 + int((stage_progress / 100) * 15)
            message = f"üîç OCR Extraction in progress... ({overall_progress}%)"
            detailed_message = "Extracting text from PDF pages using OCR"
        elif stage == "account_splitting":
            # Account splitting continues from 55% to 75%
            overall_progress = 55 + int((stage_progress / 100) * 20)
            message = f"üîÄ Splitting accounts & caching OCR results... ({overall_progress}%)"
            detailed_message = "Detecting account boundaries, splitting pages, and caching OCR results"
        elif stage == "page_analysis":
            # Page analysis is part of account splitting (55-75%)
            overall_progress = 55 + int((stage_progress / 100) * 20)
            message = f"üìä Analyzing pages... ({overall_progress}%)"
            detailed_message = "Mapping pages to accounts"
        elif stage == "llm_extraction":
            # LLM extraction continues from 75% to 95%
            pages_processed = bg_status.get("pages_processed", 0)
            total_pages = bg_status.get("total_pages", 0)
            overall_progress = 75 + int((stage_progress / 100) * 20)
            if pages_processed > 0 and total_pages > 0:
                message = f"ü§ñ LLM Processing: Page {pages_processed}/{total_pages} sent to LLM, done"
                detailed_message = f"Extracting data from page {pages_processed} of {total_pages}"
            else:
                message = f"ü§ñ LLM Processing in progress... ({overall_progress}%)"
                detailed_message = "Extracting data from pages using LLM"
        elif stage == "completed":
            message = "‚úÖ Processing Complete!"
            detailed_message = "All pages have been processed and cached"
            overall_progress = 100
        else:
            overall_progress = stage_progress
            message = f"‚è≥ Processing... ({overall_progress}%)"
            detailed_message = f"Stage: {stage}"
        
        # When background processing is complete, get the document data and return it as result
        response_data = {
            "status": message,
            "detailed_message": detailed_message,
            "progress": overall_progress,
            "stage": stage,
            "pages_processed": bg_status.get("pages_processed", 0),
            "total_pages": bg_status.get("total_pages", 0),
            "accounts": bg_status.get("accounts", []),
            "is_complete": stage == "completed"
        }
        
        # If completed, include the document data as result
        if stage == "completed":
            doc = next((d for d in processed_documents if d["id"] == job_id), None)
            if doc:
                response_data["result"] = {"documents": doc.get("documents", [])}
        
        return jsonify(response_data)
    
    # Fall back to job status map if no background processing status
    if job_id in job_status_map:
        job_status = job_status_map[job_id]
        return jsonify(job_status)
    
    # Default: unknown job
    return jsonify({
        "status": "Unknown job ID",
        "detailed_message": "Job not found in processing queue",
        "progress": 0,
        "is_complete": False
    })


# Background Processing API Endpoints
@app.route("/api/document/<doc_id>/background-status")
def get_background_processing_status(doc_id):
    """Get background processing status for a document"""
    try:
        status = background_processor.get_document_status(doc_id)
        if status:
            return jsonify({"success": True, "status": status})
        else:
            return jsonify({"success": False, "message": "No background processing found for this document"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

@app.route("/api/document/<doc_id>/page/<int:page_num>/cached-data")
def get_cached_page_data_endpoint(doc_id, page_num):
    """Get cached page data if available, otherwise return processing status"""
    try:
        # Determine document type to use correct cache key
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        doc_type = "unknown"
        if doc:
            doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        
        # Check if page is cached (convert 1-based to 0-based)
        page_index = page_num - 1
        
        # Try death certificate cache first if it's a death certificate
        if doc_type == "death_certificate":
            try:
                cache_key = f"death_cert_page_data/{doc_id}/page_{page_index}.json"
                cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                cached_data = json.loads(cached_result['Body'].read())
                
                return jsonify({
                    "success": True,
                    "cached": True,
                    "data": cached_data.get("extracted_data", {}),
                    "page_number": cached_data.get("page_number", page_index),
                    "document_type": cached_data.get("document_type", "death_certificate"),
                    "extraction_time": cached_data.get("extraction_time")
                })
            except:
                pass  # Fall through to regular cache check
        
        # Check regular page cache
        if background_processor.is_page_cached(doc_id, page_index):
            cached_data = background_processor.get_cached_page_data(doc_id, page_index)
            if cached_data:
                return jsonify({
                    "success": True,
                    "cached": True,
                    "data": cached_data.get("extracted_data", {}),
                    "account_number": cached_data.get("account_number"),
                    "extraction_time": cached_data.get("extraction_time")
                })
        
        # Check background processing status
        bg_status = background_processor.get_document_status(doc_id)
        if bg_status:
            return jsonify({
                "success": True,
                "cached": False,
                "processing": True,
                "stage": bg_status.get("stage"),
                "progress": bg_status.get("progress", 0),
                "pages_processed": bg_status.get("pages_processed", 0),
                "total_pages": bg_status.get("total_pages", 0)
            })
        
        return jsonify({
            "success": True,
            "cached": False,
            "processing": False,
            "message": "Page not processed yet"
        })
        
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

@app.route("/api/document/<doc_id>/force-background-processing", methods=["POST"])
def force_background_processing(doc_id):
    """Force start background processing for a document"""
    try:
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        pdf_path = doc.get("pdf_path")
        if not pdf_path or not os.path.exists(pdf_path):
            return jsonify({"success": False, "message": "PDF file not found"}), 404
        
        # Queue for background processing with high priority
        background_processor.queue_document_for_processing(doc_id, pdf_path, priority=0)
        
        return jsonify({
            "success": True,
            "message": "Background processing started",
            "doc_id": doc_id
        })
        
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

@app.route("/api/background-processor/status")
def get_background_processor_status():
    """Get overall background processor status"""
    try:
        # Get status for all documents being processed
        all_statuses = {}
        for doc_id in background_processor.document_status:
            all_statuses[doc_id] = background_processor.get_document_status(doc_id)
        
        return jsonify({
            "success": True,
            "processor_running": background_processor.is_running,
            "active_documents": len(background_processor.document_threads),
            "queued_documents": background_processor.processing_queue.qsize(),
            "document_statuses": all_statuses
        })
        
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

@app.route("/api/background-processor/restart", methods=["POST"])
def restart_background_processor():
    """Restart the background processor"""
    try:
        background_processor.stop()
        time.sleep(1)
        background_processor.start()
        
        return jsonify({
            "success": True,
            "message": "Background processor restarted"
        })
        
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

@app.route("/api/document/<doc_id>/refresh-from-background", methods=["POST"])
def refresh_document_from_background(doc_id):
    """Refresh document data with background processing results"""
    try:
        # Check background processing status
        bg_status = background_processor.get_document_status(doc_id)
        if not bg_status:
            return jsonify({"success": False, "message": "No background processing found for this document"})
        
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Get document type
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        
        # Update document with background results based on document type
        accounts = bg_status.get("accounts", [])
        total_pages = bg_status.get("total_pages", 0)
        
        # Check if processing is complete
        if bg_status.get("stage") == DocumentProcessingStage.COMPLETED:
            
            if doc_type == "loan_document" and len(accounts) > 0:
                # Update loan document with accounts
                if doc["documents"] and len(doc["documents"]) > 0:
                    doc["documents"][0].update({
                        "accounts": accounts,
                        "extracted_fields": {
                            "total_accounts": len(accounts),
                            "accounts_processed": len(accounts),
                            "processing_method": "Background processing"
                        },
                        "accuracy_score": 95,
                        "filled_fields": len(accounts) * 5,
                        "total_fields": len(accounts) * 10,
                        "needs_human_review": False,
                        "optimized": True,
                        "background_processed": True
                    })
                
                # Update document status
                doc.update({
                    "status": "completed",
                    "background_processing_completed": True,
                    "total_pages": total_pages
                })
                
                # Save to database
                save_documents_db(processed_documents)
                
                return jsonify({
                    "success": True,
                    "message": "Loan document refreshed with background processing results",
                    "accounts": accounts,
                    "total_accounts": len(accounts),
                    "total_pages": total_pages,
                    "document_type": doc_type,
                    "stage": bg_status.get("stage"),
                    "progress": bg_status.get("progress", 0)
                })
                
            else:
                # For non-loan documents, check for extracted fields in cache
                try:
                    cache_key = f"document_extraction_cache/{doc_id}/full_extraction.json"
                    cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                    cached_data = json.loads(cached_result['Body'].read())
                    extracted_fields = cached_data.get("extracted_fields", {})
                    
                    if extracted_fields and not extracted_fields.get("error"):
                        # Update document with extracted fields
                        if doc["documents"] and len(doc["documents"]) > 0:
                            filled_fields = sum(1 for v in extracted_fields.values() 
                                              if v and str(v).strip() and str(v) != "N/A")
                            total_fields = len(extracted_fields)
                            accuracy_score = int((filled_fields / total_fields) * 100) if total_fields > 0 else 0
                            
                            doc["documents"][0].update({
                                "extracted_fields": extracted_fields,
                                "accuracy_score": accuracy_score,
                                "filled_fields": filled_fields,
                                "total_fields": total_fields,
                                "needs_human_review": accuracy_score < 90,
                                "optimized": True,
                                "background_processed": True
                            })
                        
                        # Update document status
                        doc.update({
                            "status": "completed",
                            "background_processing_completed": True,
                            "total_pages": total_pages
                        })
                        
                        # Save to database
                        save_documents_db(processed_documents)
                        
                        return jsonify({
                            "success": True,
                            "message": f"{doc_type} document refreshed with background processing results",
                            "extracted_fields": extracted_fields,
                            "field_count": len(extracted_fields),
                            "accuracy_score": accuracy_score,
                            "total_pages": total_pages,
                            "document_type": doc_type,
                            "stage": bg_status.get("stage"),
                            "progress": bg_status.get("progress", 0)
                        })
                        
                except Exception as cache_error:
                    print(f"[REFRESH] Failed to get cached extraction for {doc_id}: {str(cache_error)}")
                
                # If no cached data found, return processing status
                return jsonify({
                    "success": True,
                    "message": f"Background processing completed but no extracted data found for {doc_type}",
                    "document_type": doc_type,
                    "stage": bg_status.get("stage"),
                    "progress": bg_status.get("progress", 0),
                    "total_pages": total_pages
                })
        else:
            # Processing still in progress
            return jsonify({
                "success": True,
                "message": "Background processing in progress",
                "document_type": doc_type,
                "stage": bg_status.get("stage"),
                "progress": bg_status.get("progress", 0),
                "pages_processed": bg_status.get("pages_processed", 0),
                "total_pages": bg_status.get("total_pages", 0)
            })
        
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/complete-json")
def get_complete_json(doc_id):
    """Get complete merged JSON for entire document
    
    For loan documents: Merges all account JSONs without duplication
    For death certificates: Uses LLM to intelligently merge all page JSONs
    """
    try:
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        doc_data = doc.get("documents", [{}])[0]
        
        print(f"[COMPLETE_JSON] Building complete JSON for {doc_id} (type: {doc_type})")
        
        if doc_type == "loan_document":
            # LOAN DOCUMENT: Merge all account JSONs
            return _merge_loan_accounts_json(doc_id, doc, doc_data)
        else:
            # DEATH CERTIFICATE / OTHER: Merge all page JSONs using LLM
            return _merge_death_certificate_pages_json(doc_id, doc, doc_data)
            
    except Exception as e:
        print(f"[ERROR] Failed to get complete JSON: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


def _merge_loan_accounts_json(doc_id, doc, doc_data):
    """Merge all account JSONs for loan documents without duplication"""
    try:
        accounts = doc_data.get("accounts", [])
        
        if not accounts:
            return jsonify({
                "success": True,
                "document_id": doc_id,
                "document_type": "loan_document",
                "document_name": doc.get("document_name", "Unknown"),
                "total_accounts": 0,
                "accounts": [],
                "merge_method": "manual_deduplication"
            })
        
        print(f"[COMPLETE_JSON] Merging {len(accounts)} loan accounts...")
        
        merged_accounts = []
        seen_keys = set()  # Track all keys we've seen to avoid duplication
        
        for account in accounts:
            acc_num = account.get("accountNumber", "Unknown")
            result = account.get("result", {})
            
            print(f"[COMPLETE_JSON] Processing account: {acc_num}")
            
            # Create merged account object
            merged_account = {
                "account_number": acc_num,
                "pages": account.get("pages", []),
                "accuracy_score": account.get("accuracy_score", 100),
                "fields": {}
            }
            
            # Merge fields without duplication
            for key, value in result.items():
                # Skip empty/N/A values
                if not value or value == "N/A" or value == "":
                    continue
                
                # Skip if we've already seen this key-value pair
                pair_key = f"{key}:{value}"
                if pair_key in seen_keys:
                    print(f"[COMPLETE_JSON]   Skipping duplicate: {key} = {value}")
                    continue
                
                seen_keys.add(pair_key)
                merged_account["fields"][key] = value
            
            merged_accounts.append(merged_account)
            print(f"[COMPLETE_JSON] ‚úÖ Merged account {acc_num} with {len(merged_account['fields'])} unique fields")
        
        response_data = {
            "success": True,
            "document_id": doc_id,
            "document_type": "loan_document",
            "document_name": doc.get("document_name", "Unknown"),
            "total_accounts": len(merged_accounts),
            "accounts": merged_accounts,
            "merge_method": "manual_deduplication",
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"[COMPLETE_JSON] ‚úÖ Complete JSON ready: {len(merged_accounts)} accounts with deduplicated fields")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"[ERROR] Failed to merge loan accounts: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


def _merge_death_certificate_pages_json(doc_id, doc, doc_data):
    """Merge all page JSONs for death certificates using LLM
    
    Smart logic:
    1. Collect all cached pages
    2. Get total page count from document
    3. For any missing pages, extract them NOW (don't wait for background)
    4. Merge all pages with LLM
    """
    try:
        # Get total pages from document
        total_pages = doc.get("total_pages", 0)
        print(f"[COMPLETE_JSON] Death certificate has {total_pages} total pages")
        
        # Collect all page data - both cached and missing
        all_pages_data = []
        pages_to_extract = []
        
        # Step 1: Collect all CACHED pages
        print(f"[COMPLETE_JSON] Step 1: Scanning for cached pages...")
        for page_num in range(total_pages):
            cache_keys = [
                f"death_cert_page_data/{doc_id}/page_{page_num}.json",  # Primary pattern
                f"page_data/{doc_id}/page_{page_num}.json",  # Fallback pattern
                f"death_certificate_cache/{doc_id}/page_{page_num}.json"  # Legacy pattern
            ]
            
            found = False
            for cache_key in cache_keys:
                try:
                    cached_result = s3_client.get_object(Bucket=S3_BUCKET, Key=cache_key)
                    cached_data = json.loads(cached_result['Body'].read())
                    extracted_data = cached_data.get("extracted_data", {}) or cached_data.get("data", {})
                    
                    if extracted_data:
                        all_pages_data.append({
                            "page_number": page_num,
                            "data": extracted_data,
                            "source": "cache"
                        })
                        print(f"[COMPLETE_JSON]   ‚úì Page {page_num} found in cache ({len(extracted_data)} fields)")
                        found = True
                        break
                except:
                    continue
            
            if not found:
                pages_to_extract.append(page_num)
                print(f"[COMPLETE_JSON]   ‚úó Page {page_num} NOT cached - will extract now")
        
        # Step 2: Extract any MISSING pages NOW (don't wait for background)
        if pages_to_extract:
            print(f"[COMPLETE_JSON] Step 2: Extracting {len(pages_to_extract)} missing pages...")
            
            # Get OCR results for missing pages
            page_ocr_results = {}
            for page_num in pages_to_extract:
                # Try to get OCR from cache
                ocr_cache_key = f"page_ocr/{doc_id}/page_{page_num}.json"
                try:
                    ocr_result = s3_client.get_object(Bucket=S3_BUCKET, Key=ocr_cache_key)
                    ocr_data = json.loads(ocr_result['Body'].read())
                    page_ocr_results[page_num] = ocr_data.get("page_text", "")
                    print(f"[COMPLETE_JSON]   ‚úì OCR for page {page_num} found in cache")
                except:
                    print(f"[COMPLETE_JSON]   ‚úó OCR for page {page_num} not found")
            
            # Extract missing pages with LLM
            for page_num in pages_to_extract:
                try:
                    page_text = page_ocr_results.get(page_num, "")
                    
                    if not page_text or len(page_text.strip()) < 10:
                        print(f"[COMPLETE_JSON]   ‚ö†Ô∏è Page {page_num} has no OCR text, skipping")
                        continue
                    
                    # Extract with LLM (this ensures it's called only ONCE per page)
                    print(f"[COMPLETE_JSON]   ü§ñ Extracting page {page_num} with LLM...")
                    extracted_data = _extract_death_cert_page_with_llm(page_text)
                    
                    # Cache the result immediately
                    cache_key = f"death_cert_page_data/{doc_id}/page_{page_num}.json"
                    cache_data = {
                        "extracted_data": extracted_data,
                        "page_text": page_text[:500],
                        "page_number": page_num,
                        "document_type": "death_certificate",
                        "extraction_time": time.time(),
                        "cache_version": "death_cert_v1"
                    }
                    
                    s3_client.put_object(
                        Bucket=S3_BUCKET,
                        Key=cache_key,
                        Body=json.dumps(cache_data),
                        ContentType='application/json'
                    )
                    
                    all_pages_data.append({
                        "page_number": page_num,
                        "data": extracted_data,
                        "source": "extracted_now"
                    })
                    print(f"[COMPLETE_JSON]   ‚úÖ Page {page_num} extracted and cached ({len(extracted_data)} fields)")
                    
                except Exception as e:
                    print(f"[COMPLETE_JSON]   ‚ùå Failed to extract page {page_num}: {str(e)}")
        
        # Step 3: Check if we have any data at all
        if not all_pages_data:
            # Fallback: use extracted_fields from document
            extracted_fields = doc_data.get("extracted_fields", {})
            if extracted_fields and isinstance(extracted_fields, dict):
                filtered_fields = {k: v for k, v in extracted_fields.items() 
                                 if v and v != "N/A" and v != "" and not (isinstance(v, dict) and not v)}
                if filtered_fields:
                    all_pages_data = [{
                        "page_number": 0,
                        "data": filtered_fields,
                        "source": "document_extracted_fields"
                    }]
                    print(f"[COMPLETE_JSON] Using extracted_fields from document ({len(filtered_fields)} fields)")
        
        if not all_pages_data:
            print(f"[COMPLETE_JSON] ‚ö†Ô∏è No data found for document {doc_id}")
            return jsonify({
                "success": True,
                "document_id": doc_id,
                "document_type": doc.get("document_type_info", {}).get("type", "unknown"),
                "document_name": doc.get("document_name", "Unknown"),
                "merged_data": {},
                "merge_method": "no_data_available",
                "note": "Document has been uploaded but data extraction is still in progress. Please try again in a moment."
            })
        
        print(f"[COMPLETE_JSON] Merging {len(all_pages_data)} pages for {doc.get('document_type_info', {}).get('type', 'unknown')}...")
        
        # Use LLM to intelligently merge all pages
        if len(all_pages_data) == 1:
            # Single page - no merging needed
            merged_data = all_pages_data[0]["data"]
            merge_method = "single_page"
            print(f"[COMPLETE_JSON] Single page document, using data as-is ({len(merged_data)} fields)")
        else:
            # Multiple pages - use LLM to merge
            print(f"[COMPLETE_JSON] Using LLM to merge {len(all_pages_data)} pages...")
            
            # Build merge prompt
            pages_text = ""
            for page_info in all_pages_data:
                pages_text += f"\n\nPage {page_info['page_number']}:\n"
                for key, value in page_info["data"].items():
                    if value and value != "N/A":
                        pages_text += f"  {key}: {value}\n"
            
            merge_prompt = f"""You are merging extracted data from multiple pages of a document.

Pages data:
{pages_text}

Please merge this data intelligently:
1. Keep all unique information
2. If the same field appears on multiple pages with different values, keep the most complete/accurate one
3. Remove duplicates and conflicting information
4. Return ONLY valid JSON with merged fields, no explanations

Return as JSON object with field names as keys and values."""
            
            try:
                # Call Bedrock to merge
                response = bedrock_runtime.invoke_model(
                    modelId=MODEL_ID,
                    body=json.dumps({
                        "anthropic_version": "bedrock-2023-06-01",
                        "max_tokens": 2000,
                        "messages": [{
                            "role": "user",
                            "content": merge_prompt
                        }]
                    })
                )
                
                result_text = json.loads(response['body'].read())['content'][0]['text']
                
                # Extract JSON from response
                import re
                json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                if json_match:
                    merged_data = json.loads(json_match.group())
                    merge_method = "llm_merge"
                    print(f"[COMPLETE_JSON] ‚úÖ LLM merged {len(all_pages_data)} pages into {len(merged_data)} fields")
                else:
                    # Fallback: manual merge
                    merged_data = _manual_merge_pages(all_pages_data)
                    merge_method = "manual_merge_fallback"
                    print(f"[COMPLETE_JSON] ‚ö†Ô∏è LLM merge failed, using manual merge ({len(merged_data)} fields)")
                    
            except Exception as e:
                print(f"[COMPLETE_JSON] LLM merge failed: {str(e)}, using manual merge")
                merged_data = _manual_merge_pages(all_pages_data)
                merge_method = "manual_merge_fallback"
                print(f"[COMPLETE_JSON] Manual merge result: {len(merged_data)} fields")
        
        doc_type = doc.get("document_type_info", {}).get("type", "unknown")
        response_data = {
            "success": True,
            "document_id": doc_id,
            "document_type": doc_type,
            "document_name": doc.get("document_name", "Unknown"),
            "total_pages": len(all_pages_data),
            "merged_data": merged_data,
            "merge_method": merge_method,
            "pages_merged": [p["page_number"] for p in all_pages_data],
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"[COMPLETE_JSON] ‚úÖ Complete JSON ready: {len(merged_data)} fields merged from {len(all_pages_data)} pages")
        return jsonify(response_data)
        
    except Exception as e:
        print(f"[ERROR] Failed to merge death certificate pages: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


def _extract_death_cert_page_with_llm(page_text, doc_id=None):
    """Extract data from a single death certificate page using LLM
    
    This function is called ONLY ONCE per page to ensure no duplicate LLM calls
    """
    try:
        prompt = get_comprehensive_extraction_prompt()
        
        # Call Bedrock
        response = bedrock_runtime.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps({
                "anthropic_version": "bedrock-2023-06-01",
                "max_tokens": 2000,
                "messages": [{
                    "role": "user",
                    "content": f"{prompt}\n\nPage text:\n{page_text}"
                }]
            })
        )
        
        # Read response body once and reuse it
        response_body = json.loads(response['body'].read())
        result_text = response_body['content'][0]['text']
        
        # Track Bedrock cost if doc_id is provided
        if doc_id:
            try:
                input_tokens = response_body.get('usage', {}).get('input_tokens', 0)
                output_tokens = response_body.get('usage', {}).get('output_tokens', 0)
                
                if input_tokens > 0 or output_tokens > 0:
                    cost_tracker = get_cost_tracker(doc_id)
                    cost_tracker.track_bedrock_call(input_tokens, output_tokens)
                    print(f"[COST] ‚úÖ Tracked Bedrock cost: {input_tokens} input + {output_tokens} output tokens")
            except Exception as e:
                print(f"[COST] Failed to track Bedrock cost: {str(e)}")
        
        # Parse JSON response
        import re
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            extracted_data = json.loads(json_match.group())
            return extracted_data
        else:
            print(f"[EXTRACT_PAGE] Could not parse JSON from LLM response")
            return {}
            
    except Exception as e:
        print(f"[EXTRACT_PAGE] LLM extraction failed: {str(e)}")
        return {}


def _manual_merge_pages(all_pages_data):
    """Manually merge page data without duplication"""
    merged = {}
    seen_pairs = set()
    
    for page_info in all_pages_data:
        for key, value in page_info["data"].items():
            # Skip empty values
            if not value or value == "N/A" or value == "":
                continue
            
            pair_key = f"{key}:{value}"
            
            # If we haven't seen this key-value pair, add it
            if pair_key not in seen_pairs:
                if key not in merged:
                    merged[key] = value
                    seen_pairs.add(pair_key)
                elif merged[key] != value:
                    # Different value for same key - keep the longer/more complete one
                    if len(str(value)) > len(str(merged[key])):
                        merged[key] = value
                        seen_pairs.add(pair_key)
    
    return merged


@app.route("/api/document/<doc_id>/account/<int:account_index>/complete-data")
def get_complete_account_data(doc_id, account_index):
    """Get complete account data from processed_documents.json (not page-by-page LLM extraction)"""
    try:
        # Find the document
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Get accounts from the document
        doc_data = doc.get("documents", [{}])[0]
        accounts = doc_data.get("accounts", [])
        
        if account_index >= len(accounts):
            return jsonify({"success": False, "message": "Account index out of range"}), 400
        
        account = accounts[account_index]
        
        # Return the complete account data from processed_documents.json
        response_data = {
            "success": True,
            "account_number": account.get("accountNumber", "Unknown"),
            "account_index": account_index,
            "fields": account.get("result", {}),
            "accuracy_score": account.get("accuracy_score", 100),
            "filled_fields": account.get("filled_fields", 0),
            "total_fields": account.get("total_fields", 0),
            "pages": account.get("pages", []),
            "processing_method": account.get("processing_method", "unknown"),
            "needs_human_review": account.get("needs_human_review", False),
            "data_source": "processed_documents.json"
        }
        
        # Add cache headers to prevent caching
        response = jsonify(response_data)
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        
        return response
        
    except Exception as e:
        print(f"[ERROR] Failed to get complete account data: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/document/<doc_id>/cost")
def get_document_cost(doc_id):
    """Get processing cost for a specific document"""
    try:
        # Get document from processed_documents
        doc = next((d for d in processed_documents if d["id"] == doc_id), None)
        if not doc:
            return jsonify({"success": False, "message": "Document not found"}), 404
        
        # Return saved cost data
        cost_summary = doc.get("processing_cost")
        if not cost_summary:
            return jsonify({"success": False, "message": "Cost data not available yet"}), 404
        
        return jsonify({
            "success": True,
            "cost": cost_summary
        })
        
    except Exception as e:
        print(f"[ERROR] Failed to get document cost: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/costs/all")
def get_all_costs_endpoint():
    """Get costs for all documents"""
    try:
        # Collect costs from all documents
        all_costs = {}
        total_cost = 0.0
        textract_cost = 0.0
        bedrock_cost = 0.0
        s3_cost = 0.0
        
        for doc in processed_documents:
            doc_id = doc.get("id")
            cost_data = doc.get("processing_cost")
            
            if cost_data:
                all_costs[doc_id] = cost_data
                total_cost += cost_data.get("total_cost", 0)
                textract_cost += cost_data.get("textract", {}).get("cost", 0)
                bedrock_cost += cost_data.get("bedrock", {}).get("cost", 0)
                s3_cost += cost_data.get("s3", {}).get("cost", 0)
        
        total_costs = {
            "total_cost": total_cost,
            "textract_cost": textract_cost,
            "bedrock_cost": bedrock_cost,
            "s3_cost": s3_cost,
            "documents_processed": len([d for d in processed_documents if d.get("processing_cost")])
        }
        
        return jsonify({
            "success": True,
            "all_documents": all_costs,
            "total": total_costs
        })
        
    except Exception as e:
        print(f"[ERROR] Failed to get all costs: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


@app.route("/api/costs/summary")
def get_costs_summary():
    """Get summary of all costs"""
    try:
        # Calculate totals from saved cost data
        total_cost = 0.0
        textract_cost = 0.0
        bedrock_cost = 0.0
        s3_cost = 0.0
        docs_with_cost = 0
        
        for doc in processed_documents:
            cost_data = doc.get("processing_cost")
            if cost_data:
                total_cost += cost_data.get("total_cost", 0)
                textract_cost += cost_data.get("textract", {}).get("cost", 0)
                bedrock_cost += cost_data.get("bedrock", {}).get("cost", 0)
                s3_cost += cost_data.get("s3", {}).get("cost", 0)
                docs_with_cost += 1
        
        return jsonify({
            "success": True,
            "summary": {
                "total_cost": round(total_cost, 6),
                "textract_cost": round(textract_cost, 6),
                "bedrock_cost": round(bedrock_cost, 6),
                "s3_cost": round(s3_cost, 6),
                "documents_processed": docs_with_cost
            }
        })
        
    except Exception as e:
        print(f"[ERROR] Failed to get costs summary: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500
        
    except Exception as e:
        print(f"[ERROR] Failed to get complete account data: {str(e)}")
        return jsonify({"success": False, "message": str(e)}), 500


if __name__ == "__main__":
    print(f"[INFO] Starting Universal IDP - region: {AWS_REGION}, model: {MODEL_ID}")
    
    # Initialize background processor
    init_background_processor()
    
    try:
        app.run(debug=True, port=5015)
    except KeyboardInterrupt:
        print("[INFO] Application interrupted by user")
    finally:
        cleanup_background_processor()
